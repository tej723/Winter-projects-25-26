{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“š Assignment: Transfer Learning & The Power of Initialization\n",
        "## Building Intuition for MAML\n",
        "\n",
        "**Learning Objectives:**\n",
        "- Understand why initialization matters for few-shot learning\n",
        "- Experience the difference between various pre-training strategies\n",
        "- Develop intuition for what MAML tries to optimize"
      ],
      "metadata": {
        "id": "GVBnQU7-LWJ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Advice on using LLM's**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Avoid it , but unfortunately we cannot stop you from using it , dont ask it everything more you think on your own the better , but whenever you take in a code from it , understand how that part fits in the current code , is there some optimization it did on its own, node it down or comment it in the code."
      ],
      "metadata": {
        "id": "MGwIvFz-OBQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch torchvision matplotlib numpy\n",
        "\n",
        "#Understand what does each of this import do , see what all functions this hold\n",
        "#whenever you want to implement something think which of this would you use and refer to its doc for the syntax\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "print(\"âœ… Setup complete!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "iU4HQdjfLjpE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9826f4e8-09fd-4f55-a948-58996f5f3a80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Setup complete!\n",
            "PyTorch version: 2.9.0+cpu\n",
            "CUDA available: False\n",
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“Š Part A: Dataset Preparation\n",
        "\n",
        "We'll use **MNIST** for simplicity (or you can use Omniglot if you prefer).\n",
        "\n",
        "**Your Task:**\n",
        "- Split MNIST into 5 tasks (Tasks A-E), each with 2 digit classes\n",
        "- For example: Task A = {0, 1}, Task B = {2, 3}, etc."
      ],
      "metadata": {
        "id": "Jpe4MlEpLnRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Download MNIST\n",
        "transform = transforms.Compose([\n",
        "    # see what different tranformation you can do , one is converting the image into tensor\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(\n",
        "    root=\"./data\",\n",
        "    train=True,\n",
        "    transform=transform,\n",
        "    download=True\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(\n",
        "    root=\"./data\",\n",
        "    train=False,\n",
        "    transform=transform,\n",
        "    download=True\n",
        ")\n",
        "\n",
        "# we get a special parameter while loading which is 'background'\n",
        "# refer to document for what it means and how to use it\n",
        "# NOTE: 'background' is used in Omniglot, not MNIST, so we do not use it here\n",
        "\n",
        "print(f\"âœ… MNIST loaded: {len(train_dataset)} train, {len(test_dataset)} test images\")\n",
        "\n",
        "# TODO: Define your task structure\n",
        "# We'll split 10 digits into 5 tasks, each with 2 classes\n",
        "\n",
        "task_definitions = {\n",
        "    'A': [0, 1],\n",
        "    'B': [2, 3],\n",
        "    'C': [4, 5],\n",
        "    'D': [6, 7],\n",
        "    'E': [8, 9],\n",
        "}\n"
      ],
      "metadata": {
        "id": "CRoLfdMfMgFH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4665105-8549-456a-8eeb-1452db73f333"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.91M/9.91M [00:00<00:00, 137MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.9k/28.9k [00:00<00:00, 37.8MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.65M/1.65M [00:00<00:00, 29.7MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.54k/4.54k [00:00<00:00, 6.93MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… MNIST loaded: 60000 train, 10000 test images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Define your task structure\n",
        "# We'll split 10 digits into 5 tasks, each with 2 classes\n",
        "\n",
        "task_definitions = {\n",
        "    'A': [0, 1],\n",
        "    'B': [2, 3],\n",
        "    'C': [4, 5],\n",
        "    'D': [6, 7],\n",
        "    'E': [8, 9],\n",
        "}\n",
        "\n",
        "# Below function should take the given inputs and split the main dataset\n",
        "# with the given input classes into train, support and query.\n",
        "def create_task_datasets(dataset, task_classes, n_train=15, n_support=5, n_query=10):\n",
        "    \"\"\"\n",
        "    Create train, support, and query sets for a specific task.\n",
        "\n",
        "    Args:\n",
        "        dataset: Full MNIST dataset\n",
        "        task_classes: List of class labels for this task [e.g., [0, 1]]\n",
        "        n_train: Number of training examples per class\n",
        "        n_support: Number of support examples per class (for fine-tuning)\n",
        "        n_query: Number of query examples per class (for testing)\n",
        "\n",
        "    Returns:\n",
        "        train_data, support_data, query_data\n",
        "        (each is list of (image, label) tuples)\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: Implement this function\n",
        "    # HINT: Filter dataset to only include examples from task_classes\n",
        "    # HINT: Split into train/support/query sets\n",
        "\n",
        "    import random\n",
        "    from collections import defaultdict\n",
        "\n",
        "    # Collect samples for each class\n",
        "    class_to_samples = defaultdict(list)\n",
        "    for img, label in dataset:\n",
        "        if label in task_classes:\n",
        "            class_to_samples[label].append((img, label))\n",
        "\n",
        "    train_data = []\n",
        "    support_data = []\n",
        "    query_data = []\n",
        "\n",
        "    # Split samples per class\n",
        "    for cls in task_classes:\n",
        "        samples = class_to_samples[cls]\n",
        "        random.shuffle(samples)\n",
        "\n",
        "        train_data.extend(samples[:n_train])\n",
        "        support_data.extend(samples[n_train:n_train + n_support])\n",
        "        query_data.extend(samples[n_train + n_support:\n",
        "                                  n_train + n_support + n_query])\n",
        "\n",
        "    return train_data, support_data, query_data\n"
      ],
      "metadata": {
        "id": "qFKxWXDCMqZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the function\n",
        "\n",
        "train_A, support_A, query_A = create_task_datasets(train_dataset, task_definitions['A'])\n",
        "print(f\"Task A - Train: {len(train_A)}, Support: {len(support_A)}, Query: {len(query_A)}\")"
      ],
      "metadata": {
        "id": "xGdk_eqzNuh1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5959bd94-a25c-4c5a-e21d-7b6d760d1d22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task A - Train: 30, Support: 10, Query: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part A (continued): **Build Your Model**\n",
        "\n",
        "**TODO:** Design a simple CNN for digit classification"
      ],
      "metadata": {
        "id": "49q6LHJ1O1wC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# think on the architecture of the model as discussed in class\n",
        "# general flow -> convolution -> relu -> maxpooling -> ...\n",
        "# in the end some fully connected layers then final classification\n",
        "# Refer to the 60 minute pytorch implementation section of 'neural networks'\n",
        "\n",
        "# Implement the class or the model here\n",
        "# fill in the objects (layers) and methods (forward pass)\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "\n",
        "        # -------- Convolutional layers --------\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels=1,      # MNIST is grayscale\n",
        "            out_channels=32,\n",
        "            kernel_size=3,\n",
        "            padding=1\n",
        "        )\n",
        "\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            in_channels=32,\n",
        "            out_channels=64,\n",
        "            kernel_size=3,\n",
        "            padding=1\n",
        "        )\n",
        "\n",
        "        # -------- Pooling layer --------\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # -------- Fully connected layers --------\n",
        "        # After two poolings: 28x28 -> 14x14 -> 7x7\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Conv block 1\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Conv block 2\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # Flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "fhsb9ffDO-Xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now since the model is ready we decide how do we want to train it :\n",
        "\n",
        "First Do normal classification on large dataset of Task A - 0 & 1.\n",
        "\n",
        "The we will do fine tuning\n",
        "\n",
        "1.   Random Initialisation and then fine tune using support dataset, say we do this for task A which were 0 & 1 digits (save this)\n",
        "2.   Take the above model weights and fine tune it on the support dataset for some other task , say B(2's & 3's)\n",
        "3.   First train the model on all combined train dataset for all 10 digits(from all tasks A,B,C,D,E), then save it and then fine tune it on support dataset on to make a binary classifier , any 1 task say A here now digits will be classified. 0 class->0 digit , 1->1.\n",
        "\n",
        "While moving from one model to other , think what layers do i need to keep and what do i need to remove.\n",
        "\n"
      ],
      "metadata": {
        "id": "KVpBklwxPpj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Imports\n",
        "# ===============================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# ===============================\n",
        "# Model Definition\n",
        "# ===============================\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "\n",
        "        x = x.view(x.size(0), -1)  # flatten\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# ===============================\n",
        "# Prepare Task A Data\n",
        "# ===============================\n",
        "train_A, support_A, query_A = create_task_datasets(\n",
        "    train_dataset,\n",
        "    task_definitions['A']\n",
        ")\n",
        "\n",
        "def to_tensor_dataset(data):\n",
        "    images = torch.stack([x[0] for x in data])\n",
        "    labels = torch.tensor([x[1] for x in data])\n",
        "    return TensorDataset(images, labels)\n",
        "\n",
        "train_A_dataset = to_tensor_dataset(train_A)\n",
        "query_A_dataset = to_tensor_dataset(query_A)\n",
        "\n",
        "train_loader_A = DataLoader(train_A_dataset, batch_size=32, shuffle=True)\n",
        "query_loader_A = DataLoader(query_A_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# ===============================\n",
        "# Training Setup\n",
        "# ===============================\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_A = SimpleCNN(num_classes=2).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model_A.parameters(), lr=0.001)\n",
        "\n",
        "# ===============================\n",
        "# Training Loop\n",
        "# ===============================\n",
        "num_epochs = 10\n",
        "train_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model_A.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader_A:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_A(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader_A)\n",
        "    train_losses.append(avg_loss)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# ===============================\n",
        "# Evaluation on Query Set\n",
        "# ===============================\n",
        "model_A.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in query_loader_A:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model_A(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"\\nMethod 1 - Task A Query Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "nCVBCeVxXFQu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dddda825-fff6-4916-bedf-a837cf4f3411"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] - Loss: 0.7042\n",
            "Epoch [2/10] - Loss: 0.6383\n",
            "Epoch [3/10] - Loss: 0.5546\n",
            "Epoch [4/10] - Loss: 0.4613\n",
            "Epoch [5/10] - Loss: 0.3569\n",
            "Epoch [6/10] - Loss: 0.2540\n",
            "Epoch [7/10] - Loss: 0.1660\n",
            "Epoch [8/10] - Loss: 0.1004\n",
            "Epoch [9/10] - Loss: 0.0564\n",
            "Epoch [10/10] - Loss: 0.0299\n",
            "\n",
            "Method 1 - Task A Query Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import copy\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Helper functions\n",
        "# -------------------------------------------------\n",
        "def to_tensor_dataset(data, task_classes):\n",
        "    \"\"\"\n",
        "    Remap labels:\n",
        "    task_classes[0] -> 0\n",
        "    task_classes[1] -> 1\n",
        "    \"\"\"\n",
        "    images = torch.stack([x[0] for x in data])\n",
        "    labels = torch.tensor([task_classes.index(x[1]) for x in data])\n",
        "    return TensorDataset(images, labels)\n",
        "\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (preds == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Prepare Task A and Task B data\n",
        "# -------------------------------------------------\n",
        "train_A, support_A, query_A = create_task_datasets(\n",
        "    train_dataset, task_definitions['A']\n",
        ")\n",
        "train_B, support_B, query_B = create_task_datasets(\n",
        "    train_dataset, task_definitions['B']\n",
        ")\n",
        "\n",
        "support_A_loader = DataLoader(\n",
        "    to_tensor_dataset(support_A, task_definitions['A']),\n",
        "    batch_size=16, shuffle=True\n",
        ")\n",
        "query_A_loader = DataLoader(\n",
        "    to_tensor_dataset(query_A, task_definitions['A']),\n",
        "    batch_size=32, shuffle=False\n",
        ")\n",
        "\n",
        "support_B_loader = DataLoader(\n",
        "    to_tensor_dataset(support_B, task_definitions['B']),\n",
        "    batch_size=16, shuffle=True\n",
        ")\n",
        "query_B_loader = DataLoader(\n",
        "    to_tensor_dataset(query_B, task_definitions['B']),\n",
        "    batch_size=32, shuffle=False\n",
        ")\n",
        "\n",
        "# =========================================================\n",
        "# Method 2A: Random Initialization â†’ Fine-tune on Support A\n",
        "# =========================================================\n",
        "model_2A = SimpleCNN(num_classes=2).to(device)\n",
        "optimizer = torch.optim.Adam(model_2A.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(5):\n",
        "    model_2A.train()\n",
        "    for images, labels in support_A_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(model_2A(images), labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "acc_2A = evaluate(model_2A, query_A_loader)\n",
        "print(f\"Method 2A (Random Init â†’ Support A) Accuracy: {acc_2A:.2f}%\")\n",
        "\n",
        "# =========================================================\n",
        "# Method 2B: Task A â†’ Fine-tune on Support B\n",
        "# =========================================================\n",
        "model_2B = copy.deepcopy(model_A)\n",
        "\n",
        "# Replace classifier head (binary)\n",
        "model_2B.fc2 = nn.Linear(128, 2).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model_2B.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(5):\n",
        "    model_2B.train()\n",
        "    for images, labels in support_B_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(model_2B(images), labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "acc_2B = evaluate(model_2B, query_B_loader)\n",
        "print(f\"Method 2B (Task A â†’ Support B) Accuracy: {acc_2B:.2f}%\")\n",
        "\n",
        "# =========================================================\n",
        "# Method 2C: Train on ALL 10 digits â†’ Fine-tune on Support A\n",
        "# =========================================================\n",
        "\n",
        "# -------- Step 1: Train 10-class model --------\n",
        "full_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "model_10 = SimpleCNN(num_classes=10).to(device)\n",
        "optimizer = torch.optim.Adam(model_10.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(5):\n",
        "    model_10.train()\n",
        "    for images, labels in full_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(model_10(images), labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# -------- Step 2: Replace head â†’ Fine-tune on Task A --------\n",
        "model_2C = copy.deepcopy(model_10)\n",
        "model_2C.fc2 = nn.Linear(128, 2).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model_2C.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(5):\n",
        "    model_2C.train()\n",
        "    for images, labels in support_A_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(model_2C(images), labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "acc_2C = evaluate(model_2C, query_A_loader)\n",
        "print(f\"Method 2C (10-digit Pretrain â†’ Support A) Accuracy: {acc_2C:.2f}%\")\n"
      ],
      "metadata": {
        "id": "LpzPYTkkXH9Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83b2257e-26e1-4d33-cc77-2c5c977cec71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method 2A (Random Init â†’ Support A) Accuracy: 100.00%\n",
            "Method 2B (Task A â†’ Support B) Accuracy: 50.00%\n",
            "Method 2C (10-digit Pretrain â†’ Support A) Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the end compare performance of all this models and methods using the Query Set.\n",
        "\n",
        "Also plot the learning curve vs epoch for all the methods\n",
        "\n",
        "Make a table and fill in the values of different evaluation metrics you learned in previous lectures."
      ],
      "metadata": {
        "id": "68yPic0PXeR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# -------------------------------------\n",
        "# Helper: get predictions and labels\n",
        "# -------------------------------------\n",
        "def get_predictions(model, dataloader):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "    return y_true, y_pred\n",
        "\n",
        "# -------------------------------------\n",
        "# Collect predictions for all methods\n",
        "# -------------------------------------\n",
        "\n",
        "# Method 1 (Task A)\n",
        "y_true_m1, y_pred_m1 = get_predictions(model_A, query_A_loader)\n",
        "\n",
        "# Method 2A (Random Init â†’ Support A)\n",
        "y_true_2A, y_pred_2A = get_predictions(model_2A, query_A_loader)\n",
        "\n",
        "# Method 2B (Task A â†’ Support B)\n",
        "y_true_2B, y_pred_2B = get_predictions(model_2B, query_B_loader)\n",
        "\n",
        "# Method 2C (10-digit Pretrain â†’ Support A)\n",
        "y_true_2C, y_pred_2C = get_predictions(model_2C, query_A_loader)\n",
        "\n",
        "# -------------------------------------\n",
        "# Build evaluation metrics table\n",
        "# -------------------------------------\n",
        "results = {\n",
        "    \"Method\": [\n",
        "        \"Method 1: Train from Scratch (A)\",\n",
        "        \"Method 2A: Random Init â†’ Support A\",\n",
        "        \"Method 2B: Task A â†’ Support B\",\n",
        "        \"Method 2C: 10-digit Pretrain â†’ Support A\",\n",
        "    ],\n",
        "    \"Accuracy\": [\n",
        "        accuracy_score(y_true_m1, y_pred_m1),\n",
        "        accuracy_score(y_true_2A, y_pred_2A),\n",
        "        accuracy_score(y_true_2B, y_pred_2B),\n",
        "        accuracy_score(y_true_2C, y_pred_2C),\n",
        "    ],\n",
        "    \"Precision\": [\n",
        "        precision_score(y_true_m1, y_pred_m1, average=\"macro\"),\n",
        "        precision_score(y_true_2A, y_pred_2A, average=\"macro\"),\n",
        "        precision_score(y_true_2B, y_pred_2B, average=\"macro\"),\n",
        "        precision_score(y_true_2C, y_pred_2C, average=\"macro\"),\n",
        "    ],\n",
        "    \"Recall\": [\n",
        "        recall_score(y_true_m1, y_pred_m1, average=\"macro\"),\n",
        "        recall_score(y_true_2A, y_pred_2A, average=\"macro\"),\n",
        "        recall_score(y_true_2B, y_pred_2B, average=\"macro\"),\n",
        "        recall_score(y_true_2C, y_pred_2C, average=\"macro\"),\n",
        "    ],\n",
        "    \"F1-score\": [\n",
        "        f1_score(y_true_m1, y_pred_m1, average=\"macro\"),\n",
        "        f1_score(y_true_2A, y_pred_2A, average=\"macro\"),\n",
        "        f1_score(y_true_2B, y_pred_2B, average=\"macro\"),\n",
        "        f1_score(y_true_2C, y_pred_2C, average=\"macro\"),\n",
        "    ],\n",
        "}\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "print(\"\\nEvaluation Metrics on Query Sets:\\n\")\n",
        "print(df_results)\n"
      ],
      "metadata": {
        "id": "k42bvzgsaHSG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c998b95-63b6-4b50-b6ac-f5288a4296cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Metrics on Query Sets:\n",
            "\n",
            "                                     Method  Accuracy  Precision  Recall  \\\n",
            "0          Method 1: Train from Scratch (A)       1.0       1.00     1.0   \n",
            "1        Method 2A: Random Init â†’ Support A       1.0       1.00     1.0   \n",
            "2             Method 2B: Task A â†’ Support B       0.5       0.25     0.5   \n",
            "3  Method 2C: 10-digit Pretrain â†’ Support A       1.0       1.00     1.0   \n",
            "\n",
            "   F1-score  \n",
            "0  1.000000  \n",
            "1  1.000000  \n",
            "2  0.333333  \n",
            "3  1.000000  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some Theoritical Questions :\n",
        "\n",
        "1.   Which strategy in Method 2 works best and why do you feel so ?\n",
        "2.   In Part 3 of Method 2 we have trained the model already on Task B as well when we made a 10 class classifier, then when we are fine tuning it again using support set what exactly is happening ?\n",
        "3.   What if we used the 10 digit classifier to make a binary classifier for a binary letter classification , will it work or rather how will you make it work ?\n",
        "4.   Where exactly have we used Meta Learning, in which approach? Have we even used it ?\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Digit classifier and letter classifier are two dissimilar tasks can we have starting point or a initialisation such that when we fine tuning using a few datapoints for both tasks we get optmimal result ? This is what we will try to do in MAML ?\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Think on them sincerely , would love to read your answers!\n",
        "\n",
        "---\n",
        "1. Among the strategies explored in Method 2, the approach that works best is training a model on all ten digits first and then fine-tuning it using the support set for a specific task (Method 2C). This strategy performs well because pretraining on all digits allows the model to learn general and reusable features such as edges, curves, and stroke patterns that are common across digits. When the model is later fine-tuned on a small support set, only the task-specific decision boundaries need to be adjusted, which is much easier than learning features from scratch. In contrast, random initialization relies heavily on overfitting to a small dataset, and task-to-task transfer can fail if the tasks are dissimilar, leading to poor performance.\n",
        "\n",
        "2. During the 10-class training phase, the model learns to extract general digit-level features without associating them with a specific binary task. Task B contributes to this learning by helping the model understand digit structures, but it does not define the final classification objective. When fine-tuning is performed using the support set, the original classification head is replaced with a binary classifier, and the model is optimized to separate only the two relevant classes. At this stage, the learned features are reused, but the decision boundary is reshaped to match the new binary task. Fine-tuning therefore adapts the modelâ€™s interpretation of features rather than relearning them from scratch.\n",
        "\n",
        "3. A model trained only on digits is unlikely to perform well when directly applied to letter classification because digits and letters have different visual structures and stroke patterns. However, this approach can be made to work if the pretraining task is expanded to include a broader and more diverse set of visual concepts. For example, training on both digits and letters, or on datasets like Omniglot that contain multiple alphabets, would allow the model to learn more universal visual features. Another effective approach would be self-supervised or contrastive pretraining, which encourages the model to learn general representations independent of specific labels. Once such general features are learned, fine-tuning on a small letter dataset can produce good performance.\n",
        "\n",
        "4. Yes, it is possible to learn an initialization that works well for both digit and letter classification, and this is precisely the motivation behind Model-Agnostic Meta-Learning (MAML). MAML aims to learn a set of initial parameters that can be quickly adapted to a wide range of tasks using only a few training examples. Instead of optimizing the model for a single task, MAML optimizes the model so that a small number of gradient updates leads to good performance on any task drawn from a task distribution. In this sense, MAML focuses on learning how to learn, making it especially suitable for scenarios where tasks are diverse and data is limited."
      ],
      "metadata": {
        "id": "j7DID45NYPVe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ALL THE BEST !"
      ],
      "metadata": {
        "id": "TVWntvGLXtZi"
      }
    }
  ]
}