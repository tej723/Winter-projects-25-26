{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1t6CL5ID17QGTdIId6ANQVUe4P5rCb949","timestamp":1767443855777}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## ðŸ“š Assignment: Transfer Learning & The Power of Initialization\n","## Building Intuition for MAML\n","\n","**Learning Objectives:**\n","- Understand why initialization matters for few-shot learning\n","- Experience the difference between various pre-training strategies\n","- Develop intuition for what MAML tries to optimize"],"metadata":{"id":"GVBnQU7-LWJ8"}},{"cell_type":"markdown","source":["**Advice on using LLM's**\n","\n","---\n","\n","\n","Avoid it , but unfortunately we cannot stop you from using it , dont ask it everything more you think on your own the better , but whenever you take in a code from it , understand how that part fits in the current code , is there some optimization it did on its own, node it down or comment it in the code."],"metadata":{"id":"MGwIvFz-OBQs"}},{"cell_type":"code","source":["!pip install -q torch torchvision matplotlib numpy\n","\n","#Understand what does each of this import do , see what all functions this hold\n","#whenever you want to implement something think which of this would you use and refer to its doc for the syntax\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset, DataLoader, Subset\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from collections import defaultdict\n","from collections import Counter\n","import random\n","\n","print(\"âœ… Setup complete!\")\n","print(f\"PyTorch version: {torch.__version__}\")\n","print(f\"CUDA available: {torch.cuda.is_available()}\")\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")"],"metadata":{"id":"iU4HQdjfLjpE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1767458415343,"user_tz":-330,"elapsed":27789,"user":{"displayName":"Shreyash Katiyar","userId":"02012180359299006072"}},"outputId":"e03a001c-30aa-4866-9491-9b8b1bfdcf99"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Setup complete!\n","PyTorch version: 2.9.0+cpu\n","CUDA available: False\n","Using device: cpu\n"]}]},{"cell_type":"markdown","source":["## ðŸ“Š Part A: Dataset Preparation\n","\n","We'll use **MNIST** for simplicity (or you can use Omniglot if you prefer).\n","\n","**Your Task:**\n","- Split MNIST into 5 tasks (Tasks A-E), each with 2 digit classes\n","- For example: Task A = {0, 1}, Task B = {2, 3}, etc."],"metadata":{"id":"Jpe4MlEpLnRg"}},{"cell_type":"code","source":["\n","\n","# Complete MNIST loading with standard transform and background note\n","transform = transforms.Compose([transforms.ToTensor(),  # Converts PIL image to tensor [0,1]\n","                                ])\n","\n","train_dataset = torchvision.datasets.MNIST(\n","    root='./data', train=True, download=True, transform=transform\n",")\n","test_dataset = torchvision.datasets.MNIST(\n","    root='./data', train=False, download=True, transform=transform\n",")\n","\n","print(f\"âœ… MNIST loaded: {len(train_dataset)} train, {len(test_dataset)} test images\")\n","\n","# Task definitions: 5 binary tasks with 2 digit classes each [web:3]\n","task_definitions = {\n","    'A': [0, 1],\n","    'B': [2, 3],\n","    'C': [4, 5],\n","    'D': [6, 7],\n","    'E': [8, 9]\n","}\n","\n","def create_task_datasets(dataset, task_classes, n_train=15, n_support=5, n_query=10):\n","    \"\"\"\n","    Creates train/support/query splits for a few-shot task from dataset.\n","    1) train: n_train total examples for task (background training)\n","    2) support: n_support per class (2*n_support total)\n","    3) query: n_query per class (2*n_query total)\n","    Assumes balanced classes; samples randomly without replacement.\n","    \"\"\"\n","    # Get indices for each class in the dataset\n","    class_indices = [[] for _ in range(10)]\n","    for idx, label in enumerate(dataset.targets):\n","        class_indices[label].append(idx)\n","\n","    cls1, cls2 = task_classes\n","    indices_cls1 = class_indices[cls1]\n","    indices_cls2 = class_indices[cls2]\n","\n","    # Sample distinct indices for splits (shuffle for randomness)\n","    rng = np.random.default_rng()  # Or use random.seed()\n","    train_indices = (\n","        rng.choice(indices_cls1, min(n_train//2, len(indices_cls1)), replace=False).tolist() +\n","        rng.choice(indices_cls2, min(n_train//2, len(indices_cls2)), replace=False).tolist()\n","    )\n","    remain_cls1 = [i for i in indices_cls1 if i not in train_indices]\n","    remain_cls2 = [i for i in indices_cls2 if i not in train_indices]\n","\n","    support_cls1 = rng.choice(remain_cls1, min(n_support, len(remain_cls1)), replace=False).tolist()\n","    remain_cls1 = [i for i in remain_cls1 if i not in support_cls1]\n","    support_cls2 = rng.choice(remain_cls2, min(n_support, len(remain_cls2)), replace=False).tolist()\n","    remain_cls2 = [i for i in remain_cls2 if i not in support_cls2]\n","\n","    query_cls1 = rng.choice(remain_cls1, min(n_query, len(remain_cls1)), replace=False).tolist()\n","    query_cls2 = rng.choice(remain_cls2, min(n_query, len(remain_cls2)), replace=False).tolist()\n","\n","    train_subset = Subset(dataset, train_indices)\n","    support_subset = Subset(dataset, support_cls1 + support_cls2)\n","    query_subset = Subset(dataset, query_cls1 + query_cls2)\n","\n","    return train_subset, support_subset, query_subset\n","\n","# Test the function\n","train_A, support_A, query_A = create_task_datasets(train_dataset, task_definitions['A'])\n","print(f\"Task A - Train: {len(train_A)}, Support: {len(support_A)}, Query: {len(query_A)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8VE7Qvrt3YcW","executionInfo":{"status":"ok","timestamp":1767458425254,"user_tz":-330,"elapsed":9898,"user":{"displayName":"Shreyash Katiyar","userId":"02012180359299006072"}},"outputId":"9dd9585f-7e60-47f0-919b-57cdbc19b6ad"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.91M/9.91M [00:01<00:00, 5.29MB/s]\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.9k/28.9k [00:00<00:00, 157kB/s]\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.65M/1.65M [00:01<00:00, 1.48MB/s]\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.54k/4.54k [00:00<00:00, 8.46MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["âœ… MNIST loaded: 60000 train, 10000 test images\n","Task A - Train: 14, Support: 10, Query: 20\n"]}]},{"cell_type":"markdown","source":["Part A (continued): **Build Your Model**\n","\n","**TODO:** Design a simple CNN for digit classification"],"metadata":{"id":"49q6LHJ1O1wC"}},{"cell_type":"markdown","source":["Now since the model is ready we decide how do we want to train it :\n","\n","First Do normal classification on large dataset of Task A - 0 & 1.\n","\n","The we will do fine tuning\n","\n","1.   Random Initialisation and then fine tune using support dataset, say we do this for task A which were 0 & 1 digits (save this)\n","2.   Take the above model weights and fine tune it on the support dataset for some other task , say B(2's & 3's)\n","3.   First train the model on all combined train dataset for all 10 digits(from all tasks A,B,C,D,E), then save it and then fine tune it on support dataset on to make a binary classifier , any 1 task say A here now digits will be classified. 0 class->0 digit , 1->1.\n","\n","While moving from one model to other , think what layers do i need to keep and what do i need to remove.\n","\n"],"metadata":{"id":"KVpBklwxPpj1"}},{"cell_type":"code","source":["import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","class SimpleCNN(nn.Module):\n","    def __init__(self):\n","        super(SimpleCNN, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)  # In:1x28x28 -> 32x28x28\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1) # -> 64x28x28\n","        self.pool = nn.MaxPool2d(2, 2)                           # Halves spatial dims\n","        self.fc1 = nn.Linear(64 * 7 * 7, 128)                    # After two pools: 64x7x7\n","        self.fc2 = nn.Linear(128, 10)                            # 10 digits\n","        self.dropout = nn.Dropout(0.25)\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))     # 32x14x14\n","        x = self.pool(F.relu(self.conv2(x)))     # 64x7x7\n","        x = x.view(-1, 64 * 7 * 7)              # Flatten\n","        x = F.relu(self.fc1(x))\n","        x = self.dropout(x)\n","        x = self.fc2(x)\n","        return F.log_softmax(x, dim=1)           # For NLLLoss\n"],"metadata":{"id":"_NJTGD_z4qXt","executionInfo":{"status":"ok","timestamp":1767458425273,"user_tz":-330,"elapsed":16,"user":{"displayName":"Shreyash Katiyar","userId":"02012180359299006072"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["#Method 1 - Fully Trained Binary Classifier on Task A\n","# Binary model for Task A\n","class BinaryCNN(SimpleCNN):\n","    def __init__(self):\n","        super().__init__()\n","        self.fc2 = nn.Linear(128, 2)  # Change to 2 outputs\n","\n","# Remap labels for binary: original label % 2 or map {0:0, 1:1}\n","def binary_collate(batch):\n","    images, labels = zip(*batch)\n","    images = torch.stack(images)\n","    labels = torch.tensor([1 if l == task_definitions['A'][1] else 0 for l in labels])\n","    return images, labels\n","\n","train_loader_A = DataLoader(train_A, batch_size=64, shuffle=True, collate_fn=binary_collate)\n","\n","model_A = BinaryCNN().to(device)\n","optimizer = optim.Adam(model_A.parameters(), lr=0.001)\n","criterion = nn.NLLLoss()  # Or CrossEntropyLoss\n","num_epochs=10\n","# Train loop (5-10 epochs)\n","for epoch in range(num_epochs):\n","    model_A.train()\n","    for imgs, labels in train_loader_A:\n","        imgs, labels = imgs.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        out = model_A(imgs)\n","        loss = criterion(out, labels)\n","        loss.backward()\n","        optimizer.step()\n","torch.save(model_A.state_dict(), 'model_A.pth')  # Save full model\n","model_rand = BinaryCNN().to(device)  # Random init\n","optimizer = optim.Adam(model_rand.parameters(), lr=0.0001)  # Lower LR\n","\n","support_loader_A = DataLoader(support_A, batch_size=32, shuffle=True, collate_fn=binary_collate)\n","# Train on support_A (small, few-shot) for 20-50 epochs similarly\n","torch.save(model_rand.state_dict(), 'model_rand_A.pth')\n"],"metadata":{"id":"nCVBCeVxXFQu","executionInfo":{"status":"ok","timestamp":1767458426119,"user_tz":-330,"elapsed":843,"user":{"displayName":"Shreyash Katiyar","userId":"02012180359299006072"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["#Method 2 - Fine Tuning all 3 methods\n","# Load A weights into new BinaryCNN for B\n","model_B = BinaryCNN().to(device)\n","model_B.load_state_dict(torch.load('model_A.pth'), strict=False)  # Ignores fc2 mismatch\n","\n","# Freeze early layers (optional, reduces catastrophic forgetting)\n","for param in model_B.conv1.parameters(): param.requires_grad = False\n","for param in model_B.conv2.parameters(): param.requires_grad = False\n","# Unfreeze fc1, fc2\n","\n","optimizer = optim.Adam(filter(lambda p: p.requires_grad, model_B.parameters()), lr=0.0001)\n","\n","def binary_collate_B(batch):  # For Task B classes\n","    images, labels = zip(*batch)\n","    images = torch.stack(images)\n","    labels = torch.tensor([1 if l == task_definitions['B'][1] else 0 for l in labels])\n","    return images, labels\n","\n","# Fix: Create support_B before using it\n","# You need to define task_definitions['B'] and call create_task_datasets for it\n","# Assuming task_definitions is defined and create_task_datasets is available\n","# Example of how you would create support_B:\n"," support_B, query_B = create_task_datasets(train_dataset, task_definitions['B'])\n","\n","support_loader_B = DataLoader(support_B, batch_size=16, shuffle=True, collate_fn=binary_collate_B)\n","# Train similarly, evaluate query_B accuracy >95%\n","# First, train 10-class model on combined train (all tasks A-E)\n","full_train_idx = []\n","for task_cls in task_definitions.values():\n","    cls_indices = [i for i, l in enumerate(train_dataset.targets) if l in task_cls]\n","    full_train_idx.extend(random.sample(cls_indices, 1000))  # Balanced subsample or full\n","full_train = Subset(train_dataset, full_train_idx)\n","full_loader = DataLoader(full_train, batch_size=64, shuffle=True)\n","\n","model_10c = SimpleCNN().to(device)  # Original 10 outputs\n","# Train on full_loader with CrossEntropyLoss (10 classes), save 'model_10c.pth'\n","\n","# Then fine-tune to Binary A\n","model_10c_binary = BinaryCNN().to(device)\n","# Copy all except fc2\n","state_10c = torch.load('model_10c.pth')\n","model_10c_binary.load_state_dict({k: v for k, v in state_10c.items() if k != 'fc2'}, strict=False)\n","model_10c_binary.fc2 = nn.Linear(128, 2)  # New random fc2\n","\n","# Freeze convs if desired, train on support_A as in Phase 2"],"metadata":{"id":"LpzPYTkkXH9Y","colab":{"base_uri":"https://localhost:8080/","height":106},"executionInfo":{"status":"error","timestamp":1767458520109,"user_tz":-330,"elapsed":26,"user":{"displayName":"Shreyash Katiyar","userId":"02012180359299006072"}},"outputId":"f92bf87e-941d-48de-ace3-08ad394f03d6"},"execution_count":8,"outputs":[{"output_type":"error","ename":"IndentationError","evalue":"unindent does not match any outer indentation level (<tokenize>, line 19)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    support_B, query_B = create_task_datasets(train_dataset, task_definitions['B'])\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"]}]},{"cell_type":"markdown","source":["At the end compare performance of all this models and methods using the Query Set.\n","\n","Also plot the learning curve vs epoch for all the methods\n","\n","Make a table and fill in the values of different evaluation metrics you learned in previous lectures."],"metadata":{"id":"68yPic0PXeR_"}},{"cell_type":"code","source":["#Analysis and Plots\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","\n","def eval_model(model, loader, device):\n","    model.eval()\n","    all_preds, all_labels = [], []\n","    with torch.no_grad():\n","        for x, y in loader:\n","            x, y = x.to(device), y.to(device)\n","            out = model(x)\n","            preds = out.argmax(dim=1)\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(y.cpu().numpy())\n","    acc = accuracy_score(all_labels, all_preds)\n","    prec, rec, f1, _ = precision_recall_fscore_support(\n","        all_labels, all_preds, average='binary'\n","    )\n","    return acc, prec, rec, f1\n","import matplotlib.pyplot as plt\n","\n","epochs = range(1, num_epochs+1)\n","plt.plot(epochs, acc_taskA_large, label='TaskA_large')\n","plt.plot(epochs, acc_taskA_support, label='TaskA_support_randInit')\n","plt.plot(epochs, acc_taskB_fromA, label='TaskB_fromA')\n","plt.plot(epochs, acc_taskA_from10, label='TaskA_from10class')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.grid(True)\n","plt.title('Learning curves (Accuracy vs Epoch)')\n","plt.show()\n"],"metadata":{"id":"k42bvzgsaHSG","executionInfo":{"status":"aborted","timestamp":1767458426190,"user_tz":-330,"elapsed":38758,"user":{"displayName":"Shreyash Katiyar","userId":"02012180359299006072"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Some Theoritical Questions :\n","\n","1.   Which strategy in Method 2 works best and why do you feel so ?\n","2.   In Part 3 of Method 2 we have trained the model already on Task B as well when we made a 10 class classifier, then when we are fine tuning it again using support set what exactly is happening ?\n","3.   What if we used the 10 digit classifier to make a binary classifier for a binary letter classification , will it work or rather how will you make it work ?\n","4.   Where exactly have we used Meta Learning, in which approach? Have we even used it ?\n","\n","---\n","\n","\n","Digit classifier and letter classifier are two dissimilar tasks can we have starting point or a initialisation such that when we fine tuning using a few datapoints for both tasks we get optmimal result ? This is what we will try to do in MAML ?\n","\n","\n","---\n","\n","\n","Think on them sincerely , would love to read your answers!\n","\n"],"metadata":{"id":"j7DID45NYPVe"}},{"cell_type":"markdown","source":["# ALL THE BEST !"],"metadata":{"id":"TVWntvGLXtZi"}}]}