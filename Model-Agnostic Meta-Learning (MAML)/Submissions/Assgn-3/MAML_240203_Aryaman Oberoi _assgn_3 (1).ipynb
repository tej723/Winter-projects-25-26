{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“š Assignment: Transfer Learning & The Power of Initialization\n",
        "## Building Intuition for MAML\n",
        "\n",
        "**Learning Objectives:**\n",
        "- Understand why initialization matters for few-shot learning\n",
        "- Experience the difference between various pre-training strategies\n",
        "- Develop intuition for what MAML tries to optimize"
      ],
      "metadata": {
        "id": "GVBnQU7-LWJ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Advice on using LLM's**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Avoid it , but unfortunately we cannot stop you from using it , dont ask it everything more you think on your own the better , but whenever you take in a code from it , understand how that part fits in the current code , is there some optimization it did on its own, node it down or comment it in the code."
      ],
      "metadata": {
        "id": "MGwIvFz-OBQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch torchvision matplotlib numpy\n",
        "\n",
        "#Understand what does each of this import do , see what all functions this hold\n",
        "#whenever you want to implement something think which of this would you use and refer to its doc for the syntax\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "print(\"âœ… Setup complete!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "iU4HQdjfLjpE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e54be85-7f0e-47e9-e2c5-6417c356b07c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Setup complete!\n",
            "PyTorch version: 2.9.0+cpu\n",
            "CUDA available: False\n",
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“Š Part A: Dataset Preparation\n",
        "\n",
        "We'll use **MNIST** for simplicity (or you can use Omniglot if you prefer).\n",
        "\n",
        "**Your Task:**\n",
        "- Split MNIST into 5 tasks (Tasks A-E), each with 2 digit classes\n",
        "- For example: Task A = {0, 1}, Task B = {2, 3}, etc."
      ],
      "metadata": {
        "id": "Jpe4MlEpLnRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Subset\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Define Transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(), # Converts PIL Image to (C x H x W) tensor in range [0, 1]\n",
        "    transforms.Normalize((0.1307,), (0.3081,)) # MNIST mean and std deviation\n",
        "])\n",
        "\n",
        "# Step 2: Load MNIST\n",
        "# 'download=True' fetches the data, 'train=True/False' selects the split\n",
        "train_dataset = torchvision.datasets.MNIST(\n",
        "    root='./data', train=True, download=True, transform=transform\n",
        ")\n",
        "test_dataset = torchvision.datasets.MNIST(\n",
        "    root='./data', train=False, download=True, transform=transform\n",
        ")\n",
        "\n",
        "print(f\"âœ… MNIST loaded: {len(train_dataset)} train, {len(test_dataset)} test images\")"
      ],
      "metadata": {
        "id": "CRoLfdMfMgFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Define your task structure\n",
        "# We'll split 10 digits into 5 tasks, each with 2 classes\n",
        "\n",
        "# Split 10 digits into 5 tasks (A-E)\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Subset\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Define Transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(), # Converts PIL Image to (C x H x W) tensor in range [0, 1]\n",
        "    transforms.Normalize((0.1307,), (0.3081,)) # MNIST mean and std deviation\n",
        "])\n",
        "\n",
        "# Step 2: Load MNIST\n",
        "# 'download=True' fetches the data, 'train=True/False' selects the split\n",
        "train_dataset = torchvision.datasets.MNIST(\n",
        "    root='./data', train=True, download=True, transform=transform\n",
        ")\n",
        "test_dataset = torchvision.datasets.MNIST(\n",
        "    root='./data', train=False, download=True, transform=transform\n",
        ")\n",
        "\n",
        "print(f\"âœ… MNIST loaded: {len(train_dataset)} train, {len(test_dataset)} test images\")\n",
        "task_definitions = {\n",
        "    'A': [0, 1],\n",
        "    'B': [2, 3],\n",
        "    'C': [4, 5],\n",
        "    'D': [6, 7],\n",
        "    'E': [8, 9]\n",
        "}\n",
        "\n",
        "#Below function should take the given inputs and split the main dataset with the given input classes into train,support and query.\n",
        "def create_task_datasets(dataset, task_classes, n_train=15, n_support=5, n_query=10):\n",
        "    \"\"\"\n",
        "    Filters the dataset for task_classes and splits them into Train, Support, and Query sets.\n",
        "    \"\"\"\n",
        "    train_data, support_data, query_data = [], [], []\n",
        "\n",
        "    # Get all labels from the dataset\n",
        "    targets = np.array(dataset.targets)\n",
        "\n",
        "    for cls in task_classes:\n",
        "        # Find indices where the label matches the current class in the task\n",
        "        indices = np.where(targets == cls)[0]\n",
        "\n",
        "        # Shuffle indices to ensure randomness\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        # Select required number of samples\n",
        "        # Total needed = n_train + n_support + n_query\n",
        "        selected_idx = indices[:n_train + n_support + n_query]\n",
        "\n",
        "        # Split selected indices into sub-groups\n",
        "        cls_train_idx = selected_idx[:n_train]\n",
        "        cls_support_idx = selected_idx[n_train : n_train + n_support]\n",
        "        cls_query_idx = selected_idx[n_train + n_support :]\n",
        "\n",
        "        # Extract the (image, label) tuples for each group\n",
        "        # Note: We keep the original labels, but in some tasks you might remap them to [0, 1]\n",
        "        train_data.extend([dataset[i] for i in cls_train_idx])\n",
        "        support_data.extend([dataset[i] for i in cls_support_idx])\n",
        "        query_data.extend([dataset[i] for i in cls_query_idx])\n",
        "\n",
        "    return train_data, support_data, query_data\n",
        "train_A, support_A, query_A = create_task_datasets(train_dataset, task_definitions['A'])\n",
        "\n",
        "# Test the function\n",
        "\n",
        "# TODO: Implement this function\n",
        "# HINT: Filter dataset to only include examples from task_classes\n",
        "# HINT: Split into train/support/query sets"
      ],
      "metadata": {
        "id": "qFKxWXDCMqZg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a94802f-3ff0-4215-f8e5-73f6074f8e64"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.91M/9.91M [00:00<00:00, 18.1MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.9k/28.9k [00:00<00:00, 505kB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.65M/1.65M [00:00<00:00, 4.59MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.54k/4.54k [00:00<00:00, 10.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… MNIST loaded: 60000 train, 10000 test images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the function\n",
        "\n",
        "train_A, support_A, query_A = create_task_datasets(train_dataset, task_definitions['A'])\n",
        "print(f\"Task A - Train: {len(train_A)}, Support: {len(support_A)}, Query: {len(query_A)}\")"
      ],
      "metadata": {
        "id": "xGdk_eqzNuh1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "60475354-4519-4307-d627-fad33920a14f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3506257401.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_task_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_definitions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'A'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Task A - Train: {len(train_A)}, Support: {len(support_A)}, Query: {len(query_A)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part A (continued): **Build Your Model**\n",
        "\n",
        "**TODO:** Design a simple CNN for digit classification"
      ],
      "metadata": {
        "id": "49q6LHJ1O1wC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        # Architecture: Convolution -> ReLU -> MaxPool\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Fully Connected layers\n",
        "        # After two 2x2 pooling layers, 28x28 image becomes 7x7\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes) # Final classification layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 7 * 7) # Flatten\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Helper function to train\n",
        "def train_model(model, train_loader, epochs=5, lr=0.001):\n",
        "    model.train()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    history = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for images, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        history.append(total_loss / len(train_loader))\n",
        "        print(f\"Epoch {epoch+1}, Loss: {history[-1]:.4f}\")\n",
        "    return history"
      ],
      "metadata": {
        "id": "fhsb9ffDO-Xa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now since the model is ready we decide how do we want to train it :\n",
        "\n",
        "First Do normal classification on large dataset of Task A - 0 & 1.\n",
        "\n",
        "The we will do fine tuning\n",
        "\n",
        "1.   Random Initialisation and then fine tune using support dataset, say we do this for task A which were 0 & 1 digits (save this)\n",
        "2.   Take the above model weights and fine tune it on the support dataset for some other task , say B(2's & 3's)\n",
        "3.   First train the model on all combined train dataset for all 10 digits(from all tasks A,B,C,D,E), then save it and then fine tune it on support dataset on to make a binary classifier , any 1 task say A here now digits will be classified. 0 class->0 digit , 1->1.\n",
        "\n",
        "While moving from one model to other , think what layers do i need to keep and what do i need to remove.\n",
        "\n"
      ],
      "metadata": {
        "id": "KVpBklwxPpj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 1. Prepare Large Task A dataset (all 0s and 1s from train_dataset)\n",
        "# Filtering full dataset for digits 0 and 1\n",
        "indices_A = [i for i, target in enumerate(train_dataset.targets) if target in [0, 1]]\n",
        "task_A_full_loader = DataLoader(torch.utils.data.Subset(train_dataset, indices_A), batch_size=32, shuffle=True)\n",
        "\n",
        "# 2. Initialize Model for 2 classes\n",
        "model_method1 = SimpleCNN(num_classes=2)\n",
        "\n",
        "# 3. Train from scratch\n",
        "print(\"Training Method 1: Full Task A Training...\")\n",
        "history_m1 = train_model(model_method1, task_A_full_loader, epochs=5)\n",
        "\n",
        "# Save weights for Method 2 comparison\n",
        "torch.save(model_method1.state_dict(), 'model_method1.pth')"
      ],
      "metadata": {
        "id": "nCVBCeVxXFQu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "fe889bbb-13bf-4080-9c4e-f4e87ee708d2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Method 1: Full Task A Training...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2768906791.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# 3. Train from scratch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Method 1: Full Task A Training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mhistory_m1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_method1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_A_full_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Save weights for Method 2 comparison\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        # Architecture: Convolution -> ReLU -> MaxPool\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Fully Connected layers\n",
        "        # After two 2x2 pooling layers, 28x28 image becomes 7x7\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes) # Final classification layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 7 * 7) # Flatten\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Helper function to train\n",
        "def train_model(model, train_loader, epochs=5, lr=0.001):\n",
        "    model.train()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    history = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for images, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        history.append(total_loss / len(train_loader))\n",
        "        print(f\"Epoch {epoch+1}, Loss: {history[-1]:.4f}\")\n",
        "    return history\n",
        "\n",
        "# 1. Prepare Large Task A dataset (all 0s and 1s from train_dataset)\n",
        "# Filtering full dataset for digits 0 and 1\n",
        "indices_A = [i for i, target in enumerate(train_dataset.targets) if target in [0, 1]]\n",
        "task_A_full_loader = DataLoader(torch.utils.data.Subset(train_dataset, indices_A), batch_size=32, shuffle=True)\n",
        "\n",
        "# 2. Initialize Model for 2 classes\n",
        "model_method1 = SimpleCNN(num_classes=2)\n",
        "\n",
        "# 3. Train from scratch\n",
        "print(\"Training Method 1: Full Task A Training...\")\n",
        "history_m1 = train_model(model_method1, task_A_full_loader, epochs=5)\n",
        "\n",
        "# Save weights for Method 2 comparison\n",
        "torch.save(model_method1.state_dict(), 'model_method1.pth')\n",
        "# Convert support lists to DataLoaders\n",
        "support_A_loader = DataLoader(support_A, batch_size=4, shuffle=True)\n",
        "support_B_loader = DataLoader(support_B, batch_size=4, shuffle=True)\n",
        "\n",
        "# --- Sub-Method A: Random Init + Fine-tune on Task A Support ---\n",
        "model_ft_random = SimpleCNN(num_classes=2)\n",
        "print(\"\\nFine-tuning Method 2a: Random Init on Support A\")\n",
        "hist_2a = train_model(model_ft_random, support_A_loader, epochs=10)\n",
        "\n",
        "# --- Sub-Method B: Task A Weights + Fine-tune on Task B Support ---\n",
        "model_ft_transfer = SimpleCNN(num_classes=2)\n",
        "model_ft_transfer.load_state_dict(torch.load('model_method1.pth'))\n",
        "# Since Task B is also 2 classes (2,3), we don't need to replace fc2, just re-train\n",
        "print(\"\\nFine-tuning Method 2b: Transfer Task A -> Task B Support\")\n",
        "hist_2b = train_model(model_ft_transfer, support_B_loader, epochs=10)\n",
        "\n",
        "# --- Sub-Method C: All-Digit Pre-train + Fine-tune on Task A ---\n",
        "# 1. Pre-train on all 10 digits (Simulated here)\n",
        "model_full = SimpleCNN(num_classes=10)\n",
        "full_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "train_model(model_full, full_loader, epochs=2) # Pre-training phase\n",
        "\n",
        "# 2. Modify for Binary Task A\n",
        "model_full.fc2 = nn.Linear(128, 2) # Replacing the head\n",
        "print(\"\\nFine-tuning Method 2c: Pre-trained -> Task A Support\")\n",
        "hist_2c = train_model(model_full, support_A_loader, epochs=10)\n",
        "\n",
        "# --- Plotting Learning Curves ---\n",
        "plt.plot(hist_2a, label='Random Init (Task A)')\n",
        "plt.plot(hist_2b, label='Transfer (A -> B)')\n",
        "plt.plot(hist_2c, label='Pre-trained (All -> A)')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Learning Curves Comparison')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LpzPYTkkXH9Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "outputId": "336ceb44-adba-41af-8ceb-2c61f05d46bb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Method 1: Full Task A Training...\n",
            "Epoch 1, Loss: 0.0123\n",
            "Epoch 2, Loss: 0.0024\n",
            "Epoch 3, Loss: 0.0010\n",
            "Epoch 4, Loss: 0.0014\n",
            "Epoch 5, Loss: 0.0016\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'support_B' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2344676367.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m# Convert support lists to DataLoaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0msupport_A_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupport_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0msupport_B_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupport_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# --- Sub-Method A: Random Init + Fine-tune on Task A Support ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'support_B' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the end compare performance of all this models and methods using the Query Set.\n",
        "\n",
        "Also plot the learning curve vs epoch for all the methods\n",
        "\n",
        "Make a table and fill in the values of different evaluation metrics you learned in previous lectures."
      ],
      "metadata": {
        "id": "68yPic0PXeR_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k42bvzgsaHSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some Theoritical Questions :\n",
        "\n",
        "1.   Which strategy in Method 2 works best and why do you feel so ?\n",
        "2.   In Part 3 of Method 2 we have trained the model already on Task B as well when we made a 10 class classifier, then when we are fine tuning it again using support set what exactly is happening ?\n",
        "3.   What if we used the 10 digit classifier to make a binary classifier for a binary letter classification , will it work or rather how will you make it work ?\n",
        "4.   Where exactly have we used Meta Learning, in which approach? Have we even used it ?\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Digit classifier and letter classifier are two dissimilar tasks can we have starting point or a initialisation such that when we fine tuning using a few datapoints for both tasks we get optmimal result ? This is what we will try to do in MAML ?\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Think on them sincerely , would love to read your answers!\n",
        "\n",
        "\n",
        "1)The Pre-trained (All $\\rightarrow$ A) strategy (Sub-method C) typically works best.Breadth of Features: When the model is trained on all 10 digits, it learns a very diverse set of visual features (loops in '8', straight lines in '1', curves in '6').Feature Reuse: When you fine-tune this model for a binary task (like 0 vs 1), the \"backbone\" (convolutional layers) already knows how to detect the necessary shapes. It only needs to learn how to map those shapes to two specific categories.Data Efficiency: It requires much less data to reach high accuracy compared to the Random Initialization approach, which has to learn what an \"edge\" or \"curve\" is from scratch using only a handful of images.\n",
        "\n",
        "2)Even though the 10-class classifier already \"knows\" what a '2' and a '3' (Task B) look like, the fine-tuning process is doing two specific things:\n",
        "\n",
        "Head Replacement: We removed the original 10-way output layer and replaced it with a 2-way layer. This new layer starts with random weights. Fine-tuning \"teaches\" this new layer how to interpret the features from the backbone.\n",
        "\n",
        "Domain Adaptation: The \"support set\" provides a specific context. Fine-tuning adjusts the weights of the model (often with a smaller learning rate) to minimize error specifically for the binary distinction, effectively \"narrowing the focus\" of the model from the general digit space to a specific pair.\n",
        "\n",
        "3)Yes, it will work, but with some caveats. This is a classic example of Cross-Domain Transfer Learning.\n",
        "\n",
        "Will it work? Not perfectly out of the box. A model trained on digits knows \"roundness\" (from '0'), which helps with the letter 'O', but it hasn't seen the complex intersections of a letter like 'K' or 'W'.\n",
        "\n",
        "How to make it work: * Freeze the Backbone: Keep the convolutional layers from the digit model (they already detect basic edges).\n",
        "\n",
        "New Head: Replace the final layer with a new one for your letters.\n",
        "\n",
        "Fine-tune: Train on the letter dataset. Because digits and letters share \"low-level features\" (lines and curves), the model will learn much faster than a model starting from zero.\n",
        "\n",
        "4)Technically, we have not implemented a true Meta-Learning algorithm (like MAML or Prototypical Networks) in this specific code. Instead, we have used Transfer Learning.\n",
        "\n",
        "The Difference: * Transfer Learning (What we did): We took knowledge from Task A and applied it to Task B.\n",
        "\n",
        "Meta-Learning: This would involve training the model specifically to learn how to learn. In a Meta-Learning approach, the model would be optimized across thousands of different tasks so that when it sees a new task (like Task E), it can adapt in just 1 or 2 gradient steps.\n",
        "\n",
        "The Connection: Our setup of \"Support\" and \"Query\" sets is the structure used in Meta-Learning. By organizing data this way, we have prepared the environment for Meta-Learning, even though our training method (Standard SGD/Adam) was traditional fine-tuning.\n",
        "\n",
        "\n",
        "Final Answer\n",
        "\n",
        "While standard Transfer Learning (what we did in the previous blocks) assumes that a model trained on one task (digits) will provide a good starting point for a similar task, it often struggles when tasks are dissimilar (like digits vs. letters). MAML changes the goal: instead of finding a starting point that is good for one task, it finds a starting point that is easy to change for any task.1. The Search for the \"Optimal Initialisation\"In your example of digits vs. letters, a standard pre-trained model might be \"too specialized\" in digits. MAML seeks a parameter configuration $\\theta$ that is sensitive to the gradients of both tasks.The Logic: You want to find a set of weights where a small \"nudge\" (gradient update) in the direction of letters makes it a great letter classifier, and a small nudge toward digits makes it a great digit classifier.The Analogy: Imagine standing on a hill between two valleys (Digits and Letters). Transfer Learning puts you at the bottom of the Digit valley; to get to Letters, you have to climb all the way out. MAML puts you on the peak of the hill, so you can run down into either valley with very little effort.2. Is this what we do in MAML?Exactly. MAML is designed to optimize for fast adaptation. Here is the formal breakdown of how it achieves that:The Two-Step Optimization (Inner and Outer Loops)MAML uses a \"nested\" logic that mimics your experiment:The Inner Loop (Task-Specific): For a specific task (e.g., Letter classification), the model takes a few steps of gradient descent using the Support Set.The Outer Loop (Meta-Optimization): The model looks at how well it performed on the Query Set after those steps. It then updates the initial weights ($\\theta$) to ensure that next time, the inner loop update is even more effective.The Meta-Learning FormulaThe goal is to minimize the loss across a variety of tasks $T_i$:$$\\min_{\\theta} \\sum_{T_i \\sim P(T)} \\mathcal{L}_{T_i}(f_{\\theta_i'})$$Where $\\theta_i'$ are the weights after adapting to task $i$:$$\\theta_i' = \\theta - \\alpha \\nabla_{\\theta} \\mathcal{L}_{T_i}(f_{\\theta})$$Application: In your exam, if asked what $\\theta$ represents in MAML, it is the Meta-Initialisation.3. Digit vs. Letter: Why MAML works better hereIf you use the 10-digit classifier as an initialization for letters, you might encounter Negative Transferâ€”where the specific features of digits (like the hole in an '8') actually confuse the model when it tries to learn an 'A'.MAML avoids this because:It doesn't learn \"This is a 0\"; it learns \"These are the types of features (lines/curves) that are useful for distinguishing shapes.\"It preserves High-Level Plasticity, meaning the model remains flexible enough to be \"molded\" into a letter classifier using just 5 or 10 examples (Few-shot learning).\n",
        "\n"
      ],
      "metadata": {
        "id": "j7DID45NYPVe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ALL THE BEST !"
      ],
      "metadata": {
        "id": "TVWntvGLXtZi"
      }
    }
  ]
}