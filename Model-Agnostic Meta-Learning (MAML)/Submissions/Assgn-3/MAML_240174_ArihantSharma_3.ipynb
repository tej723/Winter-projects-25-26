{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“š Assignment: Transfer Learning & The Power of Initialization\n",
        "## Building Intuition for MAML\n",
        "\n",
        "**Learning Objectives:**\n",
        "- Understand why initialization matters for few-shot learning\n",
        "- Experience the difference between various pre-training strategies\n",
        "- Develop intuition for what MAML tries to optimize"
      ],
      "metadata": {
        "id": "GVBnQU7-LWJ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Advice on using LLM's**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Avoid it , but unfortunately we cannot stop you from using it , dont ask it everything more you think on your own the better , but whenever you take in a code from it , understand how that part fits in the current code , is there some optimization it did on its own, node it down or comment it in the code."
      ],
      "metadata": {
        "id": "MGwIvFz-OBQs"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2534125",
        "outputId": "df497b72-9d8f-479e-d23f-bb98c6cac781"
      },
      "source": [
        "!pip install -q torch torchvision matplotlib numpy\n",
        "\n",
        "#Understand what does each of this import do , see what all functions this hold\n",
        "#whenever you want to implement something think which of this would you use and refer to its doc for the syntax\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "print(\"âœ… Setup complete!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Setup complete!\n",
            "PyTorch version: 2.9.0+cpu\n",
            "CUDA available: False\n",
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“Š Part A: Dataset Preparation\n",
        "\n",
        "We'll use **MNIST** for simplicity (or you can use Omniglot if you prefer).\n",
        "\n",
        "**Your Task:**\n",
        "- Split MNIST into 5 tasks (Tasks A-E), each with 2 digit classes\n",
        "- For example: Task A = {0, 1}, Task B = {2, 3}, etc."
      ],
      "metadata": {
        "id": "Jpe4MlEpLnRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# think on the architecture of the model as discussed in class\n",
        "#general flow -> convolution->relu->maxpooling and so on , in the end some fully connected layers then final classification\n",
        "# Refer to the 60 minute pytorch implementation section of 'neural networks'\n",
        "\n",
        "\n",
        "#Implement the class or the model here\n",
        "#fill in the objects(layers) and methods(forward pass)"
      ],
      "metadata": {
        "id": "fhsb9ffDO-Xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now since the model is ready we decide how do we want to train it :\n",
        "\n",
        "First Do normal classification on large dataset of Task A - 0 & 1.\n",
        "\n",
        "The we will do fine tuning\n",
        "\n",
        "1.   Random Initialisation and then fine tune using support dataset, say we do this for task A which were 0 & 1 digits (save this)\n",
        "2.   Take the above model weights and fine tune it on the support dataset for some other task , say B(2's & 3's)\n",
        "3.   First train the model on all combined train dataset for all 10 digits(from all tasks A,B,C,D,E), then save it and then fine tune it on support dataset on to make a binary classifier , any 1 task say A here now digits will be classified. 0 class->0 digit , 1->1.\n",
        "\n",
        "While moving from one model to other , think what layers do i need to keep and what do i need to remove.\n",
        "\n"
      ],
      "metadata": {
        "id": "KVpBklwxPpj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Method 1 - Fully Trained Binary Classifier on Task A"
      ],
      "metadata": {
        "id": "nCVBCeVxXFQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Method 2 - Fine Tuning all 3 methods"
      ],
      "metadata": {
        "id": "LpzPYTkkXH9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the end compare performance of all this models and methods using the Query Set.\n",
        "\n",
        "Also plot the learning curve vs epoch for all the methods\n",
        "\n",
        "Make a table and fill in the values of different evaluation metrics you learned in previous lectures."
      ],
      "metadata": {
        "id": "68yPic0PXeR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Analysis and Plots"
      ],
      "metadata": {
        "id": "k42bvzgsaHSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some Theoritical Questions :\n",
        "\n",
        "1.   Which strategy in Method 2 works best and why do you feel so ?\n",
        "2.   In Part 3 of Method 2 we have trained the model already on Task B as well when we made a 10 class classifier, then when we are fine tuning it again using support set what exactly is happening ?\n",
        "3.   What if we used the 10 digit classifier to make a binary classifier for a binary letter classification , will it work or rather how will you make it work ?\n",
        "4.   Where exactly have we used Meta Learning, in which approach? Have we even used it ?\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Digit classifier and letter classifier are two dissimilar tasks can we have starting point or a initialisation such that when we fine tuning using a few datapoints for both tasks we get optmimal result ? This is what we will try to do in MAML ?\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Think on them sincerely , would love to read your answers!\n",
        "\n"
      ],
      "metadata": {
        "id": "j7DID45NYPVe"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "686f403c",
        "outputId": "46816842-cd72-4f19-e78f-8acc31490aad"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(), # Converts PIL Image or numpy.ndarray to tensor\n",
        "    transforms.Normalize((0.1307,), (0.3081,)) # Normalize with MNIST mean and std\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "test_dataset = torchvision.datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "# we get a special parameter while loading which is 'background'\n",
        "#refer to document for what it means and how to use it\n",
        "# NOTE: The 'background' parameter is not a standard parameter for torchvision.datasets.MNIST.\n",
        "# It has been omitted from the dataset loading calls.\n",
        "\n",
        "print(f\"âœ… MNIST loaded: {len(train_dataset)} train, {len(test_dataset)} test images\")\n",
        "\n",
        "# TODO: Define your task structure\n",
        "# We'll split 10 digits into 5 tasks, each with 2 classes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.91M/9.91M [00:00<00:00, 147MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.9k/28.9k [00:00<00:00, 37.3MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.65M/1.65M [00:00<00:00, 69.8MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.54k/4.54k [00:00<00:00, 9.26MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… MNIST loaded: 60000 train, 10000 test images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05f2dca0"
      },
      "source": [
        "# TODO: Define your task structure\n",
        "# We'll split 10 digits into 5 tasks, each with 2 classes\n",
        "\n",
        "task_definitions = {\n",
        "    'A': [0, 1],\n",
        "    'B': [2, 3],\n",
        "    'C': [4, 5],\n",
        "    'D': [6, 7],\n",
        "    'E': [8, 9]\n",
        "}\n",
        "\n",
        "#Below function should take the given inputs and split the main dataset with the given input classes into train,support and query.\n",
        "def create_task_datasets(dataset, task_classes, n_train=15, n_support=5, n_query=10):\n",
        "    \"\"\"\n",
        "    Create train, support, and query sets for a specific task.\n",
        "\n",
        "    Args:\n",
        "        dataset: Full MNIST dataset\n",
        "        task_classes: List of class labels for this task [e.g., [0, 1]]\n",
        "        n_train: Number of training examples per class\n",
        "        n_support: Number of support examples per class (for fine-tuning)\n",
        "        n_query: Number of query examples per class (for testing)\n",
        "\n",
        "    Returns:\n",
        "        train_data, support_data, query_data (each is list of (image, label) tuples\n",
        "    \"\"\"\n",
        "# TODO: Implement this function\n",
        "# HINT: Filter dataset to only include examples from task_classes\n",
        "# HINT: Split into train/support/query sets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "891b4336"
      },
      "source": [
        "task_definitions = {\n",
        "    'A': [0, 1],\n",
        "    'B': [2, 3],\n",
        "    'C': [4, 5],\n",
        "    'D': [6, 7],\n",
        "    'E': [8, 9]\n",
        "}\n",
        "\n",
        "def create_task_datasets(dataset, task_classes, n_train=15, n_support=5, n_query=10):\n",
        "    \"\"\"\n",
        "    Create train, support, and query sets for a specific task.\n",
        "\n",
        "    Args:\n",
        "        dataset: Full MNIST dataset\n",
        "        task_classes: List of class labels for this task [e.g., [0, 1]]\n",
        "        n_train: Number of training examples per class\n",
        "        n_support: Number of support examples per class (for fine-tuning)\n",
        "        n_query: Number of query examples per class (for testing)\n",
        "\n",
        "    Returns:\n",
        "        train_data, support_data, query_data (each is list of (image, label) tuples\n",
        "    \"\"\"\n",
        "    task_data_by_class = defaultdict(list)\n",
        "    for img, label in dataset:\n",
        "        if label in task_classes:\n",
        "            task_data_by_class[label].append((img, label))\n",
        "\n",
        "    train_data = []\n",
        "    support_data = []\n",
        "    query_data = []\n",
        "\n",
        "    for class_label in task_classes:\n",
        "        class_samples = task_data_by_class[class_label]\n",
        "        random.shuffle(class_samples)\n",
        "\n",
        "        # Ensure we have enough samples for each set\n",
        "        if len(class_samples) < (n_train + n_support + n_query):\n",
        "            # Handle cases where there aren't enough samples for specified counts\n",
        "            # For now, we'll just take what's available up to the limit\n",
        "            print(f\"Warning: Not enough samples for class {class_label}. Required: {n_train + n_support + n_query}, Available: {len(class_samples)}\")\n",
        "\n",
        "        train_data.extend(class_samples[:n_train])\n",
        "        support_data.extend(class_samples[n_train : n_train + n_support])\n",
        "        query_data.extend(class_samples[n_train + n_support : n_train + n_support + n_query])\n",
        "\n",
        "    return train_data, support_data, query_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ecfdec8",
        "outputId": "b06c444c-8c37-4156-f966-8502dc887efe"
      },
      "source": [
        "train_A, support_A, query_A = create_task_datasets(train_dataset, task_definitions['A'])\n",
        "print(f\"Task A - Train: {len(train_A)}, Support: {len(support_A)}, Query: {len(query_A)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task A - Train: 30, Support: 10, Query: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7806282",
        "outputId": "ad6052e9-a8a6-47b9-b643-a45c3135bab4"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(Net, self).__init__()\n",
        "        # First convolutional block\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1) # Input channels: 1 (MNIST is grayscale)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Second convolutional block\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Fully connected layers\n",
        "        # Calculate the size of the flattened features\n",
        "        # Input image size: 28x28\n",
        "        # After pool1: (28/2) x (28/2) = 14x14\n",
        "        # After pool2: (14/2) x (14/2) = 7x7\n",
        "        # Number of channels after conv2: 64\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128) # 64 channels * 7 * 7 spatial dimensions\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 7 * 7) # Flatten the output for the fully connected layers\n",
        "        x = self.relu3(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "print(\"âœ… CNN model class `Net` defined successfully!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… CNN model class `Net` defined successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eafb572",
        "outputId": "a3f62ef2-1aa3-4994-d357-260008612136"
      },
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.data[idx]\n",
        "        # Assuming images are already tensors and labels are integers\n",
        "        # If labels need to be adjusted (e.g., 0 and 1 instead of 0 and 1 for specific tasks),\n",
        "        # this is where you'd do it. For Task A (0,1), it's straightforward.\n",
        "        return image, label\n",
        "\n",
        "# 1. Instantiate the Net model with num_classes=2\n",
        "model_method1 = Net(num_classes=2).to(device)\n",
        "\n",
        "# 2. Define a loss function and an optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_method1.parameters(), lr=0.001)\n",
        "\n",
        "# 3. Create a DataLoader for the train_A dataset\n",
        "train_dataset_method1 = CustomDataset(train_A)\n",
        "train_loader_method1 = DataLoader(train_dataset_method1, batch_size=16, shuffle=True)\n",
        "\n",
        "# Parameters for training\n",
        "epochs = 10\n",
        "\n",
        "# Lists to store training metrics\n",
        "train_losses_method1 = []\n",
        "train_accuracies_method1 = []\n",
        "\n",
        "print(f\"\\nðŸš€ Starting training for Method 1 on Task A for {epochs} epochs...\")\n",
        "\n",
        "# 4. Train the model\n",
        "for epoch in range(epochs):\n",
        "    model_method1.train()\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(train_loader_method1):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model_method1(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_predictions += labels.size(0)\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader_method1)\n",
        "    epoch_accuracy = 100 * correct_predictions / total_predictions\n",
        "    train_losses_method1.append(epoch_loss)\n",
        "    train_accuracies_method1.append(epoch_accuracy)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
        "\n",
        "print(\"âœ… Training for Method 1 complete!\")\n",
        "\n",
        "# 6. Save the state dictionary of the trained model\n",
        "torch.save(model_method1.state_dict(), 'model_method1.pth')\n",
        "print(\"Model weights for Method 1 saved to 'model_method1.pth'\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸš€ Starting training for Method 1 on Task A for 10 epochs...\n",
            "Epoch 1/10, Loss: 0.5870, Accuracy: 70.00%\n",
            "Epoch 2/10, Loss: 0.2199, Accuracy: 100.00%\n",
            "Epoch 3/10, Loss: 0.0508, Accuracy: 100.00%\n",
            "Epoch 4/10, Loss: 0.0308, Accuracy: 100.00%\n",
            "Epoch 5/10, Loss: 0.0034, Accuracy: 100.00%\n",
            "Epoch 6/10, Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch 7/10, Loss: 0.0041, Accuracy: 100.00%\n",
            "Epoch 8/10, Loss: 0.0031, Accuracy: 100.00%\n",
            "Epoch 9/10, Loss: 0.0014, Accuracy: 100.00%\n",
            "Epoch 10/10, Loss: 0.0002, Accuracy: 100.00%\n",
            "âœ… Training for Method 1 complete!\n",
            "Model weights for Method 1 saved to 'model_method1.pth'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82785f1d",
        "outputId": "95d26bb5-c217-4bbf-abe2-8dbf9d45558d"
      },
      "source": [
        "model_method2_1 = Net(num_classes=2).to(device)\n",
        "\n",
        "criterion_2_1 = nn.CrossEntropyLoss()\n",
        "optimizer_2_1 = optim.Adam(model_method2_1.parameters(), lr=0.001)\n",
        "\n",
        "support_dataset_2_1 = CustomDataset(support_A)\n",
        "support_loader_2_1 = DataLoader(support_dataset_2_1, batch_size=len(support_A), shuffle=True)\n",
        "\n",
        "epochs_2_1 = 50 # Fine-tuning usually requires fewer epochs or more targeted training\n",
        "\n",
        "train_losses_method2_1 = []\n",
        "train_accuracies_method2_1 = []\n",
        "\n",
        "print(f\"\\nðŸš€ Starting fine-tuning for Method 2.1 (Random Init + Task A Support) for {epochs_2_1} epochs...\")\n",
        "\n",
        "for epoch in range(epochs_2_1):\n",
        "    model_method2_1.train()\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    for inputs, labels in support_loader_2_1:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer_2_1.zero_grad()\n",
        "\n",
        "        outputs = model_method2_1(inputs)\n",
        "        loss = criterion_2_1(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer_2_1.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_predictions += labels.size(0)\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(support_loader_2_1)\n",
        "    epoch_accuracy = 100 * correct_predictions / total_predictions\n",
        "    train_losses_method2_1.append(epoch_loss)\n",
        "    train_accuracies_method2_1.append(epoch_accuracy)\n",
        "\n",
        "    if (epoch + 1) % 10 == 0 or epoch == epochs_2_1 - 1:\n",
        "        print(f\"Epoch {epoch+1}/{epochs_2_1}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
        "\n",
        "print(\"âœ… Fine-tuning for Method 2.1 complete!\")\n",
        "\n",
        "torch.save(model_method2_1.state_dict(), 'model_method2_1.pth')\n",
        "print(\"Model weights for Method 2.1 saved to 'model_method2_1.pth'\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸš€ Starting fine-tuning for Method 2.1 (Random Init + Task A Support) for 50 epochs...\n",
            "Epoch 10/50, Loss: 0.0036, Accuracy: 100.00%\n",
            "Epoch 20/50, Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch 30/50, Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch 40/50, Loss: 0.0000, Accuracy: 100.00%\n",
            "Epoch 50/50, Loss: 0.0000, Accuracy: 100.00%\n",
            "âœ… Fine-tuning for Method 2.1 complete!\n",
            "Model weights for Method 2.1 saved to 'model_method2_1.pth'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72b94e2b",
        "outputId": "dc043f8e-bcd0-4d94-a444-3f58d5c07595"
      },
      "source": [
        "print(\"\\n--- Method 2.2: Transfer from Task A to Task B ---\")\n",
        "\n",
        "# 1. Generate support_B and query_B datasets\n",
        "train_B, support_B, query_B = create_task_datasets(train_dataset, task_definitions['B'])\n",
        "print(f\"Task B - Train: {len(train_B)}, Support: {len(support_B)}, Query: {len(query_B)}\")\n",
        "\n",
        "# 2. Instantiate a *new* Net model for 2 classes\n",
        "model_method2_2 = Net(num_classes=2).to(device)\n",
        "\n",
        "# 3. Load the state dictionary from model_method1.pth (pre-trained on Task A)\n",
        "try:\n",
        "    model_method2_2.load_state_dict(torch.load('model_method1.pth'))\n",
        "    print(\"âœ… Loaded weights from 'model_method1.pth' successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error loading model_method1.pth: {e}\")\n",
        "    print(\"Ensure Method 1 was executed and the model was saved.\")\n",
        "\n",
        "# 2.1. Remap labels in support_B (2 and 3) to 0 and 1\n",
        "# This creates a new list of (image, remapped_label) tuples\n",
        "remapped_support_B = []\n",
        "for img, label in support_B:\n",
        "    if label == 2:\n",
        "        remapped_support_B.append((img, 0))\n",
        "    elif label == 3:\n",
        "        remapped_support_B.append((img, 1))\n",
        "# Also remap query_B for evaluation later\n",
        "remapped_query_B = []\n",
        "for img, label in query_B:\n",
        "    if label == 2:\n",
        "        remapped_query_B.append((img, 0))\n",
        "    elif label == 3:\n",
        "        remapped_query_B.append((img, 1))\n",
        "\n",
        "# 4. Define a loss function and an optimizer (smaller learning rate for fine-tuning)\n",
        "criterion_2_2 = nn.CrossEntropyLoss()\n",
        "optimizer_2_2 = optim.Adam(model_method2_2.parameters(), lr=0.0001) # Smaller LR\n",
        "\n",
        "# 5. Create a DataLoader for the remapped support_B dataset\n",
        "support_dataset_2_2 = CustomDataset(remapped_support_B)\n",
        "support_loader_2_2 = DataLoader(support_dataset_2_2, batch_size=len(remapped_support_B), shuffle=True)\n",
        "\n",
        "epochs_2_2 = 50 # Fine-tuning usually requires fewer epochs or more targeted training\n",
        "\n",
        "train_losses_method2_2 = []\n",
        "train_accuracies_method2_2 = []\n",
        "\n",
        "print(f\"ðŸš€ Starting fine-tuning for Method 2.2 (Transfer from A to B) for {epochs_2_2} epochs...\")\n",
        "\n",
        "# 6. Train the model for a few epochs on the support_B dataset\n",
        "for epoch in range(epochs_2_2):\n",
        "    model_method2_2.train()\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    for inputs, labels in support_loader_2_2:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer_2_2.zero_grad()\n",
        "\n",
        "        outputs = model_method2_2(inputs)\n",
        "        loss = criterion_2_2(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer_2_2.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_predictions += labels.size(0)\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(support_loader_2_2)\n",
        "    epoch_accuracy = 100 * correct_predictions / total_predictions\n",
        "    train_losses_method2_2.append(epoch_loss)\n",
        "    train_accuracies_method2_2.append(epoch_accuracy)\n",
        "\n",
        "    if (epoch + 1) % 10 == 0 or epoch == epochs_2_2 - 1:\n",
        "        print(f\"Epoch {epoch+1}/{epochs_2_2}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
        "\n",
        "print(\"âœ… Fine-tuning for Method 2.2 complete!\")\n",
        "\n",
        "# 8. Save the model's state dictionary\n",
        "torch.save(model_method2_2.state_dict(), 'model_method2_2.pth')\n",
        "print(\"Model weights for Method 2.2 saved to 'model_method2_2.pth'\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Method 2.2: Transfer from Task A to Task B ---\n",
            "Task B - Train: 30, Support: 10, Query: 20\n",
            "âœ… Loaded weights from 'model_method1.pth' successfully.\n",
            "ðŸš€ Starting fine-tuning for Method 2.2 (Transfer from A to B) for 50 epochs...\n",
            "Epoch 10/50, Loss: 0.8752, Accuracy: 70.00%\n",
            "Epoch 20/50, Loss: 0.5991, Accuracy: 80.00%\n",
            "Epoch 30/50, Loss: 0.3114, Accuracy: 80.00%\n",
            "Epoch 40/50, Loss: 0.1274, Accuracy: 100.00%\n",
            "Epoch 50/50, Loss: 0.0616, Accuracy: 100.00%\n",
            "âœ… Fine-tuning for Method 2.2 complete!\n",
            "Model weights for Method 2.2 saved to 'model_method2_2.pth'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4446fecb",
        "outputId": "1eccb97b-5427-4ddd-ae36-2ec52cb125c4"
      },
      "source": [
        "print(\"\\n--- Method 2.3: Pre-training on All Digits + Fine-tuning on Task A Support Set ---\")\n",
        "\n",
        "# 1. Create a combined dataset of all 10 digits from the train_dataset\n",
        "# The original 'train_dataset' already contains all 10 digits, so we can use it directly.\n",
        "full_train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# 2. Instantiate a new Net model with num_classes=10 (for the pre-training phase)\n",
        "model_pretrained_all_digits = Net(num_classes=10).to(device)\n",
        "\n",
        "# Define loss and optimizer for pre-training\n",
        "criterion_pretrain = nn.CrossEntropyLoss()\n",
        "optimizer_pretrain = optim.Adam(model_pretrained_all_digits.parameters(), lr=0.001)\n",
        "\n",
        "epochs_pretrain = 5 # Pre-train for a few epochs\n",
        "\n",
        "print(f\"\\nðŸš€ Starting pre-training for Method 2.3 (All Digits) for {epochs_pretrain} epochs...\")\n",
        "\n",
        "# Lists to store training metrics for pre-training\n",
        "pretrain_losses = []\n",
        "pretrain_accuracies = []\n",
        "\n",
        "# 3. Train this 10-class classifier on the combined dataset\n",
        "for epoch in range(epochs_pretrain):\n",
        "    model_pretrained_all_digits.train()\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    for inputs, labels in full_train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer_pretrain.zero_grad()\n",
        "\n",
        "        outputs = model_pretrained_all_digits(inputs)\n",
        "        loss = criterion_pretrain(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer_pretrain.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_predictions += labels.size(0)\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(full_train_loader)\n",
        "    epoch_accuracy = 100 * correct_predictions / total_predictions\n",
        "    pretrain_losses.append(epoch_loss)\n",
        "    pretrain_accuracies.append(epoch_accuracy)\n",
        "\n",
        "    print(f\"Pre-train Epoch {epoch+1}/{epochs_pretrain}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
        "\n",
        "print(\"âœ… Pre-training for Method 2.3 complete!\")\n",
        "\n",
        "# Save pre-trained model weights\n",
        "torch.save(model_pretrained_all_digits.state_dict(), 'model_pretrained_all_digits.pth')\n",
        "print(\"Pre-trained model weights saved to 'model_pretrained_all_digits.pth'\")\n",
        "\n",
        "# --- Fine-tuning Phase ---\n",
        "print(\"\\nðŸš€ Starting fine-tuning for Method 2.3 on Task A Support Set...\")\n",
        "\n",
        "# 4. Instantiate another Net model for num_classes=2 (for fine-tuning on Task A)\n",
        "model_method2_3 = Net(num_classes=2).to(device)\n",
        "\n",
        "# 5. Load the weights from model_pretrained_all_digits.pth into the feature extraction layers\n",
        "#    We load all layers except the final classification layer (fc2) as its output size differs.\n",
        "pretrained_state_dict = torch.load('model_pretrained_all_digits.pth')\n",
        "filtered_state_dict = {k: v for k, v in pretrained_state_dict.items() if 'fc2' not in k}\n",
        "model_method2_3.load_state_dict(filtered_state_dict, strict=False)\n",
        "print(\"âœ… Loaded pre-trained weights (excluding final classifier) for fine-tuning.\")\n",
        "\n",
        "# Optional: Freeze feature extraction layers, only train the new head\n",
        "# for param in model_method2_3.parameters():\n",
        "#     param.requires_grad = False\n",
        "# for param in model_method2_3.fc2.parameters():\n",
        "#     param.requires_grad = True # Ensure the new head is trainable\n",
        "\n",
        "# 6. Define a new loss function and optimizer for the fine-tuning phase\n",
        "criterion_2_3 = nn.CrossEntropyLoss()\n",
        "optimizer_2_3 = optim.Adam(model_method2_3.parameters(), lr=0.00005) # Very small LR for fine-tuning\n",
        "\n",
        "# 7. Create a DataLoader for the support_A dataset (labels 0 and 1 are already correct)\n",
        "support_dataset_2_3 = CustomDataset(support_A)\n",
        "support_loader_2_3 = DataLoader(support_dataset_2_3, batch_size=len(support_A), shuffle=True)\n",
        "\n",
        "epochs_2_3 = 50 # Fine-tune for a few epochs\n",
        "\n",
        "# Lists to store training metrics for fine-tuning\n",
        "train_losses_method2_3 = []\n",
        "train_accuracies_method2_3 = []\n",
        "\n",
        "# 8. Train the 2-class model on the support_A dataset\n",
        "for epoch in range(epochs_2_3):\n",
        "    model_method2_3.train()\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    for inputs, labels in support_loader_2_3:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer_2_3.zero_grad()\n",
        "\n",
        "        outputs = model_method2_3(inputs)\n",
        "        loss = criterion_2_3(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer_2_3.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_predictions += labels.size(0)\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(support_loader_2_3)\n",
        "    epoch_accuracy = 100 * correct_predictions / total_predictions\n",
        "    train_losses_method2_3.append(epoch_loss)\n",
        "    train_accuracies_method2_3.append(epoch_accuracy)\n",
        "\n",
        "    if (epoch + 1) % 10 == 0 or epoch == epochs_2_3 - 1:\n",
        "        print(f\"Fine-tune Epoch {epoch+1}/{epochs_2_3}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
        "\n",
        "print(\"âœ… Fine-tuning for Method 2.3 complete!\")\n",
        "\n",
        "# 10. Save the final fine-tuned model's state dictionary\n",
        "torch.save(model_method2_3.state_dict(), 'model_method2_3.pth')\n",
        "print(\"Model weights for Method 2.3 saved to 'model_method2_3.pth'\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Method 2.3: Pre-training on All Digits + Fine-tuning on Task A Support Set ---\n",
            "\n",
            "ðŸš€ Starting pre-training for Method 2.3 (All Digits) for 5 epochs...\n",
            "Pre-train Epoch 1/5, Loss: 0.1316, Accuracy: 95.97%\n",
            "Pre-train Epoch 2/5, Loss: 0.0432, Accuracy: 98.69%\n",
            "Pre-train Epoch 3/5, Loss: 0.0292, Accuracy: 99.07%\n",
            "Pre-train Epoch 4/5, Loss: 0.0203, Accuracy: 99.34%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. Pretraining on all 10 digits and then fine-tuning works best because the model learns general digit features first and only needs small adjustments for a specific task, giving better accuracy with less data.\n",
        "\n",
        "\n",
        "2. Fine-tuning sharpens the modelâ€™s decision boundary for only the required classes, adapting general knowledge to a specific binary task.\n",
        "\n",
        "\n",
        "3. Directly no, but it can work by keeping early convolution layers, replacing the final classifier, and fine-tuning on letter data, since low-level visual features are transferable.\n",
        "\n",
        "\n",
        "4. We have not truly used meta-learning. We only used transfer learning and fine-tuning, not a meta-learning algorithm like MAML.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rGwCERo8BRm0"
      }
    }
  ]
}