{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“š Assignment: Transfer Learning & The Power of Initialization\n",
        "## Building Intuition for MAML\n",
        "\n",
        "**Learning Objectives:**\n",
        "- Understand why initialization matters for few-shot learning\n",
        "- Experience the difference between various pre-training strategies\n",
        "- Develop intuition for what MAML tries to optimize"
      ],
      "metadata": {
        "id": "GVBnQU7-LWJ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Advice on using LLM's**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Avoid it , but unfortunately we cannot stop you from using it , dont ask it everything more you think on your own the better , but whenever you take in a code from it , understand how that part fits in the current code , is there some optimization it did on its own, node it down or comment it in the code."
      ],
      "metadata": {
        "id": "MGwIvFz-OBQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch torchvision matplotlib numpy\n",
        "\n",
        "#Understand what does each of this import do , see what all functions this hold\n",
        "#whenever you want to implement something think which of this would you use and refer to its doc for the syntax\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "print(\"âœ… Setup complete!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "iU4HQdjfLjpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“Š Part A: Dataset Preparation\n",
        "\n",
        "We'll use **MNIST** for simplicity (or you can use Omniglot if you prefer).\n",
        "\n",
        "**Your Task:**\n",
        "- Split MNIST into 5 tasks (Tasks A-E), each with 2 digit classes\n",
        "- For example: Task A = {0, 1}, Task B = {2, 3}, etc."
      ],
      "metadata": {
        "id": "Jpe4MlEpLnRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download MNIST\n",
        "transform = transforms.Compose([\n",
        "    # see what different tranformation you can do , one is converting the image into tensor\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(\n",
        "   # fill this\n",
        ")\n",
        "test_dataset = torchvision.datasets.MNIST(\n",
        "    # fill this\n",
        ")\n",
        "# we get a special parameter while loading which is 'background'\n",
        "#refer to document for what it means and how to use it\n",
        "\n",
        "print(f\"âœ… MNIST loaded: {len(train_dataset)} train, {len(test_dataset)} test images\")\n",
        "\n",
        "# TODO: Define your task structure\n",
        "# We'll split 10 digits into 5 tasks, each with 2 classes"
      ],
      "metadata": {
        "id": "CRoLfdMfMgFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Define your task structure\n",
        "# We'll split 10 digits into 5 tasks, each with 2 classes\n",
        "\n",
        "task_definitions = {\n",
        "    #fill this\n",
        "}\n",
        "\n",
        "#Below function should take the given inputs and split the main dataset with the given input classes into train,support and query.\n",
        "def create_task_datasets(dataset, task_classes, n_train=15, n_support=5, n_query=10):\n",
        "    \"\"\"\n",
        "    Create train, support, and query sets for a specific task.\n",
        "\n",
        "    Args:\n",
        "        dataset: Full MNIST dataset\n",
        "        task_classes: List of class labels for this task [e.g., [0, 1]]\n",
        "        n_train: Number of training examples per class\n",
        "        n_support: Number of support examples per class (for fine-tuning)\n",
        "        n_query: Number of query examples per class (for testing)\n",
        "\n",
        "    Returns:\n",
        "        train_data, support_data, query_data (each is list of (image, label) tuples\n",
        "    \"\"\"\n",
        "# TODO: Implement this function\n",
        "# HINT: Filter dataset to only include examples from task_classes\n",
        "# HINT: Split into train/support/query sets"
      ],
      "metadata": {
        "id": "qFKxWXDCMqZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the function\n",
        "\n",
        "train_A, support_A, query_A = create_task_datasets(train_dataset, task_definitions['A'])\n",
        "print(f\"Task A - Train: {len(train_A)}, Support: {len(support_A)}, Query: {len(query_A)}\")"
      ],
      "metadata": {
        "id": "xGdk_eqzNuh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part A (continued): **Build Your Model**\n",
        "\n",
        "**TODO:** Design a simple CNN for digit classification"
      ],
      "metadata": {
        "id": "49q6LHJ1O1wC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# think on the architecture of the model as discussed in class\n",
        "#general flow -> convolution->relu->maxpooling and so on , in the end some fully connected layers then final classification\n",
        "# Refer to the 60 minute pytorch implementation section of 'neural networks'\n",
        "\n",
        "\n",
        "#Implement the class or the model here\n",
        "#fill in the objects(layers) and methods(forward pass)"
      ],
      "metadata": {
        "id": "fhsb9ffDO-Xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now since the model is ready we decide how do we want to train it :\n",
        "\n",
        "First Do normal classification on large dataset of Task A - 0 & 1.\n",
        "\n",
        "The we will do fine tuning\n",
        "\n",
        "1.   Random Initialisation and then fine tune using support dataset, say we do this for task A which were 0 & 1 digits (save this)\n",
        "2.   Take the above model weights and fine tune it on the support dataset for some other task , say B(2's & 3's)\n",
        "3.   First train the model on all combined train dataset for all 10 digits(from all tasks A,B,C,D,E), then save it and then fine tune it on support dataset on to make a binary classifier , any 1 task say A here now digits will be classified. 0 class->0 digit , 1->1.\n",
        "\n",
        "While moving from one model to other , think what layers do i need to keep and what do i need to remove.\n",
        "\n"
      ],
      "metadata": {
        "id": "KVpBklwxPpj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Method 1 - Fully Trained Binary Classifier on Task A"
      ],
      "metadata": {
        "id": "nCVBCeVxXFQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Method 2 - Fine Tuning all 3 methods"
      ],
      "metadata": {
        "id": "LpzPYTkkXH9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the end compare performance of all this models and methods using the Query Set.\n",
        "\n",
        "Also plot the learning curve vs epoch for all the methods\n",
        "\n",
        "Make a table and fill in the values of different evaluation metrics you learned in previous lectures."
      ],
      "metadata": {
        "id": "68yPic0PXeR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Analysis and Plots"
      ],
      "metadata": {
        "id": "k42bvzgsaHSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some Theoritical Questions :\n",
        "\n",
        "1.   Which strategy in Method 2 works best and why do you feel so ?\n",
        "2.   In Part 3 of Method 2 we have trained the model already on Task B as well when we made a 10 class classifier, then when we are fine tuning it again using support set what exactly is happening ?\n",
        "3.   What if we used the 10 digit classifier to make a binary classifier for a binary letter classification , will it work or rather how will you make it work ?\n",
        "4.   Where exactly have we used Meta Learning, in which approach? Have we even used it ?\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Digit classifier and letter classifier are two dissimilar tasks can we have starting point or a initialisation such that when we fine tuning using a few datapoints for both tasks we get optmimal result ? This is what we will try to do in MAML ?\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Think on them sincerely , would love to read your answers!\n",
        "\n"
      ],
      "metadata": {
        "id": "j7DID45NYPVe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ALL THE BEST !"
      ],
      "metadata": {
        "id": "TVWntvGLXtZi"
      }
    }
  ]
}