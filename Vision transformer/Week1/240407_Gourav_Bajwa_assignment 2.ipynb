{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "SUBMISSION INSTRUCTIONS\n",
        "\n",
        "It is recommended that you make a copy of this colab file and then solve the assignment and upload your final notebook on github.\n",
        "\n",
        "Before uploading your downloaded notebook, RENAME the file as rollno_name.ipynb\n",
        "\n",
        "Submission Deadline : 18/12/2025 Thursday EOD i.e before 11:59 PM\n",
        "\n",
        "The deadline is strict and will not be extended, Late submissions are not allowed\n",
        "\n",
        "Note that you have to upload your solution on the github page of the project Vision Transformer and under Week1\n",
        "\n",
        "when submitting your pull request on the submission repo keep title as ViT_name_rollno_assgn2\n",
        "\n",
        "Github Submission repo - https://github.com/electricalengineersiitk/Winter-projects-25-26/tree/main/Vision%20transformer/Week1\n"
      ],
      "metadata": {
        "id": "6UgUdG-lKTZk"
      },
      "id": "6UgUdG-lKTZk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NOTE:\n",
        "Do not use any pre-built functions from any new libraries apart from numpy, matplot-lib, pandas except for importing datasets."
      ],
      "metadata": {
        "id": "27cU1xGjF6mm"
      },
      "id": "27cU1xGjF6mm"
    },
    {
      "cell_type": "markdown",
      "id": "bored-updating",
      "metadata": {
        "id": "bored-updating"
      },
      "source": [
        "#### Importing Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8gATfPCgKVlV"
      },
      "id": "8gATfPCgKVlV"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "prime-observer",
      "metadata": {
        "id": "prime-observer"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "81YcAGXOK2x8"
      },
      "id": "81YcAGXOK2x8"
    },
    {
      "cell_type": "markdown",
      "id": "primary-craps",
      "metadata": {
        "id": "primary-craps"
      },
      "source": [
        "# Importing Datasets\n",
        "\n",
        "Import the MNIST dataset and create X_train, Y_train, and X_test, Y_test.\n",
        "\n",
        "After importing the dataset you can resize it such that you are dealing with only 1000 images of each number.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load dataset\n",
        "(X_train_orig, Y_train_orig), (X_test_orig, Y_test_orig) = mnist.load_data()\n",
        "\n",
        "# Resize: 1000 images per digit\n",
        "X_list, Y_list = [], []\n",
        "for i in range(10):\n",
        "    idx = np.where(Y_train_orig == i)[0][:1000]\n",
        "    X_list.append(X_train_orig[idx])\n",
        "    Y_list.append(Y_train_orig[idx])\n",
        "\n",
        "X_train_raw = np.concatenate(X_list)\n",
        "Y_train_raw = np.concatenate(Y_list)\n",
        "\n",
        "# SHAPE ALIGNMENT: Flatten (28*28=784) and Transpose to (784, 10000)\n",
        "X_train = X_train_raw.reshape(X_train_raw.shape[0], -1).T / 255.\n",
        "X_test = X_test_orig.reshape(X_test_orig.shape[0], -1).T / 255.\n",
        "\n",
        "# One-Hot Encode Labels for shape (10, 10000)\n",
        "def one_hot(Y):\n",
        "    oh = np.zeros((10, Y.size))\n",
        "    oh[Y, np.arange(Y.size)] = 1\n",
        "    return oh\n",
        "\n",
        "Y_train = one_hot(Y_train_raw)\n",
        "Y_test = one_hot(Y_test_orig)\n",
        "\n",
        "print(f\"Original Training Size: {X_train_raw.shape[0]}\")\n",
        "print(f\"New Training Size: {X_train.shape[0]} (Should be 10,000)\")\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"Y_train shape: {Y_train.shape}\")"
      ],
      "metadata": {
        "id": "ou82GN8DFRwE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c62b7ebb-3c7a-48e8-dcd9-66cb04e9533a"
      },
      "id": "ou82GN8DFRwE",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Training Size: 10000\n",
            "New Training Size: 784 (Should be 10,000)\n",
            "X_train shape: (784, 10000)\n",
            "Y_train shape: (10, 10000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "conservative-graphic",
      "metadata": {
        "id": "conservative-graphic"
      },
      "source": [
        "# Model\n",
        "\n",
        "\n",
        "## Initialize parameters Randomly\n",
        "$ W_1 = np.random.randn(n_1, n_0) $\n",
        "\n",
        "$ b_1 = np.zeros((n_1, 1))$\n",
        "\n",
        "$ W_2 = np.random.randn(n_2, n_1) $\n",
        "\n",
        "$ b_2 = np.zeros((n_2, 1))$\n",
        "\n",
        "\n",
        "## Repeat Below Steps for many times :\n",
        "\n",
        "\n",
        "## Forward Propagation\n",
        "\n",
        "$ Z_1 = W_1 * X + B_1 $\n",
        "\n",
        "$ A_1 = f ( Z_1 ) $  \n",
        "\n",
        "$ Z_2 = W2 * A_1 + B_2 $\n",
        "\n",
        "$ A_2 = Softmax( Z_2 ) $\n",
        "\n",
        "## Softmax\n",
        "\n",
        "$ a_i = \\frac{e^{z_i}}{\\sum_{i=k}^ne^{z_k}}$\n",
        "\n",
        "\n",
        "## Cost Function\n",
        "\n",
        "$Loss = - \\sum_{i=k}^{n}[ y_k*log(a_k) ]$\n",
        "\n",
        "$Cost = - \\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{n}[ y_k*log(a_k) ]$\n",
        "\n",
        "\n",
        "\n",
        "## Backward Propagation\n",
        "$dZ_2 = ( A_2 - Y )$\n",
        "\n",
        "$ dW_2 = \\frac{1}{m}. dZ_2 . A_1^T$\n",
        "\n",
        "$ dB_2 = \\frac{1}{m}.sum(dZ_2, 1)$\n",
        "\n",
        "\n",
        "\n",
        "$dZ_1 = W_2^T . dZ_2 * f_1^|(Z_1) $\n",
        "\n",
        "$dW_1 = \\frac{1}{m}.dZ_1.X^T$\n",
        "\n",
        "$dB_1 = \\frac{1}{m}.sum(dZ_1, 1)$\n",
        "\n",
        "\n",
        "## Updating Parameters\n",
        "\n",
        "$ W_2 = W_2 -  \\alpha * \\frac{\\partial Cost }{\\partial W_2}$\n",
        "\n",
        "$ B_2 = B_2 -  \\alpha * \\frac{\\partial Cost }{\\partial B_2}$\n",
        "\n",
        "$ W_1 = W_1 -  \\alpha * \\frac{\\partial Cost }{\\partial W_1}$\n",
        "\n",
        "$ B_1 = B_1 -  \\alpha * \\frac{\\partial Cost }{\\partial B_1}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "private-seven",
      "metadata": {
        "id": "private-seven"
      },
      "source": [
        "# Activation Functions\n",
        "\n",
        "***Now, its your time to implement !***\n",
        "\n",
        "Complete the below functions for Activations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "better-winning",
      "metadata": {
        "id": "better-winning"
      },
      "outputs": [],
      "source": [
        "def tanh(x):\n",
        "    ## Your Code Here ##\n",
        "    return np.tanh(x)\n",
        "\n",
        "    ## Code Ends ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "immune-wright",
      "metadata": {
        "id": "immune-wright"
      },
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "    ## Your Code Here ##\n",
        "    return np.maximum(0, x)\n",
        "    ## Code Ends ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "several-roommate",
      "metadata": {
        "id": "several-roommate"
      },
      "outputs": [],
      "source": [
        "def softmax(z):\n",
        "    ## Your Code Here ##\n",
        "    e_z = np.exp(z - np.max(z, axis=0, keepdims=True))\n",
        "    return e_z / np.sum(e_z, axis=0, keepdims=True)\n",
        "\n",
        "    ## Code Ends ##"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "smart-gambling",
      "metadata": {
        "id": "smart-gambling"
      },
      "source": [
        "The function *derivative_tanh* must return the derivative of tanh.\n",
        "The function *derivative_relu* must return the derivative of ReLU\n",
        "\n",
        "\n",
        "derivative of tanh is given by 1 - tanh^2(x).\n",
        "\n",
        "\n",
        "so, derivative_tanh(x):\n",
        "\n",
        "return 1 - np.power(np.tanh(x), 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "german-stake",
      "metadata": {
        "id": "german-stake"
      },
      "outputs": [],
      "source": [
        "def derivative_tanh(x):\n",
        "    ## Your Code Here ##\n",
        "    return 1 - np.power(np.tanh(x), 2)\n",
        "\n",
        "    ## Code Ends ##\n",
        "\n",
        "def derivative_relu(x):\n",
        "    ## Your Code Here ##\n",
        "    return (x > 0).astype(float)\n",
        "    ## Code Ends ##"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "forward-heading",
      "metadata": {
        "id": "forward-heading"
      },
      "source": [
        "# Initialize Parameters\n",
        "\n",
        "We need to initialize the **W** parameters randomly, and **B** with zeros\n",
        "\n",
        "- np.random.randn(a, b) return a numpy array of shape(a, b) with small random values\n",
        "- For making the values small, we multiply 0.01\n",
        "- np.zeros(a, b) return a numpy array of shape(a, b) with zeros\n",
        "\n",
        "### Why need small weights W?\n",
        "If we initialize weights will large values, then Z = W * X + B, will be large. For functions like tanh and sigmoid, the slope becomes very less for large Z value, thus learning can be very slow.\n",
        "\n",
        "#### We have an increase in the cost function at the beginning while training the model with ReLU activation function.\n",
        "It is because our weights were still very large and it was creating problem for training our model.\n",
        "\n",
        "Multiply weights with 0.001 instead of 0.01, and you will see that the graph becomes normal, with a smooth decrease in cost value.\n",
        "\n",
        "There are many weight initialization techniques available as well, to solve other such problems.\n",
        "\n",
        "Now, We need to return a dictionary containing all the parameters.\n",
        "\n",
        "More about np.random.randn here : https://numpy.org/doc/stable/reference/random/generated/numpy.random.randn.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "several-textbook",
      "metadata": {
        "id": "several-textbook"
      },
      "outputs": [],
      "source": [
        "def initialize_parameters(n_x, n_h, n_y):\n",
        "\n",
        "    ## Complete the code below ##\n",
        "    w1 = np.random.randn(n_h, n_x) * 0.01\n",
        "    b1 = np.zeros((n_h, 1))\n",
        "    w2 = np.random.randn(n_y, n_h) * 0.01\n",
        "    b2 = np.zeros((n_y, 1))\n",
        "    ## Your Code ends ##\n",
        "\n",
        "    parameters = {\n",
        "        \"w1\" : w1,\n",
        "        \"b1\" : b1,\n",
        "        \"w2\" : w2,\n",
        "        \"b2\" : b2\n",
        "    }\n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "listed-sauce",
      "metadata": {
        "id": "listed-sauce"
      },
      "source": [
        "# Forward Propagation\n",
        "\n",
        "We need to impletement the following equation for forward propagation :\n",
        "\n",
        "$ Z_1 = W_1 * X + B_1 $\n",
        "\n",
        "$ A_1 = f ( Z_1 ) $  \n",
        "\n",
        "$ Z_2 = W2 * A_1 + B_2 $\n",
        "\n",
        "$ A_2 = Softmax( Z_2 ) $\n",
        "\n",
        "For f(x), you can use either tanh or ReLU activation function.\n",
        "\n",
        "But also use the same for Backpropagation as well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "described-excess",
      "metadata": {
        "id": "described-excess"
      },
      "outputs": [],
      "source": [
        "def forward_prop(x, parameters):\n",
        "\n",
        "    # To fetch the parameters\n",
        "    w1 = parameters['w1']\n",
        "    b1 = parameters['b1']\n",
        "    w2 = parameters['w2']\n",
        "    b2 = parameters['b2']\n",
        "\n",
        "    ## Complete the Code below : ##\n",
        "    z1 = np.dot(w1, x) + b1\n",
        "    a1 = relu(z1)\n",
        "    z2 = np.dot(w2, a1) + b2\n",
        "    a2 = softmax(z2)\n",
        "\n",
        "    ## Your Code ends ##\n",
        "\n",
        "    # To return our Zs and As\n",
        "    forward_cache = {\n",
        "        \"z1\" : z1,\n",
        "        \"a1\" : a1,\n",
        "        \"z2\" : z2,\n",
        "        \"a2\" : a2\n",
        "    }\n",
        "\n",
        "    return forward_cache"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sharp-tourist",
      "metadata": {
        "id": "sharp-tourist"
      },
      "source": [
        "# Cost Function\n",
        "\n",
        "$Loss = - \\sum_{k=1}^{n}[ y_k*log(a_k) ]$ .. *for 1 observation*\n",
        "\n",
        "$Cost = - \\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{n}[ y_k*log(a_k) ]$  .. *for all m observations*\n",
        "\n",
        "You need to return the cost in the below function\n",
        "\n",
        "You can use np.sum()\n",
        "- np.sum(A, axis = 1, keepdims = True) return the column-wise sum for a matrix A\n",
        "- np.sum(A, axis = 0, keepdims = True) returns the row-wise sum for a matrix A\n",
        "- np.sum(A) returns the summation of all the elements of A\n",
        "\n",
        "*keepdims = True keeps the dimenstion in place. In certain cases, the returned sum can be of shape(m,) instead of shape(m, 1).\n",
        "So, keepdims = True forces it to return the sum in shape(m, 1) instead of shape(m,)*\n",
        "\n",
        "\n",
        "More about np.sum() here : https://numpy.org/doc/stable/reference/generated/numpy.sum.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "conservative-straight",
      "metadata": {
        "id": "conservative-straight"
      },
      "outputs": [],
      "source": [
        "def cost_function(a2, y):\n",
        "\n",
        "    ## Your Code Here ##\n",
        "    m = y.shape[1]\n",
        "    log_probs = np.multiply(y, np.log(a2 + 1e-8))\n",
        "    cost = - (1/m) * np.sum(log_probs)\n",
        "\n",
        "    ## Code Ends ##\n",
        "    return cost"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "configured-draft",
      "metadata": {
        "id": "configured-draft"
      },
      "source": [
        "# Backward Propagation\n",
        "\n",
        "We need to implement the below equations\n",
        "\n",
        "$dZ_2 = ( A_2 - Y )$\n",
        "\n",
        "$ dW_2 = \\frac{1}{m}. dZ_2 . A_2^T$\n",
        "\n",
        "$ dB_2 = \\frac{1}{m}.sum(dZ_2, 1)$\n",
        "\n",
        "\n",
        "\n",
        "$dZ_1 = W_2^T . dZ_2 * f_1^|(Z_1) $\n",
        "\n",
        "$dW_1 = \\frac{1}{m}.dZ_1.A_1^T$\n",
        "\n",
        "$dB_1 = \\frac{1}{m}.sum(dZ_1, 1)$\n",
        "\n",
        "Helper python functions :\n",
        "- A.T returns the transpose of matrix A\n",
        "- np.dot(A, B) returns the matrix multiplication of A and B\n",
        "- A*B returns the element wise multi-plication for A and B\n",
        "- np.sum(A, axis = 1, keepdims = True) return the column-wise sum for a matrix A\n",
        "- np.sum(A, axis = 0, keepdims = True) returns the row-wise sum for a matrix A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "regulation-internship",
      "metadata": {
        "id": "regulation-internship"
      },
      "outputs": [],
      "source": [
        "def backward_prop(x, y, parameters, forward_cache):\n",
        "\n",
        "    m = x.shape[1]\n",
        "\n",
        "    # Fetching our parameters\n",
        "    w1 = parameters['w1']\n",
        "    b1 = parameters['b1']\n",
        "    w2 = parameters['w2']\n",
        "    b2 = parameters['b2']\n",
        "\n",
        "    # Fetching our forward_cache\n",
        "    a1 = forward_cache['a1']\n",
        "    a2 = forward_cache['a2']\n",
        "    z1 = forward_cache['z1']\n",
        "\n",
        "\n",
        "\n",
        "    ## Complete the Code below ##\n",
        "    dz2 = a2 - y\n",
        "    dw2 = (1/m) * np.dot(dz2, a1.T)\n",
        "    db2 = (1/m) * np.sum(dz2, axis=1, keepdims=True)\n",
        "\n",
        "    dz1 = np.dot(w2.T, dz2) * derivative_tanh(a1)\n",
        "    dw1 = (1/m) * np.dot(dz1, x.T)\n",
        "    db1 = (1/m) * np.sum(dz1, axis=1, keepdims=True)\n",
        "    ## Your Code ends ##\n",
        "\n",
        "    ## Returning the Gradients\n",
        "    gradients = {\n",
        "        \"dw1\" : dw1,\n",
        "        \"db1\" : db1,\n",
        "        \"dw2\" : dw2,\n",
        "        \"db2\" : db2\n",
        "    }\n",
        "\n",
        "    return gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "soviet-sentence",
      "metadata": {
        "id": "soviet-sentence"
      },
      "source": [
        "# Update Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "convertible-samba",
      "metadata": {
        "id": "convertible-samba"
      },
      "outputs": [],
      "source": [
        "def update_parameters(parameters, gradients, learning_rate):\n",
        "\n",
        "    # Fetching our parameters\n",
        "    w1 = parameters['w1']\n",
        "    b1 = parameters['b1']\n",
        "    w2 = parameters['w2']\n",
        "    b2 = parameters['b2']\n",
        "\n",
        "    # Fetching our gradients\n",
        "    dw1 = gradients['dw1']\n",
        "    db1 = gradients['db1']\n",
        "    dw2 = gradients['dw2']\n",
        "    db2 = gradients['db2']\n",
        "\n",
        "    ## Complete the Code below ##\n",
        "    w1 = w1 - learning_rate * dw1\n",
        "    b1 = b1 - learning_rate * db1\n",
        "    w2 = w2 - learning_rate * dw2\n",
        "    b2 = b2 - learning_rate * db2\n",
        "    ## Your code ends ##\n",
        "\n",
        "    # Returning the updated parameters\n",
        "    Parameters = {\n",
        "        \"w1\" : w1,\n",
        "        \"b1\" : b1,\n",
        "        \"w2\" : w2,\n",
        "        \"b2\" : b2\n",
        "    }\n",
        "\n",
        "    return Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sharing-effort",
      "metadata": {
        "id": "sharing-effort"
      },
      "source": [
        "# Complete Model\n",
        "\n",
        "Implement the entire Neural Network here\n",
        "\n",
        "### Instructions :\n",
        "\n",
        "We need to initialize parameters once, and after that, we will run the following in a loop:\n",
        "- forward_prop(x, parameters)\n",
        "- cost_function(a2, y)\n",
        "- backward_prop(x, y, parameters, forward_cache)\n",
        "- parameters = update_parameters(parameters, gradients, learning_rate)\n",
        "\n",
        "### Return :\n",
        "- parameters, which will be our trained parameters\n",
        "- cost_list, which contains cost for every iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "powerful-sailing",
      "metadata": {
        "id": "powerful-sailing"
      },
      "outputs": [],
      "source": [
        "def model(x, y, n_h, learning_rate, iterations):\n",
        "\n",
        "    ## Complete the Code Below ##\n",
        "    n_x = x.shape[0]                    # must return the number of neurons/features in input layer\n",
        "    n_y = y.shape[0]              # must return the number of neurons in output layer\n",
        "    ## Your Code ends ##\n",
        "\n",
        "    cost_list = []\n",
        "\n",
        "    ## Complete the Code Below ##\n",
        "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
        "    ## Your Code ends ##\n",
        "\n",
        "    ## Complete the Code Below ##\n",
        "    for i in range(iterations):\n",
        "\n",
        "        # Forward Propagation\n",
        "        forward_cache = forward_prop(x, parameters)\n",
        "        a2 = forward_cache[\"a2\"]\n",
        "\n",
        "        # Cost Function\n",
        "        cost = cost_function(a2, y)\n",
        "\n",
        "        # Backward propagation\n",
        "        gradients = backward_prop(x, y, parameters, forward_cache)\n",
        "\n",
        "        # Update Parameters\n",
        "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
        "\n",
        "        cost_list.append(cost)\n",
        "\n",
        "        if i%(iterations/10) == 0 :\n",
        "            print(f\"cost after {i} iterations: {cost}\")\n",
        "\n",
        "\n",
        "    ## Your Code ends ##\n",
        "\n",
        "\n",
        "    return parameters, cost_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "democratic-williams",
      "metadata": {
        "id": "democratic-williams",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05fbea29-65bf-4651-c209-f9eac557d14a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cost after 0 iterations: 2.301819119438726\n",
            "cost after 100 iterations: 1.7994591720227684\n",
            "cost after 200 iterations: 1.3471350938145183\n",
            "cost after 300 iterations: 1.1637646138953879\n",
            "cost after 400 iterations: 1.0408218194079533\n"
          ]
        }
      ],
      "source": [
        "## Complete the Code Below ##\n",
        "\n",
        "n_h = 64\n",
        "learning_rate = 0.1\n",
        "iterations = 1000\n",
        "\n",
        "## Your Code ends ##\n",
        "\n",
        "Parameters, Cost_list = model(X_train, Y_train, n_h = n_h, learning_rate = learning_rate, iterations = iterations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "permanent-music",
      "metadata": {
        "id": "permanent-music"
      },
      "outputs": [],
      "source": [
        "t = np.arange(0, iterations)\n",
        "plt.plot(t, Cost_list)\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.title(\"Cost Reduction Over Iterations\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "apparent-contribution",
      "metadata": {
        "id": "apparent-contribution"
      },
      "source": [
        "# Checking Accuracy\n",
        "\n",
        "Run the below cells to check your model Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "alone-liver",
      "metadata": {
        "id": "alone-liver"
      },
      "outputs": [],
      "source": [
        "def accuracy(inp, labels, parameters):\n",
        "    forward_cache = forward_prop(inp, parameters)\n",
        "    a_out = forward_cache['a2']   # containes propabilities with shape(10, 1)\n",
        "\n",
        "    a_out = np.argmax(a_out, 0)  # 0 represents row wise\n",
        "\n",
        "    labels = np.argmax(labels, 0)\n",
        "\n",
        "    acc = np.mean(a_out == labels)*100\n",
        "\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "later-brake",
      "metadata": {
        "id": "later-brake"
      },
      "outputs": [],
      "source": [
        "print(\"Accuracy of Train Dataset\", accuracy(X_train, Y_train, Parameters), \"%\")\n",
        "print(\"Accuracy of Test Dataset\", round(accuracy(X_test, Y_test, Parameters), 2), \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "banned-clone",
      "metadata": {
        "id": "banned-clone",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "outputId": "5b3428f7-994b-4af7-9bfb-69ac8d91a794"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAGzZJREFUeJzt3X9sVeUdx/HP5dcFtb2s1Pa20mJBhUWEZUxKRZmOhtItRAT54fwDFwPBFTMp6tJlim5LurHwIy4M98cCMxOELgOiWUiw2JJhiwEhxGxrKOnWEmiZJNwLRQqhz/5ouPNKC57Dvf32Xt6v5Em455xvz9eHk3489x6eG3DOOQEA0M8GWTcAALg9EUAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwMcS6ga/q7u7WqVOnlJGRoUAgYN0OAMAj55zOnz+v/Px8DRrU933OgAugU6dOqaCgwLoNAMAtamtr0+jRo/vcP+DegsvIyLBuAQCQADf7fZ60ANq4caPuvfdeDR8+XMXFxfrkk0++Vh1vuwFAerjZ7/OkBND27dtVWVmp1atX69NPP9XkyZNVVlamM2fOJON0AIBU5JJg6tSprqKiIvb66tWrLj8/31VXV9+0NhKJOEkMBoPBSPERiURu+Ps+4XdAly9f1uHDh1VaWhrbNmjQIJWWlqqhoeG647u6uhSNRuMGACD9JTyAPv/8c129elW5ublx23Nzc9Xe3n7d8dXV1QqFQrHBE3AAcHswfwquqqpKkUgkNtra2qxbAgD0g4T/O6Ds7GwNHjxYHR0dcds7OjoUDoevOz4YDCoYDCa6DQDAAJfwO6Bhw4ZpypQpqq2tjW3r7u5WbW2tSkpKEn06AECKSspKCJWVlVqyZIm+853vaOrUqdqwYYM6Ozv1ox/9KBmnAwCkoKQE0KJFi/Tf//5Xr7/+utrb2/Wtb31Le/bsue7BBADA7SvgnHPWTXxZNBpVKBSybgMAcIsikYgyMzP73G/+FBwA4PZEAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwMQQ6waQuhYuXOi55p577vFcs3LlSs81BQUFnmvS0bp163zVrVq1KsGdANfjDggAYIIAAgCYSHgAvfHGGwoEAnFjwoQJiT4NACDFJeUzoAcffFAffvjh/08yhI+aAADxkpIMQ4YMUTgcTsaPBgCkiaR8BnT8+HHl5+dr7NixevbZZ9Xa2trnsV1dXYpGo3EDAJD+Eh5AxcXF2rJli/bs2aNNmzappaVFjz32mM6fP9/r8dXV1QqFQrHB47MAcHtIeACVl5drwYIFmjRpksrKyvS3v/1N586d044dO3o9vqqqSpFIJDba2toS3RIAYABK+tMBI0eO1AMPPKDm5uZe9weDQQWDwWS3AQAYYJL+74AuXLigEydOKC8vL9mnAgCkkIQH0Msvv6z6+nr9+9//1scff6ynnnpKgwcP1jPPPJPoUwEAUljC34I7efKknnnmGZ09e1Z33323Hn30UTU2Nuruu+9O9KkAACks4Jxz1k18WTQaVSgUsm4jZfl5ivDAgQP9dq7+0tDQ4Kvu5MmTnmv+8pe/+DqXV08//bTnmgULFvg6V01NjecaP4vTIr1FIhFlZmb2uZ+14AAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJhgMdI009ra6rnG76Kifhb83LBhg+eavr5NFzfn53qQ/F0TlZWVnmvWr1/vuQapg8VIAQADEgEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABKthp5n+/OsMBAL9di74M23aNF91flY6b2tr81xTWFjouQapg9WwAQADEgEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABNDrBtAYtXU1HiuWbBgga9zLVy40HPNjh07fJ0L/jQ2NqbluZAeuAMCAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgIuCcc9ZNfFk0GlUoFLJuI2UVFBR4rmltbU1CJ72rrKz0XLN+/fokdIIbmTZtmucaFiPFV0UiEWVmZva5nzsgAIAJAggAYMJzAO3fv19z5sxRfn6+AoGAdu3aFbffOafXX39deXl5GjFihEpLS3X8+PFE9QsASBOeA6izs1OTJ0/Wxo0be92/Zs0avfXWW3r77bd18OBB3XnnnSorK9OlS5duuVkAQPrw/I2o5eXlKi8v73Wfc04bNmzQz3/+cz355JOSpHfeeUe5ubnatWuXFi9efGvdAgDSRkI/A2ppaVF7e7tKS0tj20KhkIqLi9XQ0NBrTVdXl6LRaNwAAKS/hAZQe3u7JCk3Nzdue25ubmzfV1VXVysUCsWGn8eIAQCpx/wpuKqqKkUikdhoa2uzbgkA0A8SGkDhcFiS1NHREbe9o6Mjtu+rgsGgMjMz4wYAIP0lNICKiooUDodVW1sb2xaNRnXw4EGVlJQk8lQAgBTn+Sm4CxcuqLm5Ofa6paVFR48eVVZWlgoLC/XSSy/pV7/6le6//34VFRXptddeU35+vubOnZvIvgEAKc5zAB06dEhPPPFE7PW1tb2WLFmiLVu26NVXX1VnZ6eWLVumc+fO6dFHH9WePXs0fPjwxHUNAEh5LEYKLVy40Ffd9u3bE9xJ7/p6hP9GFi1a5OtcPATTY+XKlZ5rBvLb7H6vcdwaFiMFAAxIBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATnr+OAelnx44dvur8rFJ94MABzzV+VllubW31XCNJ69at81yzatUqX+fyau3atZ5rrn1dykDlZ/Xx9evXJ6ETWOAOCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgImAc85ZN/Fl0WhUoVDIug0MICtXrvRc42dRUb/8LKjpR0FBQb+cR/I3fwcPHvRc43chXKSGSCSizMzMPvdzBwQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEi5EiLU2bNs1XXUNDQ4I7sVVYWOirrr8WWEV6YzFSAMCARAABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwMQQ6waAm/GzsGh/LirqZ+HOgoKCJHRyvQMHDviq87uIKeAFd0AAABMEEADAhOcA2r9/v+bMmaP8/HwFAgHt2rUrbv9zzz2nQCAQN2bPnp2ofgEAacJzAHV2dmry5MnauHFjn8fMnj1bp0+fjo1t27bdUpMAgPTj+SGE8vJylZeX3/CYYDCocDjsuykAQPpLymdAdXV1ysnJ0fjx4/XCCy/o7NmzfR7b1dWlaDQaNwAA6S/hATR79my98847qq2t1W9+8xvV19ervLxcV69e7fX46upqhUKh2Oivx1MBALYS/u+AFi9eHPvzQw89pEmTJmncuHGqq6vTzJkzrzu+qqpKlZWVsdfRaJQQAoDbQNIfwx47dqyys7PV3Nzc6/5gMKjMzMy4AQBIf0kPoJMnT+rs2bPKy8tL9qkAACnE81twFy5ciLubaWlp0dGjR5WVlaWsrCy9+eabmj9/vsLhsE6cOKFXX31V9913n8rKyhLaOAAgtXkOoEOHDumJJ56Ivb72+c2SJUu0adMmHTt2TH/605907tw55efna9asWfrlL3+pYDCYuK4BACkv4Jxz1k18WTQaVSgUsm4DSeLnAZPW1tYkdNK7devWea5ZtWqV5xo/8+BnYVG/D/T4Wcz1kUce8XUupK9IJHLDz/VZCw4AYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYCLhX8kN3MjatWv75Tx+VrWW/K1s7UdbW5vnmsLCQs81fuf72teseOFn1fLp06d7rvEzdxiYuAMCAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgIuCcc9ZNfFk0GlUoFLJuA0ni53JraGjwXPPII494rsH/7dixw3PNggULPNf4+btdtGiR5xoWMLURiUSUmZnZ537ugAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgYYt0AcDOjR4/2XFNQUODrXCxa2WPhwoWea/prAdOnn37ac8369es91yD5uAMCAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgIuCcc9ZNfFk0GlUoFLJuA0mydu1azzWVlZX9UiOxaGV/+/jjjz3X+Fmcdvr06Z5rJBanvVWRSESZmZl97ucOCABgggACAJjwFEDV1dV6+OGHlZGRoZycHM2dO1dNTU1xx1y6dEkVFRUaNWqU7rrrLs2fP18dHR0JbRoAkPo8BVB9fb0qKirU2NiovXv36sqVK5o1a5Y6Oztjx6xcuVLvv/++ampqVF9fr1OnTmnevHkJbxwAkNo8fSPqnj174l5v2bJFOTk5Onz4sGbMmKFIJKI//vGP2rp1q773ve9JkjZv3qxvfvObamxs1LRp0xLXOQAgpd3SZ0CRSESSlJWVJUk6fPiwrly5otLS0tgxEyZMUGFhoRoaGnr9GV1dXYpGo3EDAJD+fAdQd3e3XnrpJU2fPl0TJ06UJLW3t2vYsGEaOXJk3LG5ublqb2/v9edUV1crFArFRkFBgd+WAAApxHcAVVRU6LPPPtN77713Sw1UVVUpEonEBs/dA8DtwdNnQNesWLFCH3zwgfbv3x/3j8LC4bAuX76sc+fOxd0FdXR0KBwO9/qzgsGggsGgnzYAACnM0x2Qc04rVqzQzp07tW/fPhUVFcXtnzJlioYOHara2trYtqamJrW2tqqkpCQxHQMA0oKnO6CKigpt3bpVu3fvVkZGRuxznVAopBEjRigUCun5559XZWWlsrKylJmZqRdffFElJSU8AQcAiOMpgDZt2iRJevzxx+O2b968Wc8995yknrW0Bg0apPnz56urq0tlZWX6/e9/n5BmAQDpg8VIMeD5WbDS71u+NTU1nmtWrVrluYaHbXosXLjQc8327ds916xbt85zjeTv7xb/x2KkAIABiQACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABggtWwkZbWrl3rq66ysjLBnSTuPOvXr09CJ6mnP39lBQKBfjtXOmI1bADAgEQAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEi5ECXzJt2jTPNQsWLOiXmoKCAs81DQ0Nnmv8OnnypOcaP/PtZx5qamo810jSwoULfdWhB4uRAgAGJAIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACZYjBRIEWvXrvVc42fhTsnfYqn9Zd26dZ5rNmzY4OtcbW1tvurQg8VIAQADEgEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMsRgoASAoWIwUADEgEEADAhKcAqq6u1sMPP6yMjAzl5ORo7ty5ampqijvm8ccfVyAQiBvLly9PaNMAgNTnKYDq6+tVUVGhxsZG7d27V1euXNGsWbPU2dkZd9zSpUt1+vTp2FizZk1CmwYApL4hXg7es2dP3OstW7YoJydHhw8f1owZM2Lb77jjDoXD4cR0CABIS7f0GVAkEpEkZWVlxW1/9913lZ2drYkTJ6qqqkoXL17s82d0dXUpGo3GDQDAbcD5dPXqVfeDH/zATZ8+PW77H/7wB7dnzx537Ngx9+c//9ndc8897qmnnurz56xevdpJYjAYDEaajUgkcsMc8R1Ay5cvd2PGjHFtbW03PK62ttZJcs3Nzb3uv3TpkotEIrHR1tZmPmkMBoPBuPVxswDy9BnQNStWrNAHH3yg/fv3a/To0Tc8tri4WJLU3NyscePGXbc/GAwqGAz6aQMAkMI8BZBzTi+++KJ27typuro6FRUV3bTm6NGjkqS8vDxfDQIA0pOnAKqoqNDWrVu1e/duZWRkqL29XZIUCoU0YsQInThxQlu3btX3v/99jRo1SseOHdPKlSs1Y8YMTZo0KSn/AQCAFOXlcx/18T7f5s2bnXPOtba2uhkzZrisrCwXDAbdfffd51555ZWbvg/4ZZFIxPx9SwaDwWDc+rjZ734WIwUAJAWLkQIABiQCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgIkBF0DOOesWAAAJcLPf5wMugM6fP2/dAgAgAW72+zzgBtgtR3d3t06dOqWMjAwFAoG4fdFoVAUFBWpra1NmZqZRh/aYhx7MQw/moQfz0GMgzINzTufPn1d+fr4GDer7PmdIP/b0tQwaNEijR4++4TGZmZm39QV2DfPQg3nowTz0YB56WM9DKBS66TED7i04AMDtgQACAJhIqQAKBoNavXq1gsGgdSummIcezEMP5qEH89AjleZhwD2EAAC4PaTUHRAAIH0QQAAAEwQQAMAEAQQAMJEyAbRx40bde++9Gj58uIqLi/XJJ59Yt9Tv3njjDQUCgbgxYcIE67aSbv/+/ZozZ47y8/MVCAS0a9euuP3OOb3++uvKy8vTiBEjVFpaquPHj9s0m0Q3m4fnnnvuuutj9uzZNs0mSXV1tR5++GFlZGQoJydHc+fOVVNTU9wxly5dUkVFhUaNGqW77rpL8+fPV0dHh1HHyfF15uHxxx+/7npYvny5Uce9S4kA2r59uyorK7V69Wp9+umnmjx5ssrKynTmzBnr1vrdgw8+qNOnT8fG3//+d+uWkq6zs1OTJ0/Wxo0be92/Zs0avfXWW3r77bd18OBB3XnnnSorK9OlS5f6udPkutk8SNLs2bPjro9t27b1Y4fJV19fr4qKCjU2Nmrv3r26cuWKZs2apc7OztgxK1eu1Pvvv6+amhrV19fr1KlTmjdvnmHXifd15kGSli5dGnc9rFmzxqjjPrgUMHXqVFdRURF7ffXqVZefn++qq6sNu+p/q1evdpMnT7Zuw5Qkt3Pnztjr7u5uFw6H3W9/+9vYtnPnzrlgMOi2bdtm0GH/+Oo8OOfckiVL3JNPPmnSj5UzZ844Sa6+vt451/N3P3ToUFdTUxM75p///KeT5BoaGqzaTLqvzoNzzn33u991P/nJT+ya+hoG/B3Q5cuXdfjwYZWWlsa2DRo0SKWlpWpoaDDszMbx48eVn5+vsWPH6tlnn1Vra6t1S6ZaWlrU3t4ed32EQiEVFxffltdHXV2dcnJyNH78eL3wwgs6e/asdUtJFYlEJElZWVmSpMOHD+vKlStx18OECRNUWFiY1tfDV+fhmnfffVfZ2dmaOHGiqqqqdPHiRYv2+jTgFiP9qs8//1xXr15Vbm5u3Pbc3Fz961//MurKRnFxsbZs2aLx48fr9OnTevPNN/XYY4/ps88+U0ZGhnV7Jtrb2yWp1+vj2r7bxezZszVv3jwVFRXpxIkT+tnPfqby8nI1NDRo8ODB1u0lXHd3t1566SVNnz5dEydOlNRzPQwbNkwjR46MOzadr4fe5kGSfvjDH2rMmDHKz8/XsWPH9NOf/lRNTU3661//athtvAEfQPi/8vLy2J8nTZqk4uJijRkzRjt27NDzzz9v2BkGgsWLF8f+/NBDD2nSpEkaN26c6urqNHPmTMPOkqOiokKfffbZbfE56I30NQ/Lli2L/fmhhx5SXl6eZs6cqRMnTmjcuHH93WavBvxbcNnZ2Ro8ePB1T7F0dHQoHA4bdTUwjBw5Ug888ICam5utWzFz7Rrg+rje2LFjlZ2dnZbXx4oVK/TBBx/oo48+ivv6lnA4rMuXL+vcuXNxx6fr9dDXPPSmuLhYkgbU9TDgA2jYsGGaMmWKamtrY9u6u7tVW1urkpISw87sXbhwQSdOnFBeXp51K2aKiooUDofjro9oNKqDBw/e9tfHyZMndfbs2bS6PpxzWrFihXbu3Kl9+/apqKgobv+UKVM0dOjQuOuhqalJra2taXU93GweenP06FFJGljXg/VTEF/He++954LBoNuyZYv7xz/+4ZYtW+ZGjhzp2tvbrVvrV6tWrXJ1dXWupaXFHThwwJWWlrrs7Gx35swZ69aS6vz58+7IkSPuyJEjTpJbt26dO3LkiPvPf/7jnHPu17/+tRs5cqTbvXu3O3bsmHvyySddUVGR++KLL4w7T6wbzcP58+fdyy+/7BoaGlxLS4v78MMP3be//W13//33u0uXLlm3njAvvPCCC4VCrq6uzp0+fTo2Ll68GDtm+fLlrrCw0O3bt88dOnTIlZSUuJKSEsOuE+9m89Dc3Ox+8YtfuEOHDrmWlha3e/duN3bsWDdjxgzjzuOlRAA559zvfvc7V1hY6IYNG+amTp3qGhsbrVvqd4sWLXJ5eXlu2LBh7p577nGLFi1yzc3N1m0l3UcffeQkXTeWLFninOt5FPu1115zubm5LhgMupkzZ7qmpibbppPgRvNw8eJFN2vWLHf33Xe7oUOHujFjxrilS5em3f+k9fbfL8lt3rw5dswXX3zhfvzjH7tvfOMb7o477nBPPfWUO336tF3TSXCzeWhtbXUzZsxwWVlZLhgMuvvuu8+98sorLhKJ2Db+FXwdAwDAxID/DAgAkJ4IIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCY+B+zShrozj50PgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our model says it is : 8\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "idx = int(random.randrange(0,X_test.shape[1]))\n",
        "plt.imshow(X_test[:, idx].reshape((28,28)),cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "cache = forward_prop(X_test[:, idx].reshape(X_test[:, idx].shape[0], 1), Parameters)\n",
        "a_pred = cache['a2']\n",
        "a_pred = np.argmax(a_pred, 0)\n",
        "\n",
        "print(\"Our model says it is :\", a_pred[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pediatric-submission",
      "metadata": {
        "id": "pediatric-submission",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "outputId": "b3e4bd41-7763-4396-9260-4b88c1e72a57"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our model says it is : 0\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "idx = int(random.randrange(0,X_test.shape[1]))\n",
        "plt.imshow(X_test[:, idx].reshape((28,28)),cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "cache = forward_prop(X_test[:, idx].reshape(X_test[:, idx].shape[0], 1), Parameters)\n",
        "a_pred = cache['a2']\n",
        "a_pred = np.argmax(a_pred, 0)\n",
        "\n",
        "print(\"Our model says it is :\", a_pred[0])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}