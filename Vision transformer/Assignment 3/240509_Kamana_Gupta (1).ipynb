{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**SUBMISSION INSTRUCTIONS**\n",
        "\n",
        "First make a copy of this colab file and then solve the assignment and upload your final notebook on github.\n",
        "\n",
        "**To write an answer to any question, First Add a TEXT block below the question and type it in only one single block**\n",
        "\n",
        "Before uploading your downloaded notebook, RENAME the file as rollno_name.ipynb\n",
        "\n",
        "Submission Deadline : 19/12/2025 Friday EOD i.e before 11:59 PM\n",
        "\n",
        "The deadline is strict and will not be extended, Late submissions are not allowed\n",
        "\n",
        "Note that you have to upload your solution on the github page of the project Vision Transformer and **under Assignment 3**\n",
        "\n",
        "And remember to keep title of your pull request to be ViT_name_rollno_assgn3\n",
        "\n",
        "Github Submission repo -\n",
        "https://github.com/electricalengineersiitk/Winter-projects-25-26/tree/main/Vision%20transformer/Assignment%203"
      ],
      "metadata": {
        "id": "XmTMSeaTLwVp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**THEORETICAL ASSIGNMENT**\n",
        "\n"
      ],
      "metadata": {
        "id": "-TjKxTBEz8i7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This assignment consists of theoretical questions. Objective answers will not be accepted. You need to write detailed answers (200-300 words) for the questions that require more in-depth explanations (you should be able to identify these after reading the question and finding about them). Questions that need less explanation can be answered in 20-100 words. Please ensure the answers are well-written and thorough."
      ],
      "metadata": {
        "id": "AnVa1_8AKqtv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Assume that the inputs **X**\n",
        " to some scalar function **f**\n",
        " are **n x m**\n",
        " matrices. What is the dimensionality of the gradient of **f**\n",
        " with respect to **X**?\n",
        "**Give reasons to justify your answer.**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ERhYz9qV_YXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now since **f** is a scalar function it depends on each element of the input matrix **X** and has a dimensionality **nxm** because the gradient consists of partial derivatives with respect to each matrix entry and so it has **nxm** as dimensionality."
      ],
      "metadata": {
        "id": "tyRxTUQ-ovEn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Define the evaluation metrics Accuracy, Precision, Recall, and F1 score.\n",
        "\n",
        "   Consider the following diagram:\n",
        "   Find the values of Accuracy, Precision, Recall, and F1 score.\n",
        "   |                | Actual: Cancer | Actual: No Cancer |\n",
        "   |----------------|---------------|------------------|\n",
        "   | Predicted: Cancer | 80 | 80 |\n",
        "   | Predicted: No Cancer | 20 | 820 |\n",
        "    \n",
        "   \n",
        "   Find the values of Accuracy, Precision, Recall, and F1 score using the above data.\n"
      ],
      "metadata": {
        "id": "9vnyYHlq2g4B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Accuracy measures the overall correctness of the model and is defined as the fraction of correctly classified samples out of the total number of samples.\n",
        "Accuracy = (TP + TN) / (TP + FP + FN + TN)\n",
        "\n",
        "2.  Precision measures how many of the predicted positive cases are actually positive.\n",
        "Precision = TP / (TP + FP)\n",
        "\n",
        "3. Recall, also known as sensitivity, measures how many actual positive cases are correctly identified by the model.\n",
        "Recall = TP / (TP + FN)\n",
        "\n",
        "4. The F1 score is the harmonic mean of precision and recall and balances both metrics.\n",
        "F1 score = 2 × (Precision × Recall) / (Precision + Recall)\n",
        "\n",
        "From the given confusion matrix, taking “Cancer” as the positive class:\n",
        "TP = 80, FP = 80, FN = 20, TN = 820, and total samples = 1000.\n",
        "\n",
        "Accuracy = (80 + 820) / 1000 = 0.90 or 90%.\n",
        "Precision = 80 / (80 + 80) = 0.50 or 50%.\n",
        "Recall = 80 / (80 + 20) = 0.80 or 80%.\n",
        "F1 score ≈ 0.615 or 61.5%.\n",
        "\n",
        "Although accuracy is high, the relatively low precision indicates a large number of false cancer predictions."
      ],
      "metadata": {
        "id": "cZnZsLB5rTHo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is a confusion matrix ?"
      ],
      "metadata": {
        "id": "h1dJMD0vJKZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **confusion matrix** is a tabular representation used to evaluate the performance of a classification model by comparing the predicted labels with the actual labels. It summarizes how well a model performs by showing the number of correct and incorrect predictions for each class. In binary classification, it consists of four values: True Positives, False Positives, False Negatives, and True Negatives which apart from helping us with calculating accuracy also tells the type of errors made, helping derive metrics such as precision, recall, and F1 score."
      ],
      "metadata": {
        "id": "F7mSRtCfsfw-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is overfitting and underfitting ?"
      ],
      "metadata": {
        "id": "aOqF8Dzw0DdZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. ***Overfitting*** - It occurs when the model learns the data too well, including noise and irrelevant patterns which leads to poor performance on unseen data.\n",
        "2. ***Underfitting*** - It occurs when the model is too simple to capture the underlying patterns in the data and performs poorly on both the training and test data."
      ],
      "metadata": {
        "id": "y8N0BXKis9Jd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Explain vanishing and exploding gradients, give reasons why they occur and how they can be prevented.\n"
      ],
      "metadata": {
        "id": "n4KfKc0z0ulm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. ***Vanishing***  ***Gradient*** -occur when gradients shrink as they are propagated backward through many layers. This happens because gradients are repeatedly multiplied by small weights or derivatives of activation functions such as sigmoid or tanh, whose derivatives are less than one. As a result, gradients approach zero, causing earlier layers to learn very slowly or stop learning altogether.\n",
        "2. ***Exploding gradients*** - occur when gradients grow exponentially as they propagate backward. This is caused by repeatedly multiplying large weights or derivatives greater than one, leading to extremely large gradient values. It can cause unstable training, large parameter updates, and numerical overflow.\n",
        "\n",
        "* Vanishing gradients can be reduced by using activation functions like ReLU, proper weight initialization methods such as Xavier or He initialization, and batch normalization.\n",
        "* Exploding gradients can be controlled using gradient clipping, careful weight initialization, and smaller learning rates."
      ],
      "metadata": {
        "id": "ZfflvU7guy8t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is regularization ? Explain L1 and L2 regularization"
      ],
      "metadata": {
        "id": "6yaOipvU1JoD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regularization** is a technique used to prevent overfitting by adding a penalty term to the loss function, which discourages complex models and large parameter values. This helps the model generalize better to unseen data.\n",
        "\n",
        "* **L1 regularization** adds the sum of the absolute values of the weights as a penalty. It encourages sparsity by forcing some weights to become exactly zero, effectively performing feature selection.\n",
        "\n",
        "* **L2 regularization** adds the sum of the squared values of the weights as a penalty. It discourages large weights without making them zero, resulting in smoother and more stable models."
      ],
      "metadata": {
        "id": "Etkt0Ib7wmV-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What is Dropout Layer and To which part of a neural network is dropout generally applied? how does it prevent overfitting"
      ],
      "metadata": {
        "id": "NjeRGpCE7QJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **Dropout layer** is a regularization technique used to prevent overfitting in neural networks. During training, it randomly deactivates a fraction of neurons, forcing the network to learn more robust features.\n",
        "\n",
        "Dropout is generally applied to *hidden* *layers*, especially fully connected layers.\n",
        "\n",
        "By preventing neurons from becoming overly dependent on each other, dropout reduces co-adaptation and improves the model’s ability to generalize to unseen data."
      ],
      "metadata": {
        "id": "KVgdhM1jxdME"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. A hidden layer of a neural network has the following activations:\n",
        "\n",
        "   \\[\n",
        "   a = [2.0, 5.0, 7.12, 4.5, 6.0]\n",
        "   \\]\n",
        "\n",
        "   Dropout is applied **during training** with dropout probability \\(p = 0.5\\).\n",
        "\n",
        "   A randomly generated dropout mask is:\n",
        "\n",
        "   \\[\n",
        "   m = [1, 0, 0, 1, 1]\n",
        "   \\]\n",
        "\n",
        "   Here,  \n",
        "   - `1` → neuron is **kept**  \n",
        "   - `0` → neuron is **dropped**\n",
        "\n",
        "   ---\n",
        "\n",
        "   **Question:**  \n",
        "   What will be the output activations of this layer **after applying dropout**?\n",
        "   *(Show the final activation vector)*\n"
      ],
      "metadata": {
        "id": "jLZFLyv4-A67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given activations:\n",
        "a = [2.0, 5.0, 7.12, 4.5, 6.0]\n",
        "\n",
        "Dropout probability p = 0.5\n",
        "So the scaling factor during training (inverted dropout) is:\n",
        "1 / (1 − p) = 1 / 0.5 = 2\n",
        "\n",
        "Dropout mask:\n",
        "m = [1, 0, 0, 1, 1]\n",
        "\n",
        "Step 1: Applying the mask (drop neurons)\n",
        "\n",
        "Element-wise multiplication:\n",
        "\n",
        "[2.0, 5.0, 7.12, 4.5, 6.0]\n",
        "× [1, 0, 0, 1, 1]\n",
        "\n",
        "= [2.0, 0, 0, 4.5, 6.0]\n",
        "\n",
        "Step 2: Scaling the kept activations\n",
        "\n",
        "Multiply the kept values by 2:\n",
        "\n",
        "[2.0 × 2, 0, 0, 4.5 × 2, 6.0 × 2]\n",
        "\n",
        "Final output activation vector:\n",
        "\n",
        "**[4.0, 0, 0, 9.0, 12.0]**"
      ],
      "metadata": {
        "id": "EcMBaZ--xn-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Explain the difference between Gradient Descent, Stochastic Gradient Descent, Mini-Batch Gradient Descent"
      ],
      "metadata": {
        "id": "La3K0HmUGj8t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "| Aspect               | Gradient Descent (Batch GD)      | Stochastic Gradient Descent (SGD)         | Mini-Batch Gradient Descent                  |\n",
        "| -------------------- | -------------------------------- | ----------------------------------------- | -------------------------------------------- |\n",
        "| Data used per update | Uses the entire training dataset | Uses only one training example            | Uses a small subset (batch) of training data |\n",
        "| Update frequency     | One update per epoch             | One update per training sample            | Multiple updates per epoch                   |\n",
        "| Computation cost     | Very high for large datasets     | Very low per update                       | Moderate                                     |\n",
        "| Convergence behavior | Smooth and stable                | Noisy and fluctuating                     | Relatively smooth with some noise            |\n",
        "| Speed                | Slow for large datasets          | Fast but unstable                         | Faster and more stable                       |\n",
        "| Memory requirement   | High                             | Very low                                  | Moderate                                     |\n",
        "| Risk of local minima | Higher chance of getting stuck   | Can escape local minima due to noise      | Balanced behavior                            |               |\n",
        "| Example batch size   | Entire dataset                   | 1                                         | 16, 32, 64, etc.                             |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "n7zXPXqczwAe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are optimizers ? Explain Adam, RMS prop and Momentum in detail"
      ],
      "metadata": {
        "id": "FngV9WtV7JxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimizers** are algorithms used to update the weights of a neural network during training in order to minimize the loss function. They determine how the gradients are used to adjust model parameters and directly affect training speed and stability.\n",
        "\n",
        "***Momentum*** improves gradient descent by adding a fraction of the previous update to the current one. This helps accelerate learning in consistent directions and reduces oscillations.\n",
        "\n",
        "***RMSProp*** adapts the learning rate for each parameter by maintaining a moving average of squared gradients. It prevents large updates and stabilizes training, especially when gradients vary widely.\n",
        "\n",
        "***Adam*** combines the benefits of Momentum and RMSProp by keeping track of both the moving average of gradients and squared gradients. It is widely used due to its fast convergence and robustness."
      ],
      "metadata": {
        "id": "URr9fltW0jlG"
      }
    }
  ]
}