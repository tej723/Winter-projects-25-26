{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**SUBMISSION INSTRUCTIONS**\n",
        "\n",
        "First make a copy of this colab file and then solve the assignment and upload your final notebook on github.\n",
        "\n",
        "**To write an answer to any question, First Add a TEXT block below the question and type it in only one single block**\n",
        "\n",
        "Before uploading your downloaded notebook, RENAME the file as rollno_name.ipynb\n",
        "\n",
        "Submission Deadline : 19/12/2025 Friday EOD i.e before 11:59 PM\n",
        "\n",
        "The deadline is strict and will not be extended, Late submissions are not allowed\n",
        "\n",
        "Note that you have to upload your solution on the github page of the project Vision Transformer and **under Assignment 3**\n",
        "\n",
        "And remember to keep title of your pull request to be ViT_name_rollno_assgn3\n",
        "\n",
        "Github Submission repo -\n",
        "https://github.com/electricalengineersiitk/Winter-projects-25-26/tree/main/Vision%20transformer/Assignment%203"
      ],
      "metadata": {
        "id": "XmTMSeaTLwVp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**THEORETICAL ASSIGNMENT**\n",
        "\n"
      ],
      "metadata": {
        "id": "-TjKxTBEz8i7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This assignment consists of theoretical questions. Objective answers will not be accepted. You need to write detailed answers (200-300 words) for the questions that require more in-depth explanations (you should be able to identify these after reading the question and finding about them). Questions that need less explanation can be answered in 20-100 words. Please ensure the answers are well-written and thorough."
      ],
      "metadata": {
        "id": "AnVa1_8AKqtv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Assume that the inputs **X**\n",
        " to some scalar function **f**\n",
        " are **n x m**\n",
        " matrices. What is the dimensionality of the gradient of **f**\n",
        " with respect to **X**?\n",
        "**Give reasons to justify your answer.**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ERhYz9qV_YXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the dimensionality of the gradient will also be n x m. because while calculating gradient we just do the partial differentiation of function with respect to the each element of that matrix."
      ],
      "metadata": {
        "id": "nEGtOwgkEg9H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Define the evaluation metrics Accuracy, Precision, Recall, and F1 score.\n",
        "\n",
        "   Consider the following diagram:\n",
        "   Find the values of Accuracy, Precision, Recall, and F1 score.\n",
        "   |                | Actual: Cancer | Actual: No Cancer |\n",
        "   |----------------|---------------|------------------|\n",
        "   | Predicted: Cancer | 80 | 80 |\n",
        "   | Predicted: No Cancer | 20 | 820 |\n",
        "    \n",
        "   \n",
        "   Find the values of Accuracy, Precision, Recall, and F1 score using the above data.\n"
      ],
      "metadata": {
        "id": "9vnyYHlq2g4B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "answer:\n",
        "accuracy: 0.90\n",
        "precision: 0.50\n",
        "recall: 0.80\n",
        "F1 score: 0.615"
      ],
      "metadata": {
        "id": "S-9kYe9fFa8G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is a confusion matrix ?"
      ],
      "metadata": {
        "id": "h1dJMD0vJKZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Confusion Matrix is a performance evaluation tool for classification models that displays a detailed breakdown of correct and incorrect predictions. It is a table (usually 2 x 2 for binary classification) that compares the model's predicted labels against the actual ground truth labels."
      ],
      "metadata": {
        "id": "3Curk7kXGBsM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is overfitting and underfitting ?"
      ],
      "metadata": {
        "id": "aOqF8Dzw0DdZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overfitting means the model learns the training data too well, including its noise and random fluctuations.\n",
        "\n",
        "Underfitting means model is too simple to capture the underlying patterns or trends in the data. Which results in Poor performance on both the training data and the test data."
      ],
      "metadata": {
        "id": "0aue6lvFGUQ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Explain vanishing and exploding gradients, give reasons why they occur and how they can be prevented.\n"
      ],
      "metadata": {
        "id": "n4KfKc0z0ulm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vanishing Gradients:\n",
        "Gradients become exponentially smaller as they move backward through layers, eventually \"vanishing\" to nearly zero.\n",
        "\n",
        "Reason: Repeatedly multiplying small values (e.g., derivatives of sigmoid or tanh functions) causes early layers to stop learning.\n",
        "\n",
        "Prevention: Use ReLU activation functions, Batch Normalization, or Residual Connections.\n",
        "\n",
        "Exploding Gradients:\n",
        "Gradients grow exponentially large, causing massive weight updates that make the model unstable or diverge.\n",
        "\n",
        "Reason: High learning rates or large weight initializations lead to derivatives greater than 1 being multiplied repeatedly.\n",
        "\n",
        "Prevention: Use Gradient Clipping (capping the maximum gradient value) or proper weight initialization like Xavier or He initialization."
      ],
      "metadata": {
        "id": "prtZyxgnG3IC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is regularization ? Explain L1 and L2 regularization"
      ],
      "metadata": {
        "id": "6yaOipvU1JoD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularization is a technique used to prevent overfitting by adding a penalty term to the loss function, which discourages the model from becoming overly complex.\n",
        "\n",
        "L1 Regularization: Adds the absolute value of the weights as a penalty.\n",
        "\n",
        "Effect: It can shrink some weight coefficients to exactly zero, effectively performing feature selection by removing less important features.\n",
        "\n",
        "L2 Regularization: Adds the squared value of the weights as a penalty.\n",
        "\n",
        "Effect: It shrinks all coefficients evenly toward zero but never makes them exactly zero. This is particularly effective for handling multicollinearity (highly correlated features)."
      ],
      "metadata": {
        "id": "6Va6tS_SIBvW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What is Dropout Layer and To which part of a neural network is dropout generally applied? how does it prevent overfitting"
      ],
      "metadata": {
        "id": "NjeRGpCE7QJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Dropout Layer is a regularization technique where randomly selected neurons (and their connections) are ignored or \"dropped out\" during each training iteration.\n",
        "\n",
        "Application: It is generally applied to hidden layers, though it can also be used on the input layer.\n",
        "\n",
        "How it Prevents Overfitting:\n",
        "\n",
        "Reduces Co-adaptation: Neurons cannot rely on the presence of specific other neurons, forcing each one to learn more robust, independent features.\n",
        "\n",
        "Ensemble Effect: It essentially trains many \"thinned\" versions of the network simultaneously, which acts like an ensemble of models that generalizes better to new data."
      ],
      "metadata": {
        "id": "zTTQO9--In5A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. A hidden layer of a neural network has the following activations:\n",
        "\n",
        "   \\[\n",
        "   a = [2.0, 5.0, 7.12, 4.5, 6.0]\n",
        "   \\]\n",
        "\n",
        "   Dropout is applied **during training** with dropout probability \\(p = 0.5\\).\n",
        "\n",
        "   A randomly generated dropout mask is:\n",
        "\n",
        "   \\[\n",
        "   m = [1, 0, 0, 1, 1]\n",
        "   \\]\n",
        "\n",
        "   Here,  \n",
        "   - `1` → neuron is **kept**  \n",
        "   - `0` → neuron is **dropped**\n",
        "\n",
        "   ---\n",
        "\n",
        "   **Question:**  \n",
        "   What will be the output activations of this layer **after applying dropout**?\n",
        "   *(Show the final activation vector)*\n"
      ],
      "metadata": {
        "id": "jLZFLyv4-A67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[4, 0, 0, 9, 12]"
      ],
      "metadata": {
        "id": "fX0ysAx0fZkH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Explain the difference between Gradient Descent, Stochastic Gradient Descent, Mini-Batch Gradient Descent"
      ],
      "metadata": {
        "id": "La3K0HmUGj8t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch Gradient Descent: This method calculates the gradient using the entire dataset. While it provides a stable convergence and a clear path to the minimum, it is computationally expensive and slow for large datasets, as it requires loading all data into memory for a single update.\n",
        "\n",
        "Stochastic Gradient Descent (SGD): SGD updates the weights using only one random training example at a time. This makes it much faster and allows for online learning. However, the path to the minimum is very \"noisy\" or zigzagged, which can actually help it jump out of local minima but makes convergence to the exact global minimum difficult.\n",
        "\n",
        "Mini-Batch Gradient Descent: This is the most popular variation. It splits the training data into small batches (e.g., 32, 64, or 128 samples). It strikes a balance: it is more computationally efficient than Batch GD by using vectorized libraries, and more stable than SGD by reducing the variance of the gradient updates."
      ],
      "metadata": {
        "id": "2woinlsdgDrE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are optimizers ? Explain Adam, RMS prop and Momentum in detail"
      ],
      "metadata": {
        "id": "FngV9WtV7JxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "Optimizers are algorithms or methods used to change the attributes of a neural network, such as weights and learning rate, to reduce the losses.\n",
        "\n",
        "Momentum:-\n",
        "Momentum is designed to accelerate SGD in the relevant direction and dampen oscillations. It does this by adding a fraction of the update vector of the past time step to the current update. like a ball rolling down a hill; it gains momentum as it descends. It helps the optimizer navigate through \"valleys\" (narrow regions of the loss surface) where the gradient might otherwise oscillate wildly.\n",
        "\n",
        "RMSprop (Root Mean Square Propagation):-\n",
        "RMSprop is an adaptive learning rate method. Instead of using a fixed learning rate for all parameters, it maintains a moving average of the squared gradients for each weight. It then divides the gradient by the square root of this average. This has the effect of normalizing the gradient: it slows down learning for dimensions with large gradients (to prevent exploding) and speeds up learning for dimensions with small gradients.\n",
        "\n",
        "Adam (Adaptive Moment Estimation):-\n",
        "It combines the benefits of Momentum and RMSprop.\n",
        "\n",
        "1. It calculates an exponential moving average of the gradient (like Momentum) to keep track of direction.\n",
        "\n",
        "2. It calculates an exponential moving average of the squared gradient (like RMSprop) to scale the learning rate. Adam is computationally efficient, requires little memory, and is well-suited for problems that are large in terms of data or parameters."
      ],
      "metadata": {
        "id": "7nCCTsVogVzh"
      }
    }
  ]
}