{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**SUBMISSION INSTRUCTIONS**\n",
        "\n",
        "First make a copy of this colab file and then solve the assignment and upload your final notebook on github.\n",
        "\n",
        "**To write an answer to any question, First Add a TEXT block below the question and type it in only one single block**\n",
        "\n",
        "Before uploading your downloaded notebook, RENAME the file as rollno_name.ipynb\n",
        "\n",
        "Submission Deadline : 19/12/2025 Friday EOD i.e before 11:59 PM\n",
        "\n",
        "The deadline is strict and will not be extended, Late submissions are not allowed\n",
        "\n",
        "Note that you have to upload your solution on the github page of the project Vision Transformer and **under Assignment 3**\n",
        "\n",
        "And remember to keep title of your pull request to be ViT_name_rollno_assgn3\n",
        "\n",
        "Github Submission repo -\n",
        "https://github.com/electricalengineersiitk/Winter-projects-25-26/tree/main/Vision%20transformer/Assignment%203"
      ],
      "metadata": {
        "id": "XmTMSeaTLwVp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**THEORETICAL ASSIGNMENT**\n",
        "\n"
      ],
      "metadata": {
        "id": "-TjKxTBEz8i7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This assignment consists of theoretical questions. Objective answers will not be accepted. You need to write detailed answers (200-300 words) for the questions that require more in-depth explanations (you should be able to identify these after reading the question and finding about them). Questions that need less explanation can be answered in 20-100 words. Please ensure the answers are well-written and thorough."
      ],
      "metadata": {
        "id": "AnVa1_8AKqtv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Assume that the inputs **X**\n",
        " to some scalar function **f**\n",
        " are **n x m**\n",
        " matrices. What is the dimensionality of the gradient of **f**\n",
        " with respect to **X**?\n",
        "**Give reasons to justify your answer.**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ERhYz9qV_YXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The resulting dimension of the gradient matrix will be n x m.\n",
        "This is because, we calculate the gradient, that is del(f)/del(x_ij) for each i, j. So it contains n x m elements, which are represented as n x m matrix, which can be multiplied by scalar learning rate and subtracted from X, to produce updates(for gradient descent)."
      ],
      "metadata": {
        "id": "fIXOTzo_5Rud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Define the evaluation metrics Accuracy, Precision, Recall, and F1 score.\n",
        "\n",
        "   Consider the following diagram:\n",
        "   Find the values of Accuracy, Precision, Recall, and F1 score.\n",
        "   |                | Actual: Cancer | Actual: No Cancer |\n",
        "   |----------------|---------------|------------------|\n",
        "   | Predicted: Cancer | 80 | 80 |\n",
        "   | Predicted: No Cancer | 20 | 820 |\n",
        "    \n",
        "   \n",
        "   Find the values of Accuracy, Precision, Recall, and F1 score using the above data.\n"
      ],
      "metadata": {
        "id": "9vnyYHlq2g4B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy = Total correctly predicted / Total sample datapoints\n",
        "\n",
        "Precision = Actual cancer patients correctly identified / Patients identified as having cancer by the model\n",
        "\n",
        "Recall = Actual cancer patients correctly identified / Patients with cancer(both identified by model and not)\n",
        "\n",
        "F1 score = Harmonic mean of Precision and Recall.\n",
        "\n",
        "ACcuracy = (80+820) / (80+820+80+20) = 0.9\n",
        "\n",
        "Precision = 80 / (80 + 80) = 0.5\n",
        "\n",
        "Recall = 80 / (80 + 20) = 0.8\n",
        "\n",
        "F1 score = 2*(0.5*0.8)/(0.5 + 0.8) = 0.615"
      ],
      "metadata": {
        "id": "K4pPol9s6EMY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is a confusion matrix ?"
      ],
      "metadata": {
        "id": "h1dJMD0vJKZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confusion matrix is a visual metric which is made to visually check a model's performance in CLASSIFICATION tasks. Its a square matrix.\n",
        "Question 2 contains exactly a confusion matrix.\n",
        "\n",
        "It value C_ij represents the number of sample points that actually belonged to class j, but our model predicted it as class i\n",
        "\n",
        "Obviously we want the highest numbers on the principal diagonal of this confusion matrix.\n",
        "\n",
        "Various metrics can be calculated from this matrix, as seen in Q2 itself."
      ],
      "metadata": {
        "id": "It4uKggr7o2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is overfitting and underfitting ?"
      ],
      "metadata": {
        "id": "aOqF8Dzw0DdZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overfitting is the case when the model gets too good at performing on the training data, but performs worse on test(unseen) data.\n",
        "\n",
        "It means the model has very harshly remembered the training data, and whenever it sees anything a little different from the training data, it starts giving out wrong predictions.\n",
        "\n",
        "This is like a student memorising the tutorials, but not clearing their concepts, which means, they'll perform poor in tests(unseen Qs).\n",
        "\n",
        "Underfitting is due to low learning rate, low epochs, low data size, etc. It just means our model performs poor on both train and test datasets.\n",
        "\n",
        "Its like a student who just doesn't study and performs bad on tutorial questions and the test(unseen) both."
      ],
      "metadata": {
        "id": "AhbMkd527qf-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Explain vanishing and exploding gradients, give reasons why they occur and how they can be prevented.\n"
      ],
      "metadata": {
        "id": "n4KfKc0z0ulm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vanishing gradient happens when the gradients are too small, like 1e-6 magnitudes or less. This means, our weight updates are even smaller(multiplied by the learning rate), and our weights change negligibly over several epochs. Means the model training, basically stops.\n",
        "\n",
        "It can be prevented by using relu activation function, over sigmoid or tanh. The latter activation functions map inputs to very small range and their derivative is always less than 1, so during backprop over several layers, this multiplies to give vanishing gradients for earlier layers.\n",
        "\n",
        "Exploding gradients is the reverse, the gradients are too large, and the update steps are unstable.\n",
        "\n",
        "This can happen if the initial weights are initialised too large, or the weights increase too much, which increase the gradients, which in turn causes even larger increase in weights. So small initial weights, alongside regularisation can fix the large weights issue.\n",
        "\n",
        "While a more direct approach is just clipping the gradients, if they cross a certain predefined threshold.\n"
      ],
      "metadata": {
        "id": "Py3Zlg2oJYPZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is regularization ? Explain L1 and L2 regularization"
      ],
      "metadata": {
        "id": "6yaOipvU1JoD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularization is a technique used to solve the overfitting problem by adding a penalty term to the loss function. This penalty discourages the model from assigning too much importance to any single feature, effectively keeping the model weights small and simple.\n",
        "\n",
        "L1 Regularization, also known as Lasso, adds a penalty equal to the absolute value of the magnitude. This can drive some weights to exactly zero, which can cause certain issues(or help in feature selection, whatever the usecase may be).\n",
        "\n",
        "L2 Regularization, also known as Ridge, adds a penalty equal to the square of the magnitudes. This forces weights to be small but rarely zero(spreading the error among all features)."
      ],
      "metadata": {
        "id": "CDw1s4mBLQIs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What is Dropout Layer and To which part of a neural network is dropout generally applied? how does it prevent overfitting"
      ],
      "metadata": {
        "id": "NjeRGpCE7QJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropout layer takes an array of inputs and it drops a certain percentage of the values in it(makes them zero).\n",
        "\n",
        "Its usually applied to fully connected layer's outputs, during training process(not inference). And, its usually applied to lower layers of the network(to allow the model to learn small level complexities early on, but drop large level assumtions, patterns which form later on).\n",
        "\n",
        "It prevents overfitting, as the drop of outputs is random, and it occurs during training, the model cannot rely on a few neurons(which also represents few selected features). The model learns to use all features, don't focus too much on certain features too much(overfitting) and learn to generalise well on the training dataset."
      ],
      "metadata": {
        "id": "uAUnlDH0LoTm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. A hidden layer of a neural network has the following activations:\n",
        "\n",
        "   \\[\n",
        "   a = [2.0, 5.0, 7.12, 4.5, 6.0]\n",
        "   \\]\n",
        "\n",
        "   Dropout is applied **during training** with dropout probability \\(p = 0.5\\).\n",
        "\n",
        "   A randomly generated dropout mask is:\n",
        "\n",
        "   \\[\n",
        "   m = [1, 0, 0, 1, 1]\n",
        "   \\]\n",
        "\n",
        "   Here,  \n",
        "   - `1` → neuron is **kept**  \n",
        "   - `0` → neuron is **dropped**\n",
        "\n",
        "   ---\n",
        "\n",
        "   **Question:**  \n",
        "   What will be the output activations of this layer **after applying dropout**?\n",
        "   *(Show the final activation vector)*\n"
      ],
      "metadata": {
        "id": "jLZFLyv4-A67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output should the masked vector. Means where ever 1 appears in mask, only those values are kept in final output, rest becomes zero.\n",
        "\n",
        "But in practice, as repeated dropout can cause the overall magnitde of input to drop(and cause model's output magnitude to drop to zero), we scale the rest of the inputs(which are not dropped) by 1 / (1-p).\n",
        "\n",
        "Output = [4.0, 0, 0, 9, 12]"
      ],
      "metadata": {
        "id": "gNzfeU1PMv8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Explain the difference between Gradient Descent, Stochastic Gradient Descent, Mini-Batch Gradient Descent"
      ],
      "metadata": {
        "id": "La3K0HmUGj8t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Descent is the approach where the model calculates the error for the entire dataset before updating the weights once. It is stable but very slow. SGD updates the weights after every single data point. This is much faster but causes the loss to fluctuate heavily, and has a noisy convergence. Mini batch gradient descent is the best of both worlds, where the model updates weights after seeing a small batch of samples (like 32 or 64). AS we chosse this hyperparameter, we can select the tradeoff between speed and noise.\n",
        "\n",
        "Also, SGD is better at leaving local minima, while gradient descent might get stuck in a local minima, due to its smaller step size."
      ],
      "metadata": {
        "id": "nZiUKxCONrKx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are optimizers ? Explain Adam, RMS prop and Momentum in detail"
      ],
      "metadata": {
        "id": "FngV9WtV7JxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Momentum is a technique added to SGD that helps accelerate gradients in the right direction and deals with oscillations. It does this on the ball rolling down the hill logic, where it maintains the gradient of past steps, and on new steps, it maintains that \"speed\", this means there is less oscillation and the model doesn't as easily gets stuck in local minima(due to speed, even if gradient becomes zero, it moves around).\n",
        "\n",
        "RMSProp(Root Mean Square Propagation) is an adaptive learning rate method. It divides the learning rate by an exponentially decaying average of squared gradients. This means this reduce the step size for parameters with large steep gradients and increases for parameters with small flat gradients.\n",
        "\n",
        "Adam (Adaptive Moment Estimation) is essentially a combination of Momentum and RMSProp. It keeps track of both the average of past gradients and the average of past squared gradients (like RMSProp) to adapt the learning rates for each parameter individually. It is currently the most popular optimizer for most ML related tasks."
      ],
      "metadata": {
        "id": "p71hXc0zNvyG"
      }
    }
  ]
}