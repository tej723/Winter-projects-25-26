{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**SUBMISSION INSTRUCTIONS**\n",
        "\n",
        "First make a copy of this colab file and then solve the assignment and upload your final notebook on github.\n",
        "\n",
        "**To write an answer to any question, First Add a TEXT block below the question and type it in only one single block**\n",
        "\n",
        "Before uploading your downloaded notebook, RENAME the file as rollno_name.ipynb\n",
        "\n",
        "Submission Deadline : 19/12/2025 Friday EOD i.e before 11:59 PM\n",
        "\n",
        "The deadline is strict and will not be extended, Late submissions are not allowed\n",
        "\n",
        "Note that you have to upload your solution on the github page of the project Vision Transformer and **under Assignment 3**\n",
        "\n",
        "And remember to keep title of your pull request to be ViT_name_rollno_assgn3\n",
        "\n",
        "Github Submission repo -\n",
        "https://github.com/electricalengineersiitk/Winter-projects-25-26/tree/main/Vision%20transformer/Assignment%203"
      ],
      "metadata": {
        "id": "XmTMSeaTLwVp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**THEORETICAL ASSIGNMENT**\n",
        "\n"
      ],
      "metadata": {
        "id": "-TjKxTBEz8i7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This assignment consists of theoretical questions. Objective answers will not be accepted. You need to write detailed answers (200-300 words) for the questions that require more in-depth explanations (you should be able to identify these after reading the question and finding about them). Questions that need less explanation can be answered in 20-100 words. Please ensure the answers are well-written and thorough."
      ],
      "metadata": {
        "id": "AnVa1_8AKqtv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Assume that the inputs **X**\n",
        " to some scalar function **f**\n",
        " are **n x m**\n",
        " matrices. What is the dimensionality of the gradient of **f**\n",
        " with respect to **X**?\n",
        "**Give reasons to justify your answer.**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ERhYz9qV_YXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since **f** is a scalar function, it relies on each element of the input matrix **X** and has a dimensionality of **nxm**, as the gradient is made up of partial derivatives concerning each entry of the matrix, resulting in a dimensionality of **nxm**."
      ],
      "metadata": {
        "id": "_BY19swDre4e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Define the evaluation metrics Accuracy, Precision, Recall, and F1 score.\n",
        "\n",
        "   Consider the following diagram:\n",
        "   Find the values of Accuracy, Precision, Recall, and F1 score.\n",
        "   |                | Actual: Cancer | Actual: No Cancer |\n",
        "   |----------------|---------------|------------------|\n",
        "   | Predicted: Cancer | 80 | 80 |\n",
        "   | Predicted: No Cancer | 20 | 820 |\n",
        "    \n",
        "   \n",
        "   Find the values of Accuracy, Precision, Recall, and F1 score using the above data.\n"
      ],
      "metadata": {
        "id": "9vnyYHlq2g4B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Accuracy measures the overall correctness of the model and is defined as the fraction of correctly classified samples out of the total number of samples.\n",
        "Accuracy = (TP + TN) / (TP + FP + FN + TN)\n",
        "\n",
        "2.  Precision measures how many of the predicted positive cases are actually positive.\n",
        "Precision = TP / (TP + FP)\n",
        "\n",
        "3. Recall, also known as sensitivity, measures how many actual positive cases are correctly identified by the model.\n",
        "Recall = TP / (TP + FN)\n",
        "\n",
        "4. The F1 score is the harmonic mean of precision and recall and balances both metrics.\n",
        "F1 score = 2 × (Precision × Recall) / (Precision + Recall)\n",
        "\n",
        "From the given confusion matrix, taking “Cancer” as the positive class:\n",
        "TP = 80, FP = 80, FN = 20, TN = 820, and total samples = 1000.\n",
        "\n",
        "Accuracy = (80 + 820) / 1000 = 0.90 or 90%.\n",
        "Precision = 80 / (80 + 80) = 0.50 or 50%.\n",
        "Recall = 80 / (80 + 20) = 0.80 or 80%.\n",
        "F1 score ≈ 0.615 or 61.5%.\n",
        "\n",
        "Although accuracy is high, the relatively low precision indicates a large number of false cancer predictions."
      ],
      "metadata": {
        "id": "-P91ihngrqb2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is a confusion matrix ?"
      ],
      "metadata": {
        "id": "h1dJMD0vJKZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A confusion matrix is a tabular tool for assessing a classification model’s performance by lining up predicted labels against the true labels. It encapsulates how well the model does by listing the counts of correct and incorrect predictions for each class. In binary classification, it holds four quantities: True Positives, False Positives, False Negatives, and True Negatives. Beyond aiding accuracy calculations, it reveals the nature of the errors, guiding the derivation of metrics like precision, recall, and F1 score.\n",
        "\n"
      ],
      "metadata": {
        "id": "q02N9kE_uiBw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is overfitting and underfitting ?"
      ],
      "metadata": {
        "id": "aOqF8Dzw0DdZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. ***Overfitting*** - It occurs when the model learns the data too well, including noise and irrelevant patterns which leads to poor performance on unseen data.\n",
        "2. ***Underfitting*** - It occurs when the model is too simple to capture the underlying patterns in the data and performs poorly on both the training and test data."
      ],
      "metadata": {
        "id": "B8tVshnQwNif"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Explain vanishing and exploding gradients, give reasons why they occur and how they can be prevented.\n"
      ],
      "metadata": {
        "id": "n4KfKc0z0ulm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. ***Vanishing*** ***Gradient*** - occur when gradients are reduced when they are transmitted backwards through many layers. This is because gradients are multiplied repeatedly by small weights or derivatives of activation functions such as sigmoid or tanh, whose derivatives are less than one. As a result, gradients approach zero, resulting in the first layers learning very slowly or stopping completely learning. 2. ***Exploding gradients*** - occurs when gradients grow exponentially as they propagate backwards. This occurs by repeatedly multiplying large weights or derivatives more than one, which leads to extremely large gradient values. It can lead to unstable training, large parameter updates, and numerical overloads. * The extinction gradient can be reduced by activation functions such as ReLU, appropriate weight initialization methods such as Xavier or He, and batch normalization. * Exploding gradients can be controlled with gradient clips, careful weight initialization and lower learning rates."
      ],
      "metadata": {
        "id": "WzU8ocLhwR6H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is regularization ? Explain L1 and L2 regularization"
      ],
      "metadata": {
        "id": "6yaOipvU1JoD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regularization** is a technique used to prevent overfitting by adding a penalty term to the loss function, which discourages complex models and large parameter values. This helps the model generalize better to unseen data.\n",
        "\n",
        "* **L1 regularization** adds the sum of the absolute values of the weights as a penalty. It encourages sparsity by forcing some weights to become exactly zero, effectively performing feature selection.\n",
        "\n",
        "* **L2 regularization** adds the sum of the squared values of the weights as a penalty. It discourages large weights without making them zero, resulting in smoother and more stable models."
      ],
      "metadata": {
        "id": "yDl4ITAdxCTm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What is Dropout Layer and To which part of a neural network is dropout generally applied? how does it prevent overfitting"
      ],
      "metadata": {
        "id": "NjeRGpCE7QJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Dropout layer** is a regularization technique used to prevent overfitting of neural networks. In training, it randomly deactivates a small number of neurons, requiring the network to learn more robust functions. Dropout is generally applied to *hidden* *layers*, especially to fully connected layers. By preventing neurons from becoming too dependent on each other, dropout reduces co-adaptation and improves the model's ability to generalize to invisible data."
      ],
      "metadata": {
        "id": "KdXliB3RxZZs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. A hidden layer of a neural network has the following activations:\n",
        "\n",
        "   \\[\n",
        "   a = [2.0, 5.0, 7.12, 4.5, 6.0]\n",
        "   \\]\n",
        "\n",
        "   Dropout is applied **during training** with dropout probability \\(p = 0.5\\).\n",
        "\n",
        "   A randomly generated dropout mask is:\n",
        "\n",
        "   \\[\n",
        "   m = [1, 0, 0, 1, 1]\n",
        "   \\]\n",
        "\n",
        "   Here,  \n",
        "   - `1` → neuron is **kept**  \n",
        "   - `0` → neuron is **dropped**\n",
        "\n",
        "   ---\n",
        "\n",
        "   **Question:**  \n",
        "   What will be the output activations of this layer **after applying dropout**?\n",
        "   *(Show the final activation vector)*\n"
      ],
      "metadata": {
        "id": "jLZFLyv4-A67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's how dropout works with the given activations:\n",
        "\n",
        "We start with activations a = [2.0, 5.0, 7.12, 4.5, 6.0] and a dropout probability p = 0.5.\n",
        "\n",
        "During training, the scaling factor (inverted dropout) is calculated as 1 / (1 - p) = 1 / 0.5 = 2.\n",
        "\n",
        "A dropout mask m = [1, 0, 0, 1, 1] is then applied.\n",
        "\n",
        "**Step 1: Applying the mask (dropping neurons)**\n",
        "\n",
        "We perform element-wise multiplication: [2.0, 5.0, 7.12, 4.5, 6.0] * [1, 0, 0, 1, 1] = [2.0, 0, 0, 4.5, 6.0].\n",
        "\n",
        "**Step 2: Scaling the kept activations**\n",
        "\n",
        "Multiply the remaining values by 2: [2.0 * 2, 0, 0, 4.5 * 2, 6.0 * 2].\n",
        "\n",
        "The final output activation vector is **[4.0, 0, 0, 9.0, 12.0]**."
      ],
      "metadata": {
        "id": "WITm0AgnxdHY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Explain the difference between Gradient Descent, Stochastic Gradient Descent, Mini-Batch Gradient Descent"
      ],
      "metadata": {
        "id": "La3K0HmUGj8t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "| Aspect               | Gradient Descent (Batch GD)      | Stochastic Gradient Descent (SGD)         | Mini-Batch Gradient Descent                  |\n",
        "| -------------------- | -------------------------------- | ----------------------------------------- | -------------------------------------------- |\n",
        "| Data used per update | Uses the entire training dataset | Uses only one training example            | Uses a small subset (batch) of training data |\n",
        "| Update frequency     | One update per epoch             | One update per training sample            | Multiple updates per epoch                   |\n",
        "| Computation cost     | Very high for large datasets     | Very low per update                       | Moderate                                     |\n",
        "| Convergence behavior | Smooth and stable                | Noisy and fluctuating                     | Relatively smooth with some noise            |\n",
        "| Speed                | Slow for large datasets          | Fast but unstable                         | Faster and more stable                       |\n",
        "| Memory requirement   | High                             | Very low                                  | Moderate                                     |\n",
        "| Risk of local minima | Higher chance of getting stuck   | Can escape local minima due to noise      | Balanced behavior                            |               |\n",
        "| Example batch size   | Entire dataset                   | 1                                         | 16, 32, 64, etc.                             |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "zcgtqfUhyiRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are optimizers ? Explain Adam, RMS prop and Momentum in detail"
      ],
      "metadata": {
        "id": "FngV9WtV7JxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimizers** are algorithms that adjust the weights of a neural network during training to reduce the loss function. They control how gradients are used to modify model parameters, which significantly impacts training speed and stability.\n",
        "\n",
        "***Momentum*** enhances gradient descent by incorporating a fraction of the previous update into the current one. This speeds up learning in the right direction and dampens oscillations.\n",
        "\n",
        "***RMSProp*** adapts the learning rate for each parameter by tracking a moving average of squared gradients. This prevents overly large updates and stabilizes training, particularly when gradients fluctuate significantly.\n",
        "\n",
        "***Adam*** integrates the advantages of Momentum and RMSProp by keeping track of both the moving average of gradients and their squares. Its fast convergence and robustness make it a popular choice."
      ],
      "metadata": {
        "id": "8zHxNuNdzYBT"
      }
    }
  ]
}