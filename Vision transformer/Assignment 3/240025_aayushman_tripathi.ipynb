{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**SUBMISSION INSTRUCTIONS**\n",
        "\n",
        "First make a copy of this colab file and then solve the assignment and upload your final notebook on github.\n",
        "\n",
        "**To write an answer to any question, First Add a TEXT block below the question and type it in only one single block**\n",
        "\n",
        "Before uploading your downloaded notebook, RENAME the file as rollno_name.ipynb\n",
        "\n",
        "Submission Deadline : 19/12/2025 Friday EOD i.e before 11:59 PM\n",
        "\n",
        "The deadline is strict and will not be extended, Late submissions are not allowed\n",
        "\n",
        "Note that you have to upload your solution on the github page of the project Vision Transformer and **under Assignment 3**\n",
        "\n",
        "And remember to keep title of your pull request to be ViT_name_rollno_assgn3\n",
        "\n",
        "Github Submission repo -\n",
        "https://github.com/electricalengineersiitk/Winter-projects-25-26/tree/main/Vision%20transformer/Assignment%203"
      ],
      "metadata": {
        "id": "XmTMSeaTLwVp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**THEORETICAL ASSIGNMENT**\n",
        "\n"
      ],
      "metadata": {
        "id": "-TjKxTBEz8i7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This assignment consists of theoretical questions. Objective answers will not be accepted. You need to write detailed answers (200-300 words) for the questions that require more in-depth explanations (you should be able to identify these after reading the question and finding about them). Questions that need less explanation can be answered in 20-100 words. Please ensure the answers are well-written and thorough."
      ],
      "metadata": {
        "id": "AnVa1_8AKqtv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Assume that the inputs **X**\n",
        " to some scalar function **f**\n",
        " are **n x m**\n",
        " matrices. What is the dimensionality of the gradient of **f**\n",
        " with respect to **X**?\n",
        "**Give reasons to justify your answer.**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ERhYz9qV_YXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the gradient of f w.r.t X has the same dimensions as x which is m*n.\n",
        "\n",
        "**Justification** - The gradient represents the collection of partial derivatives of the scalar function X with respect to the independent variables of X .since there are only one partial derivative to each element of X, therefore the gradient must have the same shape"
      ],
      "metadata": {
        "id": "HpoojAvqIjGv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Define the evaluation metrics Accuracy, Precision, Recall, and F1 score.\n",
        "\n",
        "   Consider the following diagram:\n",
        "   Find the values of Accuracy, Precision, Recall, and F1 score.\n",
        "   |                | Actual: Cancer | Actual: No Cancer |\n",
        "   |----------------|---------------|------------------|\n",
        "   | Predicted: Cancer | 80 | 80 |\n",
        "   | Predicted: No Cancer | 20 | 820 |\n",
        "    \n",
        "   \n",
        "   Find the values of Accuracy, Precision, Recall, and F1 score using the above data.\n"
      ],
      "metadata": {
        "id": "9vnyYHlq2g4B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Accuracy**:  measures the overall correctness of a classification model. It is the proportion of total predictions that are correct, considering both positive and negative classes. While accuracy is intuitive, it can be misleading when the dataset is imbalanced.\n",
        "\n",
        "**Precision**:  focuses on the reliability of positive predictions. It answers the question: Of all instances predicted as positive, how many were actually positive? Precision is important when the cost of false positives is high.\n",
        "\n",
        "**Recall**: measures the model’s ability to correctly identify actual positive cases. It answers: Of all actual positive instances, how many did the model correctly detect? Recall is crucial when missing positive cases is costly.\n",
        "\n",
        "**F1 Score**: is the harmonic mean of Precision and Recall. It provides a balanced measure when both false positives and false negatives matter, especially in imbalanced datasets.\n",
        "\n",
        "Accuracy- 0.9\n",
        "\n",
        "Precision- 0.5\n",
        "\n",
        "Recall- 0.8\n",
        "\n",
        "F1 score- 0.615\n"
      ],
      "metadata": {
        "id": "dNP03sgtJ_Di"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is a confusion matrix ?"
      ],
      "metadata": {
        "id": "h1dJMD0vJKZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A confusion matrix is a tabular representation used to evaluate the performance of a classification model by comparing the predicted labels with the actual (ground truth) labels. It shows not only how many predictions were correct, but also the types of errors the model made, which helps in understanding model behavior beyond simple accuracy.\n",
        "\n",
        "For a binary classification problem, the confusion matrix is a\n",
        "2*2 table consisting of four outcomes:\n",
        "\n",
        "\n",
        "1.  True Positives (TP): Cases where the model correctly predicts the positive class.\n",
        "2.  False Positives (FP): Cases where the model predicts the positive class, but the actual class is negative (also called Type I error).\n",
        "\n",
        "3.   False Negatives (FN): Cases where the model predicts the negative class, but the actual class is positive (Type II error).\n",
        "4.   True Negatives (TN): Cases where the model correctly predicts the negative class.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JB5Q6Vl3MsAi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is overfitting and underfitting ?"
      ],
      "metadata": {
        "id": "aOqF8Dzw0DdZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Underfitting** occurs when a model is too simple to capture the underlying patterns in the data. Such a model fails to learn important relationships between input features and the target variable, resulting in poor performance on both training and test datasets. Underfitting often happens when the model has insufficient complexity (e.g., using a linear model for highly non-linear data), too few training iterations, or excessive regularization. An underfit model shows high bias and low variance, meaning it makes strong assumptions about the data and cannot adapt well.\n",
        "\n",
        "**Overfitting**, on the other hand, occurs when a model learns the training data too well, including noise and random fluctuations. As a result, it performs very well on the training dataset but poorly on new, unseen data. Overfitting typically arises when the model is overly complex, has too many parameters, or is trained for too long without proper regularization. Such models exhibit low bias but high variance, making them sensitive to small changes in the input data.\n"
      ],
      "metadata": {
        "id": "Un_GXcYFNSZ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Explain vanishing and exploding gradients, give reasons why they occur and how they can be prevented.\n"
      ],
      "metadata": {
        "id": "n4KfKc0z0ulm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vanishing** **gradients** occur when the gradients of the loss function with respect to earlier layers become extremely small during backpropagation. As gradients are propagated backward through the network, they are repeatedly multiplied by weights and derivatives of activation functions. If these values are less than one (as in sigmoid or tanh activations), the gradients shrink exponentially. Consequently, early layers learn very slowly or stop learning altogether. This problem is common in deep feedforward networks and traditional RNNs, where long-term dependencies are hard to capture.\n",
        "\n",
        "**Exploding** **gradients**, in contrast, occur when gradients grow exponentially as they are propagated backward. This happens when weights or activation derivatives are greater than one, causing gradients to increase rapidly. Exploding gradients lead to unstable training, very large weight updates, numerical overflow, and divergence of the model."
      ],
      "metadata": {
        "id": "_72iCrRZO2OA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is regularization ? Explain L1 and L2 regularization"
      ],
      "metadata": {
        "id": "6yaOipvU1JoD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularization is a set of techniques used in machine learning to prevent overfitting by discouraging overly complex models. It works by adding a penalty term to the loss (cost) function, which constrains the magnitude of model parameters. By limiting how large the weights can grow, regularization encourages simpler models that generalize better to unseen data.\n",
        "\n",
        "L1 Regularization:\n",
        "\n",
        "L1 regularization adds the sum of the absolute values of the weights to the loss function:\n",
        "\n",
        "1- Useful when dealing with high-dimensional data where many features may be irrelevant.\n",
        "\n",
        "2- Produces simpler, more interpretable models.\n",
        "\n",
        "L2 Regularization:\n",
        "\n",
        "L2 regularization adds the sum of the squared values of the weights to the loss function:\n",
        "\n",
        "1- Penalizes large weights more heavily than small ones.\n",
        "\n",
        "2- Shrinks weights toward zero but rarely makes them exactly zero.\n",
        "\n",
        "3- Commonly used in neural networks and regression problems."
      ],
      "metadata": {
        "id": "yjrss3DrO52A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What is Dropout Layer and To which part of a neural network is dropout generally applied? how does it prevent overfitting"
      ],
      "metadata": {
        "id": "NjeRGpCE7QJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Dropout layer is a regularization technique used in neural networks to reduce overfitting by randomly “dropping” (i.e., temporarily deactivating) a fraction of neurons during training. When a neuron is dropped, its output is set to zero and it does not participate in forward or backward propagation for that training iteration.\n",
        "\n",
        "Dropout is generally applied to:\n",
        "\n",
        "Hidden layers of a neural network, especially fully connected (dense) layers.\n",
        "\n",
        "It can also be applied after convolutional layers, though less aggressively (often using variants like spatial dropout).\n",
        "\n",
        "Dropout prevents overfitting by forcing the network to learn redundant and robust representations. Since neurons are randomly removed during training, the network cannot rely on specific neurons or paths to make predictions. This reduces co-adaptation of neurons, where certain neurons become overly dependent on others.By increasing model robustness and reducing variance, dropout improves generalization and helps the model perform better on unseen data."
      ],
      "metadata": {
        "id": "ye9z_zqcQB6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. A hidden layer of a neural network has the following activations:\n",
        "\n",
        "   \\[\n",
        "   a = [2.0, 5.0, 7.12, 4.5, 6.0]\n",
        "   \\]\n",
        "\n",
        "   Dropout is applied **during training** with dropout probability \\(p = 0.5\\).\n",
        "\n",
        "   A randomly generated dropout mask is:\n",
        "\n",
        "   \\[\n",
        "   m = [1, 0, 0, 1, 1]\n",
        "   \\]\n",
        "\n",
        "   Here,  \n",
        "   - `1` → neuron is **kept**  \n",
        "   - `0` → neuron is **dropped**\n",
        "\n",
        "   ---\n",
        "\n",
        "   **Question:**  \n",
        "   What will be the output activations of this layer **after applying dropout**?\n",
        "   *(Show the final activation vector)*\n"
      ],
      "metadata": {
        "id": "jLZFLyv4-A67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "During training, dropout is applied by element-wise multiplication of the activation vector with the dropout mask:\n",
        "So the activation vector after applying dropout layer is - [2.0 , 0 , 0 , 4.5 , 6.0]"
      ],
      "metadata": {
        "id": "ijLgGw11QnJV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Explain the difference between Gradient Descent, Stochastic Gradient Descent, Mini-Batch Gradient Descent"
      ],
      "metadata": {
        "id": "La3K0HmUGj8t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Gradient Descent (Batch Gradient Descent)\n",
        "\n",
        "In Gradient Descent, the gradient of the loss function is computed using the entire training dataset for every parameter update.\n",
        "\n",
        "Update rule: One update per epoch\n",
        "\n",
        "Advantages: Stable and smooth convergence toward the minimum\n",
        "\n",
        "Disadvantages: Computationally expensive and slow for large datasets; requires large memory\n",
        "\n",
        "Use case: Small datasets where full-batch computation is feasible\n",
        "\n",
        "Because each update uses all samples, the direction of descent is accurate, but training can be very slow.\n",
        "\n",
        "2. Stochastic Gradient Descent (SGD)\n",
        "\n",
        "In Stochastic Gradient Descent, parameters are updated using only one training example at a time.\n",
        "\n",
        "Update rule: One update per training example\n",
        "\n",
        "Advantages: Faster updates, low memory usage, can escape local minima\n",
        "\n",
        "Disadvantages: High variance in updates, noisy convergence, loss fluctuates\n",
        "\n",
        "Use case: Very large or streaming datasets\n",
        "\n",
        "SGD introduces randomness, which helps generalization but makes convergence less stable.\n",
        "\n",
        "3. Mini-Batch Gradient Descent\n",
        "\n",
        "Mini-Batch Gradient Descent is a compromise between GD and SGD. The gradient is computed using a small batch of samples (e.g., 32, 64, 128).\n",
        "\n",
        "Update rule: One update per mini-batch\n",
        "\n",
        "Advantages: Efficient, stable, compatible with parallel computation (GPUs)\n",
        "\n",
        "Disadvantages: Requires tuning batch size\n",
        "\n",
        "Use case: Most deep learning applications"
      ],
      "metadata": {
        "id": "LLaPcWipSNkW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are optimizers ? Explain Adam, RMS prop and Momentum in detail"
      ],
      "metadata": {
        "id": "FngV9WtV7JxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimizers are algorithms used in machine learning and deep learning to update model parameters (weights and biases) during training in order to minimize a loss function. They improve upon basic gradient descent by controlling how fast, in which direction, and how stably parameters are updated, leading to faster convergence and better performance.\n",
        "\n",
        "1. Momentum Optimizer\n",
        "\n",
        "\n",
        "Momentum accelerates gradient descent by accumulating a velocity vector in directions of consistent gradients, similar to a ball rolling downhill.\n",
        "\n",
        "Explanation:\n",
        "\n",
        "Reduces oscillations in steep directions.\n",
        "\n",
        "Speeds up convergence in shallow regions.\n",
        "\n",
        "Helps escape local minima.\n",
        "\n",
        "2. RMSProp (Root Mean Square Propagation)\n",
        "\n",
        "\n",
        "RMSProp adapts the learning rate individually for each parameter based on the magnitude of recent gradients.\n",
        "\n",
        "Explanation:\n",
        "\n",
        "Divides the gradient by a running average of squared gradients.\n",
        "\n",
        "Prevents excessively large updates.\n",
        "\n",
        "Works well for non-stationary objectives.\n",
        "\n",
        "3. Adam (Adaptive Moment Estimation)\n",
        "\n",
        "\n",
        "Adam combines the strengths of Momentum and RMSProp by maintaining both first and second moment estimates of gradients.\n",
        "\n",
        "Explanation:\n",
        "\n",
        "Fast convergence.\n",
        "\n",
        "Robust to noisy gradients.\n",
        "\n",
        "Widely used default optimizer in deep learning."
      ],
      "metadata": {
        "id": "qZZi-4EKSlVP"
      }
    }
  ]
}