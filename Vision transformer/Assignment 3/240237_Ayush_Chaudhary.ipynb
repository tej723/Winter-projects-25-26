{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**SUBMISSION INSTRUCTIONS**\n",
        "\n",
        "First make a copy of this colab file and then solve the assignment and upload your final notebook on github.\n",
        "\n",
        "**To write an answer to any question, First Add a TEXT block below the question and type it in only one single block**\n",
        "\n",
        "Before uploading your downloaded notebook, RENAME the file as rollno_name.ipynb\n",
        "\n",
        "Submission Deadline : 19/12/2025 Friday EOD i.e before 11:59 PM\n",
        "\n",
        "The deadline is strict and will not be extended, Late submissions are not allowed\n",
        "\n",
        "Note that you have to upload your solution on the github page of the project Vision Transformer and **under Assignment 3**\n",
        "\n",
        "And remember to keep title of your pull request to be ViT_name_rollno_assgn3\n",
        "\n",
        "Github Submission repo -\n",
        "https://github.com/electricalengineersiitk/Winter-projects-25-26/tree/main/Vision%20transformer/Assignment%203"
      ],
      "metadata": {
        "id": "XmTMSeaTLwVp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**THEORETICAL ASSIGNMENT**\n",
        "\n"
      ],
      "metadata": {
        "id": "-TjKxTBEz8i7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This assignment consists of theoretical questions. Objective answers will not be accepted. You need to write detailed answers (200-300 words) for the questions that require more in-depth explanations (you should be able to identify these after reading the question and finding about them). Questions that need less explanation can be answered in 20-100 words. Please ensure the answers are well-written and thorough."
      ],
      "metadata": {
        "id": "AnVa1_8AKqtv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Assume that the inputs **X**\n",
        " to some scalar function **f**\n",
        " are **n x m**\n",
        " matrices. What is the dimensionality of the gradient of **f**\n",
        " with respect to **X**?\n",
        "**Give reasons to justify your answer.**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ERhYz9qV_YXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As function f is a scalar and its input variable X is provided as an n x m matrix, the magnitude of the gradient of function f with respect to X will still be n x m.\n",
        "\n",
        "This is because each element in the matrix X is an independent variable that contribute to the determination of the value of the scalar function, f. The matrix X, will be of n rows and m columns. This will mean that it will be made up of nm individual elements. In order to establish the effect of changes in the value of the function, f, to changes in the value of the matrix, X, the partial derivatives of the function, f, with respect to each element in the matrix, X, will have to be determined.\n",
        "\n",
        "Therefore, the gradient is created by arranging all these partial derivatives side by side, in much the same way as the matrix X. In this way, corresponding to each entry in matrix X, there is an entry in the gradient showing how the function f is changing with regard to that entry.\n",
        "\n",
        "Another explanation also exists for the mentioned concept, referred to as \"total change,\" above. A small change change in the scalar function \"f\" can also be defined as a sum of a change due to a small change in each and every component of \"X\". To do this, it must be compatible, or match, with the dimensions of \"X\".\n",
        "\n",
        "Consequently, the gradient of a scalar function for an n x m matrix will also generate an n x m matrix."
      ],
      "metadata": {
        "id": "abzhNy8f2FQA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Define the evaluation metrics Accuracy, Precision, Recall, and F1 score.\n",
        "\n",
        "   Consider the following diagram:\n",
        "   Find the values of Accuracy, Precision, Recall, and F1 score.\n",
        "   |                | Actual: Cancer | Actual: No Cancer |\n",
        "   |----------------|---------------|------------------|\n",
        "   | Predicted: Cancer | 80 | 80 |\n",
        "   | Predicted: No Cancer | 20 | 820 |\n",
        "    \n",
        "   \n",
        "   Find the values of Accuracy, Precision, Recall, and F1 score using the above data.\n"
      ],
      "metadata": {
        "id": "9vnyYHlq2g4B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definitions\n",
        "\n",
        "Accuracy measurements indicate the total number of correct predictions. Accuracy is defined as the “measure of how often the model predicts correctly.\n",
        "\n",
        "Precision: Precision can be explained as the number of actual positive predictions with respect to the total number of predictions assumed positive. Precision can give an idea of the purity of positive predictions.\n",
        "\n",
        "\"Recall\": This refers to the ratio of properly predicted positive cases to the total actual positive cases. This measures the capacity to properly forecast positive cases.\n",
        "\n",
        "The F1 score is the harmonic mean of Precision & Recall. It can be used in case of imbalanced classes.\n",
        "\n",
        "Part 2: Based on Given Confusion Matrix\n",
        "\n",
        "Predicted Cancer & Actual Cancer = 80   (True Positive, TP)\n",
        "Predicated Cancer & Actual No Cancer = 80 (False Positive, FP)\n",
        "Predicted No Cancer and Actual Cancer = 20     (False Negative, FN)\n",
        "\n",
        "Predicted No Cancer and Actual No Cancer = 820 (True Negative, TN)\n",
        "\n",
        "Total Samples = 80 + 80 + 20 + 820 = 1000\n",
        "\n",
        "Accuracy\n",
        "\n",
        "Accuracy = (TP + TN)/ Total\n",
        "Accuracy = (80 + 820) / 1000\n",
        "\n",
        "Accuracy = 900 / 1000\n",
        "\n",
        "Accuracy = 0.9 or 90 percent\n",
        "\n",
        "\n",
        "Precision\n",
        "Precision = TP / (TP + FP)\n",
        "\n",
        "Precision = 80 / (80 + 80)\n",
        "\n",
        "Precision = 80/160\n",
        "\n",
        "Precision = 0.5 or 50 percent\n",
        "\n",
        "Recall\n",
        "\n",
        "Recall = TP / (TP + FN)\n",
        "\n",
        "Recall= 80 / (80 + 20)\n",
        "\n",
        "Recall = 80 / 100\n",
        "\n",
        "Recall Rate = 0.8 or 80%\n",
        "\n",
        "F1 Score\n",
        "\n",
        "F1 score= 2 * (Precision * Recall) / (Precision + Recall)\n",
        "\n",
        "Hence, precision and recall are required for calculating precision, and for calculating recall.\n",
        "\n",
        "F1 score = 2 * (0.5 * 0.8)/(0.5+0.8)\n",
        "F1 score = 2 * 0.4/1.3\n",
        "F1 score = 0.8/1.3\n",
        "F1 score = 0.615 = 61.5%\n",
        "\n",
        "The function Final Values Accuracy = 0.9 Precision = 0.5 Recall = 0.8 F1 score ≈ 0.615"
      ],
      "metadata": {
        "id": "g9TQVIiUECcm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is a confusion matrix ?"
      ],
      "metadata": {
        "id": "h1dJMD0vJKZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One tool for evaluating performance in machine learning classification problems is a confusion matrix. By contrasting the actual class labels with the predicted class labels, it summarizes the model's predictions. The number of correctly and incorrectly classified instances is displayed in the matrix. It aids in comprehending the various kinds of mistakes the model makes, such as predicting a positive class as negative or the opposite. Crucial evaluation metrics such as accuracy, precision, recall, and F1 score can be obtained from a confusion matrix."
      ],
      "metadata": {
        "id": "bOzhvvnj00tH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is overfitting and underfitting ?"
      ],
      "metadata": {
        "id": "aOqF8Dzw0DdZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When a machine learning model learns the training data—including noise and irrelevant patterns—too well, it is said to have overfitted. Consequently, it performs poorly on unseen data but exceptionally well on training data. When a model is too basic to identify the underlying patterns in the data, underfitting takes place. Both training and new data are poorly handled by such a model. Reduced model generalization and poorer predictive performance result from both overfitting and underfitting."
      ],
      "metadata": {
        "id": "E-DRVgPf2ZSj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Explain vanishing and exploding gradients, give reasons why they occur and how they can be prevented.\n"
      ],
      "metadata": {
        "id": "n4KfKc0z0ulm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The vanishing and exploding gradients are two issues that are commonly encountered during the training of deep networks, including the recurrent networks and the deeply layered feedforward networks.These two issues are found to emerge during backpropagation pass. In the backpropagation pass, gradients are passed backwards from the output layer to earlier layers. Gradient explosion happens when the gradient grow very large during the backpropagation process. In these circumstances, the weight update values are very large and the training becomes unstable. This kind of issue is mostly when the weights contain large values or when the gradients are repeatedly multiplied above one. This is especially true in deep learning. These issues both negatively impact the training phase and hinder the convergence of the model at the optimum solution. There are various methods that can be employed in order to deal with both issues. Appropriate initialization of the weights enables regulating the magnitude of the gradients. Activation functions such as ReLU can be employed in order to avoid the issue of vanishing gradients. Clipping gradients can be employed to deal with exploding gradients in a network. It limits the maximum value of the gradients. In the design of recurrent neural networks, models such as LSTMs and GRUs can handle both issues.\n",
        "\n",
        "Through these approaches, it is possible to train neural networks effectively and get better results."
      ],
      "metadata": {
        "id": "hdfNG6m25Gjx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is regularization ? Explain L1 and L2 regularization"
      ],
      "metadata": {
        "id": "6yaOipvU1JoD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularization is a technique applied in machine learning to avoid overfitting. Such cases may arise when the algorithm in the machine learning identifies not only the underlying concepts of a given set of information but also captures noise or random variation in the set. In the process, this may lead to an algorithm setting inaccurate results. There are three types of regularizations:\n",
        "The regularizations are generally used to keep a check on the magnitude of the model parameters, weights in case of neural network and coefficients in case of regression models. It doesn't allow these parameters to be very large, keeping the models focused only on the important features of the data sets and less sensitive to the fluctuations.\n",
        "\n",
        "L1 regularization is a technique that involves adding a term based on the absolute values of the coefficients in the model. This is a type of regularization that results in sparse models because it forces many coefficients to become zero. Actually, this kind of regularization helps perform automatic feature selection, because it gets rid of the irrelevant features. Indeed, it results in a simple model.\n",
        "\n",
        "L2 regularization adds a term to the cost function, a function of a magnitude of the square of the model weights. Unlike L1, which does not, this technique sets the weights to zero but tries to make them small and equal. This corrects the tendency of one feature to highly affect the model outputs. This kind of regression assists in retrieving a stable model that gives smooth solutions. L1 and L2 regularization techniques are considered effective ways to improve generalization performance and useful techniques to apply while developing machine learning models."
      ],
      "metadata": {
        "id": "EdrfvAlF6nYr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What is Dropout Layer and To which part of a neural network is dropout generally applied? how does it prevent overfitting"
      ],
      "metadata": {
        "id": "NjeRGpCE7QJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Dropout layer is a kind of regularization method applied to a neural network to prevent overfitting. Overfitting occurs when the neural network is reliant on some particular neurons within the network and learns useless information given to the data. The effect of overfitting can be eliminated using a Dropout layer, where some specific neurons are turned off randomly within the network. The dropout method is generally applied to hidden layers within a neural network and applied specifically to fully connected layers. In some cases, dropout can also be applied to some layers of inputs but less often. It is not usually applied to the output layer because turning off some output might influence the output results. When using dropout within a neural network for training, some specific neurons and connection are removed for this process and are not involved in any process within both forward and backward passing. When dropout is applied for testing within a neural network, this process is somehow ignored and works normally. The main function and objective of using dropout within a neural network is to prevent overfitting within the network. It ensures that the network is not overly reliant on some specific neurons. The fact that some specific different neurons are dropped at random within every step ensures that the network is learning some robust results within the given data. The other major role played by dropout within a neural network is acting as a model average because every step observes a slight different version within a neural network.\n",
        "\n",
        "The reason that dropout prevents the neurons from adapting too much is that it forces the model to learn features that are more diverse. As a result, the model will not rely much on the current input data but rather on new data. Even though the dropout concept seems simple, its effect on improving deep learning models’ generalization performance has been dramatic."
      ],
      "metadata": {
        "id": "n81U2jl66Xwf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. A hidden layer of a neural network has the following activations:\n",
        "\n",
        "   \\[\n",
        "   a = [2.0, 5.0, 7.12, 4.5, 6.0]\n",
        "   \\]\n",
        "\n",
        "   Dropout is applied **during training** with dropout probability \\(p = 0.5\\).\n",
        "\n",
        "   A randomly generated dropout mask is:\n",
        "\n",
        "   \\[\n",
        "   m = [1, 0, 0, 1, 1]\n",
        "   \\]\n",
        "\n",
        "   Here,  \n",
        "   - `1` → neuron is **kept**  \n",
        "   - `0` → neuron is **dropped**\n",
        "\n",
        "   ---\n",
        "\n",
        "   **Question:**  \n",
        "   What will be the output activations of this layer **after applying dropout**?\n",
        "   *(Show the final activation vector)*\n"
      ],
      "metadata": {
        "id": "jLZFLyv4-A67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the Activations of the Hidden Layer a = [2.0,5.0,7.12,4.5,6.0]\n",
        "\n",
        "The probability of dropout p = 0.5 so keep probability is 1-p which is 0.5\n",
        "\n",
        "The dropout mask is m =[1,0,0,1,1]\n",
        "\n",
        "Apply drouput by element wise multiplication\n",
        "a after mask = a * m\n",
        " = [2 * 1, 5 * 1, 7.12 * 0, 4.5 * 1, 6 * 1]\n",
        " = [2,0,4.5, 6]\n",
        "\n",
        " Using inverted drouput the activations are divided by the keep probability 0.5\n",
        "\n",
        " Final output = [2/0.5, 0/0.5, 4.5/0.5,6/0.5] =4.0, 0, 9.0, 12.0]\n",
        "\n",
        " Final activation vector after applying dropout is [4.0, 0, 9, 12]"
      ],
      "metadata": {
        "id": "li9eqD0R96ZN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Explain the difference between Gradient Descent, Stochastic Gradient Descent, Mini-Batch Gradient Descent"
      ],
      "metadata": {
        "id": "La3K0HmUGj8t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Descent, Stochastic Gradient Descent, and Mini-Batch Gradient Descent are optimization techniques that help in minimizing a loss function of a machine learning model. These techniques differ from each other based on the amount of data from the training data that is used for calculating the gradient while updating. Gradient Descent, also referred to as Batch Gradient Descent, involves calculating the gradient with all observations at once. It results in a very smooth and precise estimate of the gradient, ensuring smooth convergence to a minimum. However, this process can be very time-consuming with large amounts of data, as all data must be processed prior to every update of every parameter. It also requires heavy memory and does not support online learning.\n",
        "\n",
        "Stochastic Gradient Descent SGD apply the model parameters based only on single randomly chosen example from training data. Thus the process are very fast. Unfortunately, SGD has a high noise level. Moreover, the function may have a jagged path toward minimizing. The high noise levels lead to a non-smooth convergence path. Moreover, SGD converges immediately. It begins learning immediately. For instance, given that SGD converges immediately, the model can begin learning immediately. Thus, it is a benefit in scenarios where learning is limited by time. Unfortunately, the stochastic noise affects the convergence path. Thus, despite overcoming local\n",
        "\n",
        "Mini Batch Gradient Descent is the middle-ground solution between the above two. It requires the calculation of the gradients on a small sample of the training set called the mini-batch. This is a good compromise between the speed of calculation and the guarantee of stable convergence. It is much faster compared to the regular Batch Gradient Descent because it is not calculating the gradients for the entire training set. On the other hand, it is much more stable compared to Stochastic Gradient Descent because it is calculating the gradients for a sample of the training set."
      ],
      "metadata": {
        "id": "AP-Q10pz73CJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are optimizers ? Explain Adam, RMS prop and Momentum in detail"
      ],
      "metadata": {
        "id": "FngV9WtV7JxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimizers, also known as optimizers algorithms, learning algorithms, and neural networks, are used within machine learning models and neural networks that optimize model parameters by minimizing the loss functions during training. The optimizer is responsible for determining the values by which the weights need to be adjusted based on the gradient values while training the models, and a healthy optimizer enables the model to converge well during training and perform better.Momentum is an optimization algorithm that enhances basic Gradient Descent by learning from the past gradient updates. In Gradient Descent, the model updates the weights solely on the basis of the present gradient. However, in the case of the momentum algorithm, instead of updating the weights based solely on the present gradient, it calculates the average of the past gradients and updates the weights in that way. This allows the weights to update at a increased rate, particularly where the loss landscape is rough.\n",
        "\n",
        "RMS prop: This stands for Root Mean Square Propagation. This algorithm has been designed for handling the problem of different learning rates for each parameter. It calculates the moving averages of the squared values of the gradients for different parameters and adjusts the learning rates accordingly. This means that the parameters with large gradients will learn smaller values, whereas the parameters with smaller gradients will learn larger values.\n",
        "\n",
        "\"Adam\" which stands for Adaptive Moment Estimation, is a combination of ideas from \"Momentum\" and \"RMSProp\" It attempts to keep track of two values the average value of the gradients and the average squared value of the gradients. By doing this, it adjusts the learning rate for every individual parameter as well as utilizing the idea of momentum. It is a very commonly used learning algorithm as it converges quite fast and requires very little parameter tuning.\n",
        "\n",
        "In conclusion, optimizers are an essential part of efficient and accurate neural network training."
      ],
      "metadata": {
        "id": "otpXL-wR8Hia"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iX7AxAY48bht"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
