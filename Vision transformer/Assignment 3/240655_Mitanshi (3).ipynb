{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**SUBMISSION INSTRUCTIONS**\n",
        "\n",
        "First make a copy of this colab file and then solve the assignment and upload your final notebook on github.\n",
        "\n",
        "**To write an answer to any question, First Add a TEXT block below the question and type it in only one single block**\n",
        "\n",
        "Before uploading your downloaded notebook, RENAME the file as rollno_name.ipynb\n",
        "\n",
        "Submission Deadline : 19/12/2025 Friday EOD i.e before 11:59 PM\n",
        "\n",
        "The deadline is strict and will not be extended, Late submissions are not allowed\n",
        "\n",
        "Note that you have to upload your solution on the github page of the project Vision Transformer and **under Assignment 3**\n",
        "\n",
        "And remember to keep title of your pull request to be ViT_name_rollno_assgn3\n",
        "\n",
        "Github Submission repo -\n",
        "https://github.com/electricalengineersiitk/Winter-projects-25-26/tree/main/Vision%20transformer/Assignment%203"
      ],
      "metadata": {
        "id": "XmTMSeaTLwVp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**THEORETICAL ASSIGNMENT**\n",
        "\n"
      ],
      "metadata": {
        "id": "-TjKxTBEz8i7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This assignment consists of theoretical questions. Objective answers will not be accepted. You need to write detailed answers (200-300 words) for the questions that require more in-depth explanations (you should be able to identify these after reading the question and finding about them). Questions that need less explanation can be answered in 20-100 words. Please ensure the answers are well-written and thorough."
      ],
      "metadata": {
        "id": "AnVa1_8AKqtv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Assume that the inputs **X**\n",
        " to some scalar function **f**\n",
        " are **n x m**\n",
        " matrices. What is the dimensionality of the gradient of **f**\n",
        " with respect to **X**?\n",
        "**Give reasons to justify your answer.**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ERhYz9qV_YXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let\n",
        "ğ‘“\n",
        "(\n",
        "ğ‘‹\n",
        ")\n",
        "f(X) be a scalar-valued function and\n",
        "ğ‘‹\n",
        "âˆˆ\n",
        "ğ‘…\n",
        "ğ‘›\n",
        "Ã—\n",
        "ğ‘š\n",
        "XâˆˆR\n",
        "nÃ—m\n",
        ". The gradient of\n",
        "ğ‘“\n",
        "f with respect to\n",
        "ğ‘‹\n",
        "X, denoted as\n",
        "âˆ‡\n",
        "ğ‘‹\n",
        "ğ‘“\n",
        "âˆ‡\n",
        "X\n",
        "\tâ€‹\n",
        "\n",
        "f, has the same dimensionality as\n",
        "ğ‘‹\n",
        "X, i.e., an\n",
        "ğ‘›\n",
        "Ã—\n",
        "ğ‘š\n",
        "nÃ—m matrix.\n",
        "\n",
        "This is because the gradient represents the partial derivative of the scalar function with respect to each individual element of the input matrix. Formally,\n",
        "\n",
        "âˆ‡\n",
        "ğ‘‹\n",
        "ğ‘“\n",
        "=\n",
        "[\n",
        "âˆ‚\n",
        "ğ‘“\n",
        "âˆ‚\n",
        "ğ‘‹\n",
        "11\n",
        "\n",
        "â‹¯\n",
        "\n",
        "âˆ‚\n",
        "ğ‘“\n",
        "âˆ‚\n",
        "ğ‘‹\n",
        "1\n",
        "ğ‘š\n",
        "\n",
        "\n",
        "â‹®\n",
        "\n",
        "â‹±\n",
        "\n",
        "â‹®\n",
        "\n",
        "\n",
        "âˆ‚\n",
        "ğ‘“\n",
        "âˆ‚\n",
        "ğ‘‹\n",
        "ğ‘›\n",
        "1\n",
        "\n",
        "â‹¯\n",
        "\n",
        "âˆ‚\n",
        "ğ‘“\n",
        "âˆ‚\n",
        "ğ‘‹\n",
        "ğ‘›\n",
        "ğ‘š\n",
        "]\n",
        "âˆ‡\n",
        "X\n",
        "\tâ€‹\n",
        "\n",
        "f=\n",
        "\tâ€‹\n",
        "\n",
        "âˆ‚X\n",
        "11\n",
        "\tâ€‹\n",
        "\n",
        "âˆ‚f\n",
        "\tâ€‹\n",
        "\n",
        "â‹®\n",
        "âˆ‚X\n",
        "n1\n",
        "\tâ€‹\n",
        "\n",
        "âˆ‚f\n",
        "\tâ€‹\n",
        "\n",
        "\tâ€‹\n",
        "\n",
        "â‹¯\n",
        "â‹±\n",
        "â‹¯\n",
        "\tâ€‹\n",
        "\n",
        "âˆ‚X\n",
        "1m\n",
        "\tâ€‹\n",
        "\n",
        "âˆ‚f\n",
        "\tâ€‹\n",
        "\n",
        "â‹®\n",
        "âˆ‚X\n",
        "nm\n",
        "\tâ€‹\n",
        "\n",
        "âˆ‚f\n",
        "\tâ€‹\n",
        "\n",
        "\tâ€‹\n",
        "\n",
        "\tâ€‹\n",
        "\n",
        "\n",
        "Each element in\n",
        "ğ‘‹\n",
        "X contributes independently to the value of\n",
        "ğ‘“\n",
        "f, so a corresponding derivative must exist. Therefore, the gradient must match the shape of the input matrix to fully describe how changes in each element affect the output. This consistency is crucial in optimization algorithms like gradient descent, where updates are applied element-wise."
      ],
      "metadata": {
        "id": "XQWlhXKpFVfZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Define the evaluation metrics Accuracy, Precision, Recall, and F1 score.\n",
        "\n",
        "   Consider the following diagram:\n",
        "   Find the values of Accuracy, Precision, Recall, and F1 score.\n",
        "   |                | Actual: Cancer | Actual: No Cancer |\n",
        "   |----------------|---------------|------------------|\n",
        "   | Predicted: Cancer | 80 | 80 |\n",
        "   | Predicted: No Cancer | 20 | 820 |\n",
        "    \n",
        "   \n",
        "   Find the values of Accuracy, Precision, Recall, and F1 score using the above data.\n"
      ],
      "metadata": {
        "id": "9vnyYHlq2g4B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy: Proportion of total predictions that are correct\n",
        "\n",
        "Precision: Proportion of positive predictions that are correct\n",
        "\n",
        "Recall: Proportion of actual positives that are correctly identified\n",
        "\n",
        "F1-Score: Harmonic mean of precision and recall\n",
        "\n",
        "Given Confusion Matrix\n",
        "\tActual Cancer\tActual No Cancer\n",
        "Predicted Cancer\t80 (TP)\t80 (FP)\n",
        "Predicted No Cancer\t20 (FN)\t820 (TN)\n",
        "\n",
        "Total samples = 1000\n",
        "\n",
        "Calculations\n",
        "\n",
        "Accuracy\n",
        "\n",
        "ğ‘‡\n",
        "ğ‘ƒ\n",
        "+\n",
        "ğ‘‡\n",
        "ğ‘\n",
        "ğ‘‡\n",
        "ğ‘œ\n",
        "ğ‘¡\n",
        "ğ‘\n",
        "ğ‘™\n",
        "=\n",
        "80\n",
        "+\n",
        "820\n",
        "1000\n",
        "=\n",
        "0.90\n",
        "\n",
        "(\n",
        "90\n",
        "%\n",
        ")\n",
        "Total\n",
        "TP+TN\n",
        "\tâ€‹\n",
        "\n",
        "=\n",
        "1000\n",
        "80+820\n",
        "\tâ€‹\n",
        "\n",
        "=0.90(90%)\n",
        "\n",
        "Precision\n",
        "\n",
        "ğ‘‡\n",
        "ğ‘ƒ\n",
        "ğ‘‡\n",
        "ğ‘ƒ\n",
        "+\n",
        "ğ¹\n",
        "ğ‘ƒ\n",
        "=\n",
        "80\n",
        "80\n",
        "+\n",
        "80\n",
        "=\n",
        "0.50\n",
        "\n",
        "(\n",
        "50\n",
        "%\n",
        ")\n",
        "TP+FP\n",
        "TP\n",
        "\tâ€‹\n",
        "\n",
        "=\n",
        "80+80\n",
        "80\n",
        "\tâ€‹\n",
        "\n",
        "=0.50(50%)\n",
        "\n",
        "Recall\n",
        "\n",
        "ğ‘‡\n",
        "ğ‘ƒ\n",
        "ğ‘‡\n",
        "ğ‘ƒ\n",
        "+\n",
        "ğ¹\n",
        "ğ‘\n",
        "=\n",
        "80\n",
        "80\n",
        "+\n",
        "20\n",
        "=\n",
        "0.80\n",
        "\n",
        "(\n",
        "80\n",
        "%\n",
        ")\n",
        "TP+FN\n",
        "TP\n",
        "\tâ€‹\n",
        "\n",
        "=\n",
        "80+20\n",
        "80\n",
        "\tâ€‹\n",
        "\n",
        "=0.80(80%)\n",
        "\n",
        "F1-Score\n",
        "\n",
        "2\n",
        "Ã—\n",
        "ğ‘ƒ\n",
        "ğ‘Ÿ\n",
        "ğ‘’\n",
        "ğ‘\n",
        "ğ‘–\n",
        "ğ‘ \n",
        "ğ‘–\n",
        "ğ‘œ\n",
        "ğ‘›\n",
        "Ã—\n",
        "ğ‘…\n",
        "ğ‘’\n",
        "ğ‘\n",
        "ğ‘\n",
        "ğ‘™\n",
        "ğ‘™\n",
        "ğ‘ƒ\n",
        "ğ‘Ÿ\n",
        "ğ‘’\n",
        "ğ‘\n",
        "ğ‘–\n",
        "ğ‘ \n",
        "ğ‘–\n",
        "ğ‘œ\n",
        "ğ‘›\n",
        "+\n",
        "ğ‘…\n",
        "ğ‘’\n",
        "ğ‘\n",
        "ğ‘\n",
        "ğ‘™\n",
        "ğ‘™\n",
        "=\n",
        "2\n",
        "Ã—\n",
        "0.5\n",
        "Ã—\n",
        "0.8\n",
        "1.3\n",
        "â‰ˆ\n",
        "0.615\n",
        "2Ã—\n",
        "Precision+Recall\n",
        "PrecisionÃ—Recall\n",
        "\tâ€‹\n",
        "\n",
        "=2Ã—\n",
        "1.3\n",
        "0.5Ã—0.8\n",
        "\tâ€‹\n",
        "\n",
        "â‰ˆ0.615\n",
        "\n"
      ],
      "metadata": {
        "id": "QEDhBw6_F71Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is a confusion matrix ?"
      ],
      "metadata": {
        "id": "h1dJMD0vJKZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A confusion matrix is a tabular representation used to evaluate the performance of a classification model. It compares actual class labels with predicted class labels, allowing us to see not just how many predictions were correct, but what kinds of errors were made.\n",
        "\n",
        "In binary classification, it consists of four components:\n",
        "\n",
        "True Positives (TP): Correct positive predictions\n",
        "\n",
        "True Negatives (TN): Correct negative predictions\n",
        "\n",
        "False Positives (FP): Incorrect positive predictions\n",
        "\n",
        "False Negatives (FN): Incorrect negative predictions\n",
        "\n",
        "The confusion matrix is especially useful in imbalanced datasets, such as medical diagnosis, where accuracy alone can be misleading. Metrics like precision, recall, and F1-score are derived directly from it."
      ],
      "metadata": {
        "id": "2b8pCmkMGPrk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is overfitting and underfitting ?"
      ],
      "metadata": {
        "id": "aOqF8Dzw0DdZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overfitting occurs when a model learns the training data too well, including noise and irrelevant patterns. Such a model performs excellently on training data but poorly on unseen data. It usually happens when the model is too complex relative to the dataset size.\n",
        "\n",
        "Underfitting, on the other hand, occurs when a model is too simple to capture the underlying structure of the data. It performs poorly on both training and test datasets.\n",
        "\n",
        "In practice, the goal is to find a balance between these two extremes using techniques like regularization, cross-validation, and appropriate model selection."
      ],
      "metadata": {
        "id": "kwWyoU4_GRrU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Explain vanishing and exploding gradients, give reasons why they occur and how they can be prevented.\n"
      ],
      "metadata": {
        "id": "n4KfKc0z0ulm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vanishing and exploding gradients are problems encountered during training deep neural networks, especially when using backpropagation.\n",
        "\n",
        "Vanishing gradients occur when gradients become extremely small as they propagate backward through layers. This causes early layers to learn very slowly or stop learning altogether.\n",
        "\n",
        "Exploding gradients occur when gradients grow uncontrollably large, leading to unstable training and numerical overflow.\n",
        "\n",
        "Reasons\n",
        "\n",
        "Deep networks with many layers\n",
        "\n",
        "Poor weight initialization\n",
        "\n",
        "Use of activation functions like sigmoid or tanh\n",
        "\n",
        "Prevention\n",
        "\n",
        "Proper weight initialization (Xavier, He initialization)\n",
        "\n",
        "Using ReLU-based activations\n",
        "\n",
        "Gradient clipping\n",
        "\n",
        "Batch normalization\n",
        "\n",
        "Residual connections (ResNets)"
      ],
      "metadata": {
        "id": "CHutqLcMGcQ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is regularization ? Explain L1 and L2 regularization"
      ],
      "metadata": {
        "id": "6yaOipvU1JoD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularization is a technique used to reduce overfitting by adding a penalty term to the loss function, discouraging overly complex models.\n",
        "\n",
        "L1 Regularization (Lasso)\n",
        "\n",
        "Adds the sum of absolute weights:\n",
        "\n",
        "ğœ†\n",
        "âˆ‘\n",
        "âˆ£\n",
        "ğ‘¤\n",
        "âˆ£\n",
        "Î»âˆ‘âˆ£wâˆ£\n",
        "\n",
        "It encourages sparsity, meaning some weights become exactly zero, effectively performing feature selection.\n",
        "\n",
        "L2 Regularization (Ridge)\n",
        "\n",
        "Adds the sum of squared weights:\n",
        "\n",
        "ğœ†\n",
        "âˆ‘\n",
        "ğ‘¤\n",
        "2\n",
        "Î»âˆ‘w\n",
        "2\n",
        "\n",
        "It discourages large weights and leads to smoother models but does not enforce sparsity."
      ],
      "metadata": {
        "id": "ZQCSfyEFGlUt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What is Dropout Layer and To which part of a neural network is dropout generally applied? how does it prevent overfitting"
      ],
      "metadata": {
        "id": "NjeRGpCE7QJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropout is a regularization technique where neurons are randomly â€œdroppedâ€ (set to zero) during training with a fixed probability. It is generally applied to hidden layers, not the output layer.\n",
        "\n",
        "By randomly disabling neurons, dropout prevents the network from becoming overly dependent on specific neurons. This forces the model to learn more robust and distributed representations, effectively reducing overfitting. During inference, dropout is disabled, and the full network is used with scaled weights."
      ],
      "metadata": {
        "id": "gjYcfVD-GsKz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. A hidden layer of a neural network has the following activations:\n",
        "\n",
        "   \\[\n",
        "   a = [2.0, 5.0, 7.12, 4.5, 6.0]\n",
        "   \\]\n",
        "\n",
        "   Dropout is applied **during training** with dropout probability \\(p = 0.5\\).\n",
        "\n",
        "   A randomly generated dropout mask is:\n",
        "\n",
        "   \\[\n",
        "   m = [1, 0, 0, 1, 1]\n",
        "   \\]\n",
        "\n",
        "   Here,  \n",
        "   - `1` â†’ neuron is **kept**  \n",
        "   - `0` â†’ neuron is **dropped**\n",
        "\n",
        "   ---\n",
        "\n",
        "   **Question:**  \n",
        "   What will be the output activations of this layer **after applying dropout**?\n",
        "   *(Show the final activation vector)*\n"
      ],
      "metadata": {
        "id": "jLZFLyv4-A67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given:\n",
        "\n",
        "ğ‘\n",
        "=\n",
        "[\n",
        "2.0\n",
        ",\n",
        "5.0\n",
        ",\n",
        "7.2\n",
        ",\n",
        "4.5\n",
        ",\n",
        "6.0\n",
        "]\n",
        "a=[2.0,5.0,7.2,4.5,6.0]\n",
        "\n",
        "Dropout mask:\n",
        "\n",
        "ğ‘š\n",
        "=\n",
        "[\n",
        "1\n",
        ",\n",
        "0\n",
        ",\n",
        "0\n",
        ",\n",
        "1\n",
        ",\n",
        "1\n",
        "]\n",
        "m=[1,0,0,1,1]\n",
        "\n",
        "Applying dropout (element-wise multiplication):\n",
        "\n",
        "ğ‘\n",
        "â€²\n",
        "=\n",
        "[\n",
        "2.0\n",
        ",\n",
        "0\n",
        ",\n",
        "0\n",
        ",\n",
        "4.5\n",
        ",\n",
        "6.0\n",
        "]\n",
        "a\n",
        "â€²\n",
        "=[2.0,0,0,4.5,6.0]\n",
        "\n",
        "Final activation vector:\n",
        "\n",
        "[\n",
        "2.0\n",
        ",\n",
        "\n",
        "0\n",
        ",\n",
        "\n",
        "0\n",
        ",\n",
        "\n",
        "4.5\n",
        ",\n",
        "\n",
        "6.0\n",
        "]\n",
        "[2.0,0,0,4.5,6.0]"
      ],
      "metadata": {
        "id": "1kBtDgL6GudF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Explain the difference between Gradient Descent, Stochastic Gradient Descent, Mini-Batch Gradient Descent"
      ],
      "metadata": {
        "id": "La3K0HmUGj8t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch Gradient Descent uses the entire dataset to compute gradients. It is stable but computationally expensive.\n",
        "\n",
        "Stochastic Gradient Descent (SGD) updates parameters using one data point at a time. It is faster but noisy.\n",
        "\n",
        "Mini-Batch Gradient Descent uses small batches of data. It balances efficiency and stability and is most commonly used in practice."
      ],
      "metadata": {
        "id": "uEIAKYc0G5n8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are optimizers ? Explain Adam, RMS prop and Momentum in detail"
      ],
      "metadata": {
        "id": "FngV9WtV7JxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimizers control how model parameters are updated during training.\n",
        "\n",
        "Momentum\n",
        "\n",
        "Uses a moving average of gradients to accelerate convergence and reduce oscillations.\n",
        "\n",
        "RMSprop\n",
        "\n",
        "Adapts learning rates for each parameter by maintaining a running average of squared gradients, helping with non-stationary objectives.\n",
        "\n",
        "Adam\n",
        "\n",
        "Combines Momentum and RMSprop. It keeps track of both the first moment (mean) and second moment (variance) of gradients. Adam is computationally efficient, requires little tuning, and works well in most deep learning tasks."
      ],
      "metadata": {
        "id": "v4uBHhIPG9zi"
      }
    }
  ]
}