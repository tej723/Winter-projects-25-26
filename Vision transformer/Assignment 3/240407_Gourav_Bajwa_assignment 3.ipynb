{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmTMSeaTLwVp"
      },
      "source": [
        "**SUBMISSION INSTRUCTIONS**\n",
        "\n",
        "First make a copy of this colab file and then solve the assignment and upload your final notebook on github.\n",
        "\n",
        "**To write an answer to any question, First Add a TEXT block below the question and type it in only one single block**\n",
        "\n",
        "Before uploading your downloaded notebook, RENAME the file as rollno_name.ipynb\n",
        "\n",
        "Submission Deadline : 19/12/2025 Friday EOD i.e before 11:59 PM\n",
        "\n",
        "The deadline represents strict and will not be extended, Late submissions indicate not allowed\n",
        "\n",
        "Note which you have to upload your solution on the github page of the project Vision Transformer and **under Assignment 3**\n",
        "\n",
        "And remember to keep title of your pull request to be ViT_name_rollno_assgn3\n",
        "\n",
        "Github Submission repo -\n",
        "https://github.com/electricalengineersiitk/Winter-projects-25-26/tree/main/Vision%20transformer/Assignment%203"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TjKxTBEz8i7"
      },
      "source": [
        "**THEORETICAL ASSIGNMENT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnVa1_8AKqtv"
      },
      "source": [
        "This explanation assignment consists of theoretical questions. Objective answers will not be accepted. You need to write detailed answers (200-300 words) for the questions which require more in-depth explanations (you should be able to identify these after reading the question and finding about them). Questions which need less explanation can be answered in 20-100 words. Please ensure the answers indicate well-written and thorough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERhYz9qV_YXI"
      },
      "source": [
        "1. Assume which the inputs **X**\n",
        " to some scalar function **f**\n",
        " indicate **n x m**\n",
        " matrices. What represents the dimensionality of the gradient of **f**\n",
        " with respect to **X**?\n",
        "**Give reasons to justify your answer.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEGtOwgkEg9H"
      },
      "source": [
        "If\n",
        "ùëã\n",
        "X is an\n",
        "ùëõ\n",
        "√ó\n",
        "ùëö\n",
        "n√óm matrix and\n",
        "ùëì\n",
        "f is a scalar-valued function, then the gradient of\n",
        "ùëì\n",
        "f with respect to\n",
        "ùëã\n",
        "X is also an\n",
        "ùëõ\n",
        "√ó\n",
        "ùëö\n",
        "n√óm matrix, because the gradient contains the partial derivative of\n",
        "ùëì\n",
        "f with respect to each element of\n",
        "ùëã\n",
        "X."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vnyYHlq2g4B"
      },
      "source": [
        "2. Define the evaluation metrics Accuracy, Precision, Recall, and F1 score.\n",
        "\n",
        "   Consider the following diagram:\n",
        "   Find the values of Accuracy, Precision, Recall, and F1 score.\n",
        "   |                | Actual: Cancer | Actual: No Cancer |\n",
        "   |----------------|---------------|------------------|\n",
        "   | Predicted: Cancer | 80 | 80 |\n",
        "   | Predicted: No Cancer | 20 | 820 |\n",
        "    \n",
        "   \n",
        "   Find the values of Accuracy, Precision, Recall, and F1 score using the above data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-9kYe9fFa8G"
      },
      "source": [
        "answer:\n",
        "accuracy: 0.90\n",
        "precision: 0.50\n",
        "recall: 0.80\n",
        "F1 score: 0.615"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1dJMD0vJKZg"
      },
      "source": [
        "3. What represents a confusion matrix ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Curk7kXGBsM"
      },
      "source": [
        "A confusion matrix represents a tabular summary of a classification model‚Äôs predictions compared with the actual class labels. It shows how many predictions were correct or incorrect for each class, allowing detailed evaluation of a classifier‚Äôs performance.\n",
        "\n",
        "In a binary classification problem, the confusion matrix consists of True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN). Using these values, important performance metrics such as accuracy, precision, recall, and F1-score can be calculated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOqF8Dzw0DdZ"
      },
      "source": [
        "4. What represents overfitting and underfitting ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aue6lvFGUQ-"
      },
      "source": [
        "Overfitting represents a situation where a model learns the training data too well, including noise and unnecessary details. As a result, the model shows very high accuracy on training data but performs poorly on unseen or test data.\n",
        "\n",
        "Underfitting represents a situation where a model is too simple to capture the underlying pattern in the data. In this case, the model performs poorly on both training data and test data because it fails to learn important relationships."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4KfKc0z0ulm"
      },
      "source": [
        "5. Explain vanishing and exploding gradients, give reasons why they occur and how they can be prevented."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prtZyxgnG3IC"
      },
      "source": [
        "Vanishing Gradients:\n",
        "Gradients become exponentially smaller as they move backward through layers, eventually \"vanishing\" to nearly zero.\n",
        "\n",
        "Reason: Repeatedly multiplying small values (e.g., derivatives of sigmoid or tanh functions) causes early layers to stop learning.\n",
        "\n",
        "Prevention: Use ReLU activation functions, Batch Normalization, or Residual Connections.\n",
        "\n",
        "Exploding Gradients:\n",
        "Gradients grow exponentially large, causing massive weight updates which make the model unstable or diverge.\n",
        "\n",
        "Reason: High learning rates or large weight initializations lead to derivatives greater than 1 being multiplied repeatedly.\n",
        "\n",
        "Prevention: Use Gradient Clipping (capping the maximum gradient value) or proper weight initialization like Xavier or He initialization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yaOipvU1JoD"
      },
      "source": [
        "6. What represents regularization ? Explain L1 and L2 regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Va6tS_SIBvW"
      },
      "source": [
        "Regularization represents a set of techniques used in machine learning to control model complexity and prevent overfitting. It works by adding a penalty term to the loss function, discouraging the model from learning excessively large weight values and helping it generalize better to unseen data.\n",
        "\n",
        "L1 Regularization (Lasso):\n",
        "L1 regularization adds the sum of absolute values of weights to the loss function. This penalty pushes many weights exactly to zero, effectively removing less important features. As a result, L1 regularization produces a sparse model and can perform feature selection.\n",
        "\n",
        "L2 Regularization (Ridge):\n",
        "L2 regularization adds the sum of squared weights to the loss function. Instead of making weights zero, it reduces their magnitude evenly. This leads to smoother models that are less sensitive to noise and helps prevent overfitting by keeping weights small."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjeRGpCE7QJ_"
      },
      "source": [
        "7. What represents Dropout Layer and To which part of a neural network represents dropout generally applied? how does it prevent overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTTQO9--In5A"
      },
      "source": [
        "Dropout layer is a regularization technique in neural networks where a fixed percentage of neurons are randomly deactivated during training. These neurons do not contribute to forward or backward propagation in that iteration, which forces the network to learn more general and robust features.\n",
        "\n",
        "Dropout is generally applied to the hidden layers, especially dense (fully connected) layers. It is usually not applied to the output layer because removing output neurons can negatively affect prediction accuracy.\n",
        "\n",
        "Dropout helps in preventing overfitting by reducing dependency among neurons. Since neurons cannot rely on specific other neurons being present, the network avoids memorizing training data and instead learns distributed representations, which improves performance on unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLZFLyv4-A67"
      },
      "source": [
        "8. A hidden layer of a neural network has the following activations:\n",
        "\n",
        "   \\[\n",
        "   a = [2.0, 5.0, 7.12, 4.5, 6.0]\n",
        "   \\]\n",
        "\n",
        "   Dropout represents applied **during training** with dropout probability \\(p = 0.5\\).\n",
        "\n",
        "   A randomly generated dropout mask represents:\n",
        "\n",
        "   \\[\n",
        "   m = [1, 0, 0, 1, 1]\n",
        "   \\]\n",
        "\n",
        "   Here,  \n",
        "   - `1` ‚Üí neuron represents **kept**  \n",
        "   - `0` ‚Üí neuron represents **dropped**\n",
        "\n",
        "   ---\n",
        "\n",
        "   **Question:**  \n",
        "   What will be the output activations of this layer **after applying dropout**?\n",
        "   *(Show the final activation vector)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX0ysAx0fZkH"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "[2.0,0,0,4.5,6.0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "La3K0HmUGj8t"
      },
      "source": [
        "9. Explain the difference between Gradient Descent, Stochastic Gradient Descent, Mini-Batch Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2woinlsdgDrE"
      },
      "source": [
        "Gradient Descent, Stochastic Gradient Descent, and Mini-Batch Gradient Descent are optimization techniques used to minimize the loss function in machine learning models, and they mainly differ in how much training data they use for each parameter update.\n",
        "\n",
        " Gradient Descent (Batch Gradient Descent) computes the gradient using the entire training dataset before updating the model parameters, which makes the learning process stable and smooth but very slow and memory-intensive for large datasets.\n",
        "\n",
        " Stochastic Gradient Descent (SGD), on the other hand, updates the parameters using only one training example at a time, making it much faster and suitable for large datasets, but the updates are noisy and the loss function fluctuates instead of decreasing smoothly.\n",
        "\n",
        " Mini-Batch Gradient Descent lies between these two approaches and updates parameters using a small subset (batch) of training data, providing a balance between speed and stability; it is computationally efficient, works well with GPUs, and is the most commonly used method in practical deep learning applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FngV9WtV7JxI"
      },
      "source": [
        "10. What indicate optimizers ? Explain Adam, RMS prop and Momentum in detail"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nCCTsVogVzh"
      },
      "source": [
        "Answer:\n",
        "Optimizers indicate algorithms or methods applied to change the attributes of a neural network, such as weights and learning rate, to reduce the losses.\n",
        "\n",
        "Momentum:-\n",
        "Momentum represents designed to accelerate SGD in the relevant direction and dampen oscillations. It does this by adding a fraction of the update vector of the past time step to the current update. like a ball rolling down a hill; it gains momentum as it descends. It helps the optimizer navigate through \"valleys\" (narrow regions of the loss surface) where the gradient might otherwise oscillate wildly.\n",
        "\n",
        "RMSprop (Root Mean Square Propagation):-\n",
        "RMSprop represents an adaptive learning rate method. Instead of using a fixed learning rate for all parameters, it maintains a moving average of the squared gradients for each weight. It then divides the gradient by the square root of this average. This explanation has the effect of normalizing the gradient: it slows down learning for dimensions with large gradients (to prevent exploding) and speeds up learning for dimensions with small gradients.\n",
        "\n",
        "Adam (Adaptive Moment Estimation):-\n",
        "It combines the benefits of Momentum and RMSprop.\n",
        "\n",
        "1. It calculates an exponential moving average of the gradient (like Momentum) to keep track of direction.\n",
        "\n",
        "2. It calculates an exponential moving average of the squared gradient (like RMSprop) to scale the learning rate. Adam represents computationally efficient, requires little memory, and represents well-suited for problems which indicate large in terms of data or parameters."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}