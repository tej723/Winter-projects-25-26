{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**SUBMISSION INSTRUCTIONS**\n",
        "\n",
        "First make a copy of this colab file and then solve the assignment and upload your final notebook on github.\n",
        "\n",
        "**To write an answer to any question, First Add a TEXT block below the question and type it in only one single block**\n",
        "\n",
        "Before uploading your downloaded notebook, RENAME the file as rollno_name.ipynb\n",
        "\n",
        "Submission Deadline : 19/12/2025 Friday EOD i.e before 11:59 PM\n",
        "\n",
        "The deadline is strict and will not be extended, Late submissions are not allowed\n",
        "\n",
        "Note that you have to upload your solution on the github page of the project Vision Transformer and **under Assignment 3**\n",
        "\n",
        "And remember to keep title of your pull request to be ViT_name_rollno_assgn3\n",
        "\n",
        "Github Submission repo -\n",
        "https://github.com/electricalengineersiitk/Winter-projects-25-26/tree/main/Vision%20transformer/Assignment%203"
      ],
      "metadata": {
        "id": "XmTMSeaTLwVp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**THEORETICAL ASSIGNMENT**\n",
        "\n"
      ],
      "metadata": {
        "id": "-TjKxTBEz8i7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This assignment consists of theoretical questions. Objective answers will not be accepted. You need to write detailed answers (200-300 words) for the questions that require more in-depth explanations (you should be able to identify these after reading the question and finding about them). Questions that need less explanation can be answered in 20-100 words. Please ensure the answers are well-written and thorough."
      ],
      "metadata": {
        "id": "AnVa1_8AKqtv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Assume that the inputs **X**\n",
        " to some scalar function **f**\n",
        " are **n x m**\n",
        " matrices. What is the dimensionality of the gradient of **f**\n",
        " with respect to **X**?\n",
        "**Give reasons to justify your answer.**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ERhYz9qV_YXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gradient of scaler function f will have the same dimesionality as of\n",
        "n x m\n",
        "\n",
        "Reason: The function f is a scalar but depends on each individual entry Xᵢⱼ of the matrix. The gradient is defined as the collection of all partial derivatives df/dx(i,j) for i = 1,....,n and j = 1,....m.\n",
        "\n",
        "Since there is one partial derivative for every element of X, arranging these derivatives gives a matrix of size n x m."
      ],
      "metadata": {
        "id": "-2S9rXKRdpFb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Define the evaluation metrics Accuracy, Precision, Recall, and F1 score.\n",
        "\n",
        "   Consider the following diagram:\n",
        "   Find the values of Accuracy, Precision, Recall, and F1 score.\n",
        "   |                | Actual: Cancer | Actual: No Cancer |\n",
        "   |----------------|---------------|------------------|\n",
        "   | Predicted: Cancer | 80 | 80 |\n",
        "   | Predicted: No Cancer | 20 | 820 |\n",
        "    \n",
        "   \n",
        "   Find the values of Accuracy, Precision, Recall, and F1 score using the above data.\n"
      ],
      "metadata": {
        "id": "9vnyYHlq2g4B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Accuracy is the ratio of correct prefictions by total no. of prediction.\n",
        "2. Precision is the ratio of true positive prediction by total no. of positive prediction.\n",
        "3. Recall(sensitivity) measures how many of the actual positive cases are correctly identified.\n",
        "4. F1 score is the harmonic mean of Precision and Recall.\n",
        "\n",
        "Accuracy = 90%\n",
        "Precision = 50%\n",
        "Recall = 80%\n",
        "F1 score = 61.5%"
      ],
      "metadata": {
        "id": "vBttf8C6ea75"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is a confusion matrix ?"
      ],
      "metadata": {
        "id": "h1dJMD0vJKZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is a matrix which contains both the values of Predicted outputs and actual results\n",
        "It shows how many predictions are correct and incorrect."
      ],
      "metadata": {
        "id": "GeyPkywbf6rF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is overfitting and underfitting ?"
      ],
      "metadata": {
        "id": "aOqF8Dzw0DdZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overfitting** occurs when a model learns the training data too well, including noise and random fluctuations. As a result, it performs very well on training data but poorly on new or unseen data.\n",
        "\n",
        "**Underfitting** occurs when a model is too simple to capture the underlying pattern in the data. It performs poorly on both training and unseen data."
      ],
      "metadata": {
        "id": "EZBZxS9yg59Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Explain vanishing and exploding gradients, give reasons why they occur and how they can be prevented.\n"
      ],
      "metadata": {
        "id": "n4KfKc0z0ulm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vanishing gradient occurs when gradients become very small during backpropagation. As a result, early layers of a neural network learn very slowly or stop learning.\n",
        "\n",
        "Reason-\n",
        "1. Use of activation functions like sigmoid or tanh\n",
        "2. Poor weight initialization.\n",
        "3. Repeated multiplication of small derivatives (especially in deep networks).\n",
        "\n",
        "Prevention-\n",
        "\n",
        "1. Use ReLU or its variants (Leaky ReLU, ELU)\n",
        "2. Proper weight initialization (Xavier, He initialization)\n",
        "3. Use batch normalization\n",
        "4. Use residual connections (skip connections)\n",
        "\n",
        "Exploding gradient occurs when gradients become very large during backpropagation. This leads to unstable training and very large weight updates.\n",
        "\n",
        "Reasons-\n",
        "\n",
        "1. Repeated multiplication of large derivatives\n",
        "2. Very deep networks\n",
        "3. Large initial weights\n",
        "4. High learning rate\n",
        "\n",
        "Prevention-\n",
        "\n",
        "1. Gradient clipping\n",
        "2. Proper weight initialization\n",
        "3. Use batch normalization\n",
        "4. Reduce learning rate\n"
      ],
      "metadata": {
        "id": "65hNzJxx36Iu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is regularization ? Explain L1 and L2 regularization"
      ],
      "metadata": {
        "id": "6yaOipvU1JoD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularization is a technique used to prevent overfitting by adding a penalty term to the loss function. It discourages the model from learning very large weights and helps the model generalize better to unseen data.\n",
        "\n",
        "L1 regularization adds the sum of absolute values of weights to the loss function.\n",
        "\n",
        "L2 regularization adds the sum of squared values of weights to the loss function."
      ],
      "metadata": {
        "id": "jAEm5aP937Ag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What is Dropout Layer and To which part of a neural network is dropout generally applied? how does it prevent overfitting"
      ],
      "metadata": {
        "id": "NjeRGpCE7QJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A dropout layer is a regularization technique in neural networks where a random fraction of neurons is temporarily turned off (set to zero) during training.\n",
        "\n",
        "Dropout is generally applied to the hidden layers of a neural network.\n",
        "It is usually not applied to the output layer.\n",
        "\n",
        "It prevents from overfitting in the following ways\n",
        "\n",
        "1. Prevents neurons from becoming too dependent on each other\n",
        "2. Forces the network to learn more robust and distributed features\n",
        "3. Acts like training many smaller networks and averaging them\n",
        "4. Reduces memorization of training data"
      ],
      "metadata": {
        "id": "YvyLnQ7o370U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. A hidden layer of a neural network has the following activations:\n",
        "\n",
        "   \\[\n",
        "   a = [2.0, 5.0, 7.12, 4.5, 6.0]\n",
        "   \\]\n",
        "\n",
        "   Dropout is applied **during training** with dropout probability \\(p = 0.5\\).\n",
        "\n",
        "   A randomly generated dropout mask is:\n",
        "\n",
        "   \\[\n",
        "   m = [1, 0, 0, 1, 1]\n",
        "   \\]\n",
        "\n",
        "   Here,  \n",
        "   - `1` → neuron is **kept**  \n",
        "   - `0` → neuron is **dropped**\n",
        "\n",
        "   ---\n",
        "\n",
        "   **Question:**  \n",
        "   What will be the output activations of this layer **after applying dropout**?\n",
        "   *(Show the final activation vector)*\n"
      ],
      "metadata": {
        "id": "jLZFLyv4-A67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final output = [ 2.0×2, 0, 0, 4.5×2, 6.0×2 ]\n",
        "\n",
        "Final activation vector: [ 4.0, 0, 0, 9.0, 12.0 ]"
      ],
      "metadata": {
        "id": "I4QetRJ1386u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Explain the difference between Gradient Descent, Stochastic Gradient Descent, Mini-Batch Gradient Descent"
      ],
      "metadata": {
        "id": "La3K0HmUGj8t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradient Descent (Batch Gradient Descent)**\n",
        "\n",
        "Gradient Descent computes the gradient using the entire training dataset and then updates the weights.\n",
        "\n",
        "Key points:\n",
        "1. One update per epoch\n",
        "2. Stable and smooth convergence\n",
        "3. Computationally expensive for large datasets\n",
        "\n",
        "**Stochastic Gradient Descent (SGD)**\n",
        "\n",
        "Stochastic Gradient Descent updates the weights using one training example at a time.\n",
        "\n",
        "Key points:\n",
        "1. Many updates per epoch\n",
        "2. Faster but noisy updates\n",
        "3. Can escape local minima due to noise\n",
        "\n",
        "**Mini-Batch Gradient Descent**\n",
        "\n",
        "Mini-batch Gradient Descent updates weights using a small batch of samples (e.g., 32, 64).\n",
        "\n",
        "Key points:\n",
        "1. Balance between batch GD and SGD\n",
        "2. Faster than batch GD\n",
        "3. More stable than SGD"
      ],
      "metadata": {
        "id": "hZfs-SC33-PK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are optimizers ? Explain Adam, RMS prop and Momentum in detail"
      ],
      "metadata": {
        "id": "FngV9WtV7JxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimizers are algorithms used to update the weights of a neural network in order to minimize the loss function. They decide how big the weight update should be and in which direction, based on gradients.\n",
        "\n",
        "**RMSProp (Root Mean Square Propagation)**\n",
        "\n",
        "RMSProp adapts the learning rate for each weight by dividing the gradient by a moving average of squared gradients.\n",
        "\n",
        "How it works:\n",
        "1. Keeps track of squared gradients\n",
        "2. Reduces learning rate for frequently updated weights\n",
        "3. Prevents very large updates\n",
        "\n",
        "Why useful:\n",
        "1. Works well for non-stationary problems\n",
        "2. Fixes learning rate decay issue of AdaGrad\n",
        "\n",
        "**Momentum Optimizer**\n",
        "\n",
        "Momentum helps gradient descent move faster by adding a fraction of the previous update to the current update.\n",
        "\n",
        "How it works:\n",
        "1. Accumulates past gradients (velocity)\n",
        "2. Reduces oscillations\n",
        "3. Speeds up convergence in the correct direction\n",
        "\n",
        "Why useful:\n",
        "1. Faster than plain gradient descent\n",
        "2. Helps escape shallow local minima\n",
        "\n",
        "**Adam (Adaptive Moment Estimation)**\n",
        "Adam combines the ideas of Momentum and RMSProp.\n",
        "\n",
        "How it works:\n",
        "1. Uses moving average of gradients (Momentum)\n",
        "2. Uses moving average of squared gradients (RMSProp)\n",
        "3. Applies bias correction\n",
        "\n",
        "Why useful:\n",
        "1. Fast convergence\n",
        "2. Handles noisy and sparse gradients well\n",
        "3. Most widely used optimizer"
      ],
      "metadata": {
        "id": "-sMGUi3h3-4f"
      }
    }
  ]
}