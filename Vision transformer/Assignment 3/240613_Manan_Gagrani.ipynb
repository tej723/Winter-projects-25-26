{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**SUBMISSION INSTRUCTIONS**\n",
        "\n",
        "First make a copy of this colab file and then solve the assignment and upload your final notebook on github.\n",
        "\n",
        "**To write an answer to any question, First Add a TEXT block below the question and type it in only one single block**\n",
        "\n",
        "Before uploading your downloaded notebook, RENAME the file as rollno_name.ipynb\n",
        "\n",
        "Submission Deadline : 19/12/2025 Friday EOD i.e before 11:59 PM\n",
        "\n",
        "The deadline is strict and will not be extended, Late submissions are not allowed\n",
        "\n",
        "Note that you have to upload your solution on the github page of the project Vision Transformer and **under Assignment 3**\n",
        "\n",
        "And remember to keep title of your pull request to be ViT_name_rollno_assgn3\n",
        "\n",
        "Github Submission repo -\n",
        "https://github.com/electricalengineersiitk/Winter-projects-25-26/tree/main/Vision%20transformer/Assignment%203"
      ],
      "metadata": {
        "id": "XmTMSeaTLwVp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**THEORETICAL ASSIGNMENT**\n",
        "\n"
      ],
      "metadata": {
        "id": "-TjKxTBEz8i7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This assignment consists of theoretical questions. Objective answers will not be accepted. You need to write detailed answers (200-300 words) for the questions that require more in-depth explanations (you should be able to identify these after reading the question and finding about them). Questions that need less explanation can be answered in 20-100 words. Please ensure the answers are well-written and thorough."
      ],
      "metadata": {
        "id": "AnVa1_8AKqtv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Assume that the inputs **X**\n",
        " to some scalar function **f**\n",
        " are **n x m**\n",
        " matrices. What is the dimensionality of the gradient of **f**\n",
        " with respect to **X**?\n",
        "**Give reasons to justify your answer.**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ERhYz9qV_YXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Dimensionality:** $n \\times m$ (Matches input data $\\mathbf{X}$)\n",
        "* **Why?**\n",
        "    * The function $f$ gives us a single number (a scalar), but the input $\\mathbf{X}$ is a grid of numbers with $n$ rows and $m$ columns.\n",
        "    * In the case of finding the gradient, we have to identify how much the function $f$ will change when we wiggle *each individual number* in the grid.\n",
        "    * Since there are $n \\times m$ numbers in the input, we must calculate $n \\times m$ partial derivatives.\n",
        "    * We put these derivatives into a matrix that has the same dimensions so that we can easily subtract these derivatives from the original weights."
      ],
      "metadata": {
        "id": "GWFxbhJytceM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Define the evaluation metrics Accuracy, Precision, Recall, and F1 score.\n",
        "\n",
        "   Consider the following diagram:\n",
        "   Find the values of Accuracy, Precision, Recall, and F1 score.\n",
        "   |                | Actual: Cancer | Actual: No Cancer |\n",
        "   |----------------|---------------|------------------|\n",
        "   | Predicted: Cancer | 80 | 80 |\n",
        "   | Predicted: No Cancer | 20 | 820 |\n",
        "    \n",
        "   \n",
        "   Find the values of Accuracy, Precision, Recall, and F1 score using the above data.\n"
      ],
      "metadata": {
        "id": "9vnyYHlq2g4B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definitions:**\n",
        "* **Accuracy:** This is the simple \"how many did we get right?\" measure for score. Both correct \"yes\" and correct \"no\" counts for everything.\n",
        "* **Precision:** This tells us to what extent our \"positive\" predictions are to be relied upon. How likely are we to predict \"Cancer\" when the model gives \"Cancer\" as its prediction?\n",
        "* **Recall:** This is about coverage. Out of all the people that actually have cancer, what percentage of those could we find?\n",
        "* **F1 Score:** This is the combination of the Precision and Recall into one value. This is very helpful when you have an unbalanced dataset (e.g. many unhealthy vs. healthy samples) since the F1 Score will penalize the model for being poor at either precision or recall.\n",
        "\n",
        "**Calculations:**\n",
        "Looking at the table, we have 80 True Positives, 80 False Positives, 20 False Negatives, and 820 True Negatives.\n",
        "\n",
        "* **Accuracy:** $\\frac{900}{1000} = 0.9$\n",
        "* **Precision:** $\\frac{80}{160} = 0.5$\n",
        "* **Recall:** $\\frac{80}{100} = 0.8$\n",
        "* **F1 Score:** The formula gives $2 \\times \\frac{0.5 \\times 0.8}{0.5 + 0.8} \\approx 0.615$"
      ],
      "metadata": {
        "id": "aKiDU7Whtc_7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is a confusion matrix ?"
      ],
      "metadata": {
        "id": "h1dJMD0vJKZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The Confusion Matrix is essentially a report card that reveals just how a model is going wrong, and this happens because each classification task has a corresponding confusion matrix.\n",
        "* It not only provides you with one grade, like \"90% accuracy,\" but also divides the predictions into:\n",
        "    * **TP & TN:** When the model performed correctly on the data.\n",
        "    * **FP (Type 1):** Commission errors (saying Yes when we mean No).\n",
        "    * **FN (Type 2):** Missed targets (saying No when it's Yes).\n",
        "* It is really important when dealing with medical testing, for instance, because a False Negative would be far worse than a False Positive and a simple precision metric does not capture this difference."
      ],
      "metadata": {
        "id": "TDLwzgcutdbK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is overfitting and underfitting ?"
      ],
      "metadata": {
        "id": "aOqF8Dzw0DdZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Overfitting:** It is a case where \"the student has memorized the practice questions but doesn't know what's going on in the test questions.\" The model is learning the training data perfectly, including all the random noise and junk. It performs phenomenally on training data but atrociously on new data.\n",
        "\n",
        "* **Underfitting:** It just means that the model is too simplistic or \"dumb\" to understand the relationship. It's like trying to draw a line between two points that are obviously part of a \"U\" shape. It does terribly on the training data as well as the test data because it has not really learned any relationship."
      ],
      "metadata": {
        "id": "uE768tg2t3yH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Explain vanishing and exploding gradients, give reasons why they occur and how they can be prevented.\n"
      ],
      "metadata": {
        "id": "n4KfKc0z0ulm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are issues that may arise during the process of deep learning using Backpropagation.\n",
        "\n",
        "* **Vanishing Gradients:**\n",
        "    * **What happens:** While the error correction signal (or gradient) propagates from the end of the network back to the start, the error gradient shrinks and diminishes until the error gradient essentially vanishes.\n",
        "    * **Why:** It’s typically because of activation functions like Sigmoid, which squashes numbers into a small range (0 to 1). When you have a bunch of small decimals multiplied, it approaches zero quickly.\n",
        "    * **Fix:** We mostly use ReLU now (since it does not squish positive numbers) or add skip connections.\n",
        "\n",
        "* **Exploding Gradients:**\n",
        "    * **What happens:** The opposite problem—the numbers become extremely large and start multiplying rapidly, causing the weights to fluctuate randomly or result in NaN.\n",
        "    * **Fix:** One thing we can do is utilize Gradient Clipping, which essentially instils a speed limit on the gradients so that they don't go haywire."
      ],
      "metadata": {
        "id": "NHh5cm9gt8Jd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is regularization ? Explain L1 and L2 regularization"
      ],
      "metadata": {
        "id": "6yaOipvU1JoD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularization is what we do to prevent the model from cheating or overfitting. We're effectively penalizing it because it has high complexity.\n",
        "\n",
        "* **L1 Regularization (Lasso):**\n",
        "    * This introduces a penalty that depends on the absolute value of the weights.\n",
        "    * It attempts to drive its weights close to zero exactly. It is like a feature remover that removes inputs that do not provide much useful information by deleting them.\n",
        "\n",
        "* **L2 Regularization (Ridge):**\n",
        "    * It involves adding a penalty weighted by the square of the weight itself.\n",
        "    * That is the \"Weight Decay\". It does not reach zero but sets the weights low enough that no individual feature contributes heavily to the decision."
      ],
      "metadata": {
        "id": "p59PjNRkuBWc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What is Dropout Layer and To which part of a neural network is dropout generally applied? how does it prevent overfitting"
      ],
      "metadata": {
        "id": "NjeRGpCE7QJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **What does that mean?** It’s a training technique in which we stochastically \"switch off\" (drop) some random neurons in that activation layer.\n",
        "* **Where?** Typically in the Dense (Fully Connected) layers towards the end of the network.\n",
        "* **Why is this effective?** It makes the network stop being so lazy. A node cannot depend on a single neighboring node because this neighboring node might be inactive during the next cycle. It is almost like benching star players in a sports team at random times. It makes all other players improve."
      ],
      "metadata": {
        "id": "B9-fb9ILuGd_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. A hidden layer of a neural network has the following activations:\n",
        "\n",
        "   \\[\n",
        "   a = [2.0, 5.0, 7.12, 4.5, 6.0]\n",
        "   \\]\n",
        "\n",
        "   Dropout is applied **during training** with dropout probability \\(p = 0.5\\).\n",
        "\n",
        "   A randomly generated dropout mask is:\n",
        "\n",
        "   \\[\n",
        "   m = [1, 0, 0, 1, 1]\n",
        "   \\]\n",
        "\n",
        "   Here,  \n",
        "   - `1` → neuron is **kept**  \n",
        "   - `0` → neuron is **dropped**\n",
        "\n",
        "   ---\n",
        "\n",
        "   **Question:**  \n",
        "   What will be the output activations of this layer **after applying dropout**?\n",
        "   *(Show the final activation vector)*\n"
      ],
      "metadata": {
        "id": "jLZFLyv4-A67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we're training, we always use \"Inverted Dropout,\" which scales up the survivors so that math can be consistent.\n",
        "\n",
        "* **Input:** `[2.0, 5.0, 7.12, 4.5, 6.0]`\n",
        "* **Mask:** `[1, 0, 0, 1, 1]` (Indices 1 & 2 will have zero)\n",
        "* **Scaling:** Because we are given that probability $p=0.5$, we multiply by $1/(1-0.5) = 2$.\n",
        "\n",
        "**Steps:**\n",
        "1.  **Mask It:** `[2.0, 0, 0, 4.5, 6.0]`\n",
        "2.  **Scale it:** Multiply the non-zeros by 2.\n",
        "\n",
        "**Final Result:**\n",
        "`[4.0, 0.0, 0.0, 9.0, 12.0]`"
      ],
      "metadata": {
        "id": "j70AWVauuMKW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Explain the difference between Gradient Descent, Stochastic Gradient Descent, Mini-Batch Gradient Descent"
      ],
      "metadata": {
        "id": "La3K0HmUGj8t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Batch Gradient Descent:** The major drawback with Gradient Descent is that it requires assessment of all observations to calculate one update to parameter estimates, i.e., to arrive at one step, you consider all data.\n",
        "* **Stochastic Gradient Descent (SGD):** The extreme opposite. The weights are updated after every image. Very fast, very volatile: the loss values fluctuate wildly (stochastic/noisy).\n",
        "* **Mini-Batch GD:** \"The middle ground.\" You process a small group (say 32 or 64 images), average their loss, and take a gradient descent step. It's stable enough to converge but efficient enough to be useful. This is how everyone actually does Deep Learning."
      ],
      "metadata": {
        "id": "Fnkl8Qg2uP_y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are optimizers ? Explain Adam, RMS prop and Momentum in detail"
      ],
      "metadata": {
        "id": "FngV9WtV7JxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimizers are only the particular mathematical formulas for carrying out the weight updates.\n",
        "\n",
        "* **RMSProp:** This offers a clever trick for adjusting the learning rate. First, it explores past slopes. If they are enormous, this indicates that they are too steep, so it reduces the rate of learning. If they are minuscule, it accelerates because we are only getting a hint of what it looks like at this point.\n",
        "\n",
        "* **Momentum:** Standard SGD can get stuck in local optima. This problem was resolved with the introduction of a “velocity.” It “remembers\" where it was going before. If it saw the gradient going in one direction, and it kept going in that same direction, it would move faster, like a ball going down a hill.\n",
        "\n",
        "* **Adam:** This is effectively the choice by default. Essentially, it takes the best that RMSProp has to offer, which is learning rates that adapt, and the best that Momentum has to offer, which is velocity. This is the fastest of all options and does not require a lot of tweaking."
      ],
      "metadata": {
        "id": "DpZ_MnKbuVyX"
      }
    }
  ]
}