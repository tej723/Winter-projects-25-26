{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###1. Assume that the inputs X to some scalar function f are n x m matrices. What is the dimensionality of the gradient of f with respect to X? Give reasons to justify your answer.\n"
      ],
      "metadata": {
        "id": "6znQggBxHOPf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let\n",
        "\n",
        "f(X) be a scalar-valued function, where the input\n",
        "X is an\n",
        "n√óm matrix. The gradient of\n",
        "f with respect to\n",
        "X, denoted as\n",
        "‚àÇX/\n",
        "‚àÇf, has the same dimensionality as\n",
        "X, i.e., an\n",
        "n√óm matrix.\n",
        "###Reasoning:\n",
        "\n",
        "A gradient represents how the output of a function changes with respect to each input variable. Since\n",
        "X contains\n",
        "n√óm individual elements\n",
        "xij\n",
        ", the function\n",
        "f depends on each of these elements. Therefore, we compute partial derivatives with respect to every element in the matrix:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial f}{\\partial X} =\n",
        "\\begin{bmatrix}\n",
        "\\frac{\\partial f}{\\partial x_{11}} & \\cdots & \\frac{\\partial f}{\\partial x_{1m}} \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "\\frac{\\partial f}{\\partial x_{n1}} & \\cdots & \\frac{\\partial f}{\\partial x_{nm}}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "Each partial derivative measures how a small change in one matrix element affects the scalar output. Since there are\n",
        "n√óm such elements, the gradient must also contain\n",
        "n√óm components.\n"
      ],
      "metadata": {
        "id": "EeFnrmL6HcnR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.Define the evaluation metrics Accuracy, Precision, Recall, and F1 score."
      ],
      "metadata": {
        "id": "p-m1j9CaIeH_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definitions:\n",
        "\n",
        "- Accuracy: Proportion of total correct predictions.\n",
        "\n",
        "- Precision: Proportion of correct positive predictions among all predicted positives.\n",
        "\n",
        "- Recall (Sensitivity): Proportion of actual positives correctly identified.\n",
        "\n",
        "- F1 Score: Harmonic mean of Precision and Recall, balancing both\n",
        "\n",
        "Given Confusion Matrix:\n",
        "   |                | Actual: Cancer | Actual: No Cancer |\n",
        "   |----------------|---------------|------------------|\n",
        "   | Predicted: Cancer | 80 | 80 |\n",
        "   | Predicted: No Cancer | 20 | 820 |\n",
        "\n",
        "TP = 80\n",
        "\n",
        "FP = 80\n",
        "\n",
        "FN = 20\n",
        "\n",
        "TN = 820\n",
        "\n",
        "Total = 1000\n",
        "\n",
        "Calculations:\n",
        "\n",
        "**Accuracy**\n",
        "\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{TP + TN}{\\text{Total}}\n",
        "= \\frac{80 + 820}{1000}\n",
        "= 0.90 \\; (90\\%)\n",
        "$$\n",
        "\n",
        "**Precision**\n",
        "\n",
        "$$\n",
        "\\text{Precision} = \\frac{TP}{TP + FP}\n",
        "= \\frac{80}{80 + 80}\n",
        "= 0.50 \\; (50\\%)\n",
        "$$\n",
        "\n",
        "**Recall**\n",
        "\n",
        "$$\n",
        "\\text{Recall} = \\frac{TP}{TP + FN}\n",
        "= \\frac{80}{80 + 20}\n",
        "= 0.80 \\; (80\\%)\n",
        "$$\n",
        "\n",
        "**F1 Score**\n",
        "\n",
        "$$\n",
        "\\text{F1 Score}\n",
        "= 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}\n",
        "{\\text{Precision} + \\text{Recall}}\n",
        "= 2 \\times \\frac{0.5 \\times 0.8}{0.5 + 0.8}\n",
        "= 0.615 \\; (61.5\\%)\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "UBeV4ZZTIurP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.What is a confusion matrix ?"
      ],
      "metadata": {
        "id": "0Oh1Pa3eJXb3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A confusion matrix is a table used to evaluate the performance of a classification model by comparing predicted labels with actual labels. It provides a detailed breakdown of correct and incorrect predictions, allowing deeper insight than accuracy alone.\n",
        "\n",
        "The matrix consists of four key components:\n",
        "\n",
        "- **True Positive (TP):** Correctly predicted positive cases  \n",
        "- **False Positive (FP):** Incorrectly predicted positive cases  \n",
        "- **False Negative (FN):** Incorrectly predicted negative cases  \n",
        "- **True Negative (TN):** Correctly predicted negative cases  \n",
        "\n",
        "\n",
        "Confusion matrices are especially useful in imbalanced datasets, such as medical diagnosis, where false negatives or false positives carry different consequences. They form the basis for computing metrics like Precision, Recall, F1 score, and Specificity."
      ],
      "metadata": {
        "id": "3cHTqrHzJjeF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4.What is overfitting and underfitting ?"
      ],
      "metadata": {
        "id": "S2dN6gpEJmq8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overfitting occurs when a model learns the training data too well, including noise and irrelevant patterns. Such a model performs very well on training data but poorly on unseen test data. Overfitting is common in highly complex models trained on small datasets.\n",
        "\n",
        "Underfitting happens when a model is too simple to capture the underlying structure of the data. It performs poorly on both training and testing data, indicating that it has not learned the patterns effectively.\n",
        "\n",
        "Key Difference:\n",
        "\n",
        "- Overfitting ‚Üí High variance, low bias\n",
        "\n",
        "- Underfitting ‚Üí High bias, low variance\n",
        "\n",
        "Balancing model complexity is crucial to achieving good generalization."
      ],
      "metadata": {
        "id": "qRZ4Cb6NJrWC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.Explain vanishing and exploding gradients, give reasons why they occur and how they can be prevented."
      ],
      "metadata": {
        "id": "2yD-Gg2pJulY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vanishing and exploding gradients are common problems encountered while training deep neural networks, especially during backpropagation.\n",
        "\n",
        "###Vanishing Gradient:\n",
        "\n",
        "Occurs when gradients become extremely small as they propagate backward through many layers. This leads to negligible weight updates, causing early layers to learn very slowly. It commonly arises due to repeated multiplication of small values (e.g., sigmoid or tanh activations).\n",
        "\n",
        "###Exploding Gradient:\n",
        "\n",
        "Occurs when gradients grow exponentially large, leading to unstable training and numerical overflow. This happens when large weights or gradients are repeatedly multiplied.\n",
        "\n",
        "###Why They Occur:\n",
        "\n",
        "- Deep networks\n",
        "\n",
        "- Poor weight initialization\n",
        "\n",
        "- Inappropriate activation functions\n",
        "\n",
        "###Prevention Methods:\n",
        "\n",
        "- Proper weight initialization (Xavier, He initialization)\n",
        "\n",
        "- Using ReLU or its variants\n",
        "\n",
        "- Gradient clipping (for exploding gradients)\n",
        "\n",
        "- Batch normalization\n",
        "\n",
        "- Residual connections (skip connections)\n",
        "\n",
        "These techniques help stabilize training and enable effective learning in deep neural networks."
      ],
      "metadata": {
        "id": "-gghMb97Jy3h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###6.What is regularization ? Explain L1 and L2 regularization"
      ],
      "metadata": {
        "id": "GAKb5dnpJ2sk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularization is a technique used in machine learning to prevent overfitting by discouraging overly complex models. Overfitting occurs when a model learns noise and details from training data instead of the underlying pattern. Regularization addresses this problem by adding a penalty term to the loss function, which restricts the magnitude of model parameters (weights).\n",
        "\n",
        "###L1 Regularization (Lasso)\n",
        "\n",
        "L1 regularization adds the sum of absolute values of weights to the loss function:\n",
        "\n",
        "Loss = Original Loss + Œª‚àë‚à£Wi|\n",
        "\n",
        "\n",
        "It encourages sparsity, meaning some weights become exactly zero. This effectively performs feature selection, making the model simpler and easier to interpret. L1 is especially useful when many input features are irrelevant.\n",
        "\n",
        "###L2 Regularization (Ridge)\n",
        "\n",
        "L2 regularization adds the sum of squared weights to the loss function:\n",
        "\n",
        "Loss = Original Loss + Œª‚àëWi^2‚Äã\n",
        "\t‚Äã\n",
        "\n",
        "\n",
        "Instead of forcing weights to zero, L2 regularization shrinks weights smoothly, distributing importance across features. It improves numerical stability and is widely used in deep learning.\n",
        "\n",
        "Conclusion\n",
        "\n",
        "- L1 ‚Üí Feature selection, sparse model\n",
        "\n",
        "- L2 ‚Üí Weight shrinkage, smoother learning\n",
        "\n",
        "Both methods help improve model generalization."
      ],
      "metadata": {
        "id": "rdQCXjWeKTcv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###7.What is Dropout Layer and To which part of a neural network is dropout generally applied? how does it prevent overfitting"
      ],
      "metadata": {
        "id": "mi3a-iHfLTlY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Dropout layer is a regularization technique used in neural networks to reduce overfitting. During training, dropout randomly deactivates (drops) a fraction of neurons in a layer with a predefined probability\n",
        "ùëù\n",
        "p.\n",
        "\n",
        "Where Dropout is Applied\n",
        "\n",
        "Dropout is generally applied to:\n",
        "\n",
        "- Hidden layers of a neural network\n",
        "It is usually not applied to the output layer and is disabled during testing/inference.\n",
        "\n",
        "How Dropout Prevents Overfitting\n",
        "\n",
        "- Prevents neurons from becoming overly dependent on specific other neurons\n",
        "\n",
        "- Forces the network to learn robust and redundant representations\n",
        "\n",
        "- Acts like training an ensemble of multiple smaller networks\n"
      ],
      "metadata": {
        "id": "eD20d3NULaDb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###8.What will be the output activations of this layer after applying dropout? (Show the final activation vector)"
      ],
      "metadata": {
        "id": "ngBd82QXLfuN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given:\n",
        "\n",
        "Activations:\n",
        "\n",
        "   a = [2.0, 5.0, 7.12, 4.5, 6.0]\n",
        "\n",
        "Dropout mask:\n",
        "\n",
        "m = [1, 0, 0, 1, 1]\n",
        "\n",
        "Applying Dropout\n",
        "\n",
        "Dropout is applied by element-wise multiplication of activations and the mask:\n",
        "\n",
        "a\n",
        "‚Ä≤\n",
        "=a‚äôm\n",
        "\n",
        "a‚Ä≤=[2.0√ó1, 5.0√ó0, 7.12√ó0, 4.5√ó1, 6.0√ó1]\n",
        "\n",
        "Final Output Activations:\n",
        "\n",
        "[2.0,0,0,4.5,6.0]"
      ],
      "metadata": {
        "id": "lpkCldRgLp7g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###9.Explain the difference between Gradient Descent, Stochastic Gradient Descent, Mini-Batch Gradient Descent"
      ],
      "metadata": {
        "id": "QfbZx00eMWDx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Gradient Descent (Batch Gradient Descent)\n",
        "\n",
        "Uses the entire training dataset to compute gradients for each update.\n",
        "\n",
        "- Stable convergence\n",
        "\n",
        "- Computationally expensive\n",
        "\n",
        "- Slow for large datasets\n",
        "\n",
        "###Stochastic Gradient Descent (SGD)\n",
        "\n",
        "Uses one training example at a time for each update.\n",
        "\n",
        "- Faster updates\n",
        "\n",
        "- High variance in loss\n",
        "\n",
        "- Noisy but can escape local minima\n",
        "\n",
        "###Mini-Batch Gradient Descent\n",
        "\n",
        "Uses a small batch of samples (e.g., 32, 64) per update.\n",
        "\n",
        "- Balance between speed and stability\n",
        "\n",
        "- Efficient on GPUs\n",
        "\n",
        "- Most commonly used in practice"
      ],
      "metadata": {
        "id": "Md_Od2AKMb5Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###10.What are optimizers ? Explain Adam, RMS prop and Momentum in detail"
      ],
      "metadata": {
        "id": "i7TUzbjnMyo3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimizers are algorithms used to update model parameters in order to minimize the loss function efficiently during training.\n",
        "\n",
        "###Momentum\n",
        "\n",
        "Momentum accelerates gradient descent by adding a fraction of the previous update to the current update.\n",
        "\n",
        "- Reduces oscillations\n",
        "\n",
        "- Speeds up convergence in consistent directions\n",
        "\n",
        "###RMSprop\n",
        "\n",
        "RMSprop adapts the learning rate for each parameter by dividing the gradient by a moving average of squared gradients.\n",
        "\n",
        "- Prevents large updates\n",
        "\n",
        "- Works well for non-stationary objectives\n",
        "\n",
        "- Common in recurrent neural networks\n",
        "\n",
        "###Adam (Adaptive Moment Estimation)\n",
        "\n",
        "Adam combines the benefits of Momentum and RMSprop.\n",
        "\n",
        "- Maintains moving averages of both gradients and squared gradients\n",
        "\n",
        "- Adaptive learning rate\n",
        "\n",
        "- Fast convergence and stable training"
      ],
      "metadata": {
        "id": "hvtt-RFQM0mP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWvQMJ9EG4Ue"
      },
      "outputs": [],
      "source": [
        ".\n"
      ]
    }
  ]
}