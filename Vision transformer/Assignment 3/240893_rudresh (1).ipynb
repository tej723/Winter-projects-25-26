{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**SUBMISSION INSTRUCTIONS**\n",
        "\n",
        "First make a copy of this colab file and then solve the assignment and upload your final notebook on github.\n",
        "\n",
        "**To write an answer to any question, First Add a TEXT block below the question and type it in only one single block**\n",
        "\n",
        "Before uploading your downloaded notebook, RENAME the file as rollno_name.ipynb\n",
        "\n",
        "Submission Deadline : 19/12/2025 Friday EOD i.e before 11:59 PM\n",
        "\n",
        "The deadline is strict and will not be extended, Late submissions are not allowed\n",
        "\n",
        "Note that you have to upload your solution on the github page of the project Vision Transformer and **under Assignment 3**\n",
        "\n",
        "And remember to keep title of your pull request to be ViT_name_rollno_assgn3\n",
        "\n",
        "Github Submission repo -\n",
        "https://github.com/electricalengineersiitk/Winter-projects-25-26/tree/main/Vision%20transformer/Assignment%203"
      ],
      "metadata": {
        "id": "XmTMSeaTLwVp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**THEORETICAL ASSIGNMENT**\n",
        "\n"
      ],
      "metadata": {
        "id": "-TjKxTBEz8i7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This assignment consists of theoretical questions. Objective answers will not be accepted. You need to write detailed answers (200-300 words) for the questions that require more in-depth explanations (you should be able to identify these after reading the question and finding about them). Questions that need less explanation can be answered in 20-100 words. Please ensure the answers are well-written and thorough."
      ],
      "metadata": {
        "id": "AnVa1_8AKqtv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Assume that the inputs **X**\n",
        " to some scalar function **f**\n",
        " are **n x m**\n",
        " matrices. What is the dimensionality of the gradient of **f**\n",
        " with respect to **X**?\n",
        "**Give reasons to justify your answer.**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ERhYz9qV_YXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**( n \\times m )**\n",
        "Because the gradient contains one partial derivative for **each element of the ( n \\times m ) matrix ( X )**, so it has the **same dimensions** as ( X ).\n"
      ],
      "metadata": {
        "id": "OmNS45TXu0u5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Define the evaluation metrics Accuracy, Precision, Recall, and F1 score.\n",
        "\n",
        "   Consider the following diagram:\n",
        "   Find the values of Accuracy, Precision, Recall, and F1 score.\n",
        "   |                | Actual: Cancer | Actual: No Cancer |\n",
        "   |----------------|---------------|------------------|\n",
        "   | Predicted: Cancer | 80 | 80 |\n",
        "   | Predicted: No Cancer | 20 | 820 |\n",
        "    \n",
        "   \n",
        "   Find the values of Accuracy, Precision, Recall, and F1 score using the above data.\n"
      ],
      "metadata": {
        "id": "9vnyYHlq2g4B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        " **Accuracy:** Fraction of total correct predictions.\n",
        " **Precision:** Fraction of predicted positives that are actually positive.\n",
        " **Recall:** Fraction of actual positives correctly predicted.\n",
        " **F1 Score:** Harmonic mean of Precision and Recall.\n",
        "\n",
        "**From given data:**\n",
        "TP = 80, FP = 80, FN = 20, TN = 820, Total = 1000\n",
        "\n",
        " **Accuracy:** ( (TP+TN)/Total = (80+820)/1000 = 0.90 )\n",
        " **Precision:** ( TP/(TP+FP) = 80/(80+80) = 0.50 )\n",
        " **Recall:** ( TP/(TP+FN) = 80/(80+20) = 0.80 )\n",
        " **F1 Score:** ( 2PR/(P+R) = 2(0.5)(0.8)/(1.3) \\approx 0.62 )\n"
      ],
      "metadata": {
        "id": "w7fXqQNGvUkf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is a confusion matrix ?"
      ],
      "metadata": {
        "id": "h1dJMD0vJKZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **confusion matrix** is a table that shows the counts of correct and incorrect predictions by comparing **actual labels** with **predicted labels** (TP, FP, FN, TN).\n"
      ],
      "metadata": {
        "id": "yIOJP2uYv8nY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is overfitting and underfitting ?"
      ],
      "metadata": {
        "id": "aOqF8Dzw0DdZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overfitting:** Model learns training data too well and performs poorly on new data.\n",
        "**Underfitting:** Model is too simple to learn patterns and performs poorly on both training and test data.\n"
      ],
      "metadata": {
        "id": "CA1RhJHuwF73"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Explain vanishing and exploding gradients, give reasons why they occur and how they can be prevented.\n"
      ],
      "metadata": {
        "id": "n4KfKc0z0ulm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vanishing Gradient:** Gradients become very small during backpropagation, so earlier layers learn very slowly.\n",
        "**Exploding Gradient:** Gradients become very large, causing unstable training.\n",
        "\n",
        " Repeated multiplication of gradients, deep networks, and improper weight initialization.\n",
        "\n",
        "Use ReLU/Leaky ReLU, proper initialization (Xavier/He), batch normalization, gradient clipping, and residual connections.\n"
      ],
      "metadata": {
        "id": "e9FNcI2PwTcl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bE_pvIHzwJ3X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is regularization ? Explain L1 and L2 regularization"
      ],
      "metadata": {
        "id": "6yaOipvU1JoD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regularization:** A technique used to prevent overfitting by adding a penalty term to the loss function.\n",
        "\n",
        "**L1 Regularization:** Adds the sum of absolute values of weights; encourages sparsity and feature selection.\n",
        "\n",
        "**L2 Regularization:** Adds the sum of squared weights; discourages large weights and improves generalization.\n"
      ],
      "metadata": {
        "id": "3UukXiLKwoSS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What is Dropout Layer and To which part of a neural network is dropout generally applied? how does it prevent overfitting"
      ],
      "metadata": {
        "id": "NjeRGpCE7QJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dropout Layer:** A layer that randomly disables some neurons during training.\n",
        "\n",
        "**Applied to:** Mainly hidden layers of a neural network.\n",
        "\n",
        "**Prevents overfitting:** Reduces co-adaptation of neurons and forces the network to learn more robust features.\n"
      ],
      "metadata": {
        "id": "s48H1UNpwy6n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. A hidden layer of a neural network has the following activations:\n",
        "\n",
        "   \\[\n",
        "   a = [2.0, 5.0, 7.12, 4.5, 6.0]\n",
        "   \\]\n",
        "\n",
        "   Dropout is applied **during training** with dropout probability \\(p = 0.5\\).\n",
        "\n",
        "   A randomly generated dropout mask is:\n",
        "\n",
        "   \\[\n",
        "   m = [1, 0, 0, 1, 1]\n",
        "   \\]\n",
        "\n",
        "   Here,  \n",
        "   - `1` → neuron is **kept**  \n",
        "   - `0` → neuron is **dropped**\n",
        "\n",
        "   ---\n",
        "\n",
        "   **Question:**  \n",
        "   What will be the output activations of this layer **after applying dropout**?\n",
        "   *(Show the final activation vector)*\n"
      ],
      "metadata": {
        "id": "jLZFLyv4-A67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropout is applied by element-wise multiplication of activations and mask.\n",
        "\n",
        "[\n",
        "a' = a \\odot m = [2.0, 5.0, 7.12, 4.5, 6.0] \\odot [1,0,0,1,1]\n",
        "]\n",
        "\n",
        "[\n",
        "\\boxed{[2.0,; 0,; 0,; 4.5,; 6.0]}\n",
        "]\n",
        "\n",
        "(Kept neurons stay same, dropped neurons become 0.)\n"
      ],
      "metadata": {
        "id": "tLdqlKnIxAmt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Explain the difference between Gradient Descent, Stochastic Gradient Descent, Mini-Batch Gradient Descent"
      ],
      "metadata": {
        "id": "La3K0HmUGj8t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradient Descent:** Uses the entire training dataset to update weights in each step.\n",
        "\n",
        "**Stochastic Gradient Descent (SGD):** Updates weights using one training example at a time.\n",
        "\n",
        "**Mini-Batch Gradient Descent:** Updates weights using small batches of data, balancing speed and stability.\n"
      ],
      "metadata": {
        "id": "tVW7WKY4xMrk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are optimizers ? Explain Adam, RMS prop and Momentum in detail"
      ],
      "metadata": {
        "id": "FngV9WtV7JxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimizers:** Algorithms used to update neural network weights efficiently to minimize the loss function during training.\n",
        "\n",
        "**Momentum:** Uses past gradients to accelerate learning in the correct direction and reduce oscillations. It helps reach minima faster by smoothing updates.\n",
        "\n",
        "**RMSprop:** Adapts the learning rate for each parameter by dividing by the running average of squared gradients. It works well for non-stationary problems and prevents large updates.\n",
        "\n",
        "**Adam (Adaptive Moment Estimation):** Combines Momentum and RMSprop by using moving averages of both gradients and squared gradients. It provides fast convergence and is widely used in practice.\n"
      ],
      "metadata": {
        "id": "bo6rdnFSxZDr"
      }
    }
  ]
}