{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**SUBMISSION INSTRUCTIONS**\n",
        "\n",
        "First make a copy of this colab file and then solve the assignment and upload your final notebook on github.\n",
        "\n",
        "**To write an answer to any question, First Add a TEXT block below the question and type it in only one single block**\n",
        "\n",
        "Before uploading your downloaded notebook, RENAME the file as rollno_name.ipynb\n",
        "\n",
        "Submission Deadline : 19/12/2025 Friday EOD i.e before 11:59 PM\n",
        "\n",
        "The deadline is strict and will not be extended, Late submissions are not allowed\n",
        "\n",
        "Note that you have to upload your solution on the github page of the project Vision Transformer and **under Assignment 3**\n",
        "\n",
        "And remember to keep title of your pull request to be ViT_name_rollno_assgn3\n",
        "\n",
        "Github Submission repo -\n",
        "https://github.com/electricalengineersiitk/Winter-projects-25-26/tree/main/Vision%20transformer/Assignment%203"
      ],
      "metadata": {
        "id": "XmTMSeaTLwVp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**THEORETICAL ASSIGNMENT**\n",
        "\n"
      ],
      "metadata": {
        "id": "-TjKxTBEz8i7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This assignment consists of theoretical questions. Objective answers will not be accepted. You need to write detailed answers (200-300 words) for the questions that require more in-depth explanations (you should be able to identify these after reading the question and finding about them). Questions that need less explanation can be answered in 20-100 words. Please ensure the answers are well-written and thorough."
      ],
      "metadata": {
        "id": "AnVa1_8AKqtv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Assume that the inputs **X**\n",
        " to some scalar function **f**\n",
        " are **n x m**\n",
        " matrices. What is the dimensionality of the gradient of **f**\n",
        " with respect to **X**?\n",
        "**Give reasons to justify your answer.**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ERhYz9qV_YXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the input X is an n x m matrix and $f(X)$ is a scalar function, the gradient $\\nabla_X f$ is also an n x m matrix\n",
        "\n",
        " Justification: The gradient of a scalar function with respect to a matrix is defined as a matrix of the same dimensions where each element $(i, j)$ is the partial derivative of the function with respect to the corresponding element of the input matrix:$$\\nabla_X f = \\begin{bmatrix} \\frac{\\partial f}{\\partial X_{11}} & \\dots & \\frac{\\partial f}{\\partial X_{1m}} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial f}{\\partial X_{n1}} & \\dots & \\frac{\\partial f}{\\partial X_{nm}} \\end{bmatrix}$$"
      ],
      "metadata": {
        "id": "Cvo3ZCSV1NV-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Define the evaluation metrics Accuracy, Precision, Recall, and F1 score.\n",
        "\n",
        "   Consider the following diagram:\n",
        "   Find the values of Accuracy, Precision, Recall, and F1 score.\n",
        "   |                | Actual: Cancer | Actual: No Cancer |\n",
        "   |----------------|---------------|------------------|\n",
        "   | Predicted: Cancer | 80 | 80 |\n",
        "   | Predicted: No Cancer | 20 | 820 |\n",
        "    \n",
        "   \n",
        "   Find the values of Accuracy, Precision, Recall, and F1 score using the above data.\n"
      ],
      "metadata": {
        "id": "9vnyYHlq2g4B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "True positive(TP):80\n",
        "\n",
        "False Positive(FP):80\n",
        "\n",
        "False Negative(FN):20\n",
        "\n",
        "True Negative(TN):820\n",
        "\n",
        "Accuracy=(TP+TN)/total=900/1000=0.90\n",
        "\n",
        "Precision=(TP)/(TP+FP)=80/80+20=0.50\n",
        "\n",
        "Recall=TP/(TP+FP)=80/80+20=0.80\n",
        "\n",
        "F1_SCORE= $2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall} = 2 \\cdot \\frac{0.5 \\cdot 0.8}{0.5 + 0.8} = \\frac{0.8}{1.3}$"
      ],
      "metadata": {
        "id": "0Ca1Paml2NWs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is a confusion matrix ?"
      ],
      "metadata": {
        "id": "h1dJMD0vJKZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A confusion matrix is a performance measurement tool for machine learning classification problems. it is an $N \\times N$ table (where $N$ is the number of classes) that compares the actual target values with those predicted by the model. It allows for a deeper visualization of model performance than simple accuracy, as it explicitly shows which classes are being \"confused\" with one another, highlighting types of errors like False Positives and False Negatives"
      ],
      "metadata": {
        "id": "6DvoYY9i4Vjw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is overfitting and underfitting ?"
      ],
      "metadata": {
        "id": "aOqF8Dzw0DdZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overfitting: Occurs when a model learns the training data \"too well,\" including the noise and outliers. This results in low training error but high testing error (high variance). The model fails to generalize to new data.\n",
        "\n",
        "Underfitting: Occurs when a model is too simple to capture the underlying structure of the data (e.g., using a linear model for non-linear data). This results in high training error and high testing error (high bias)."
      ],
      "metadata": {
        "id": "Y5t6s50W4ZU6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Explain vanishing and exploding gradients, give reasons why they occur and how they can be prevented.\n"
      ],
      "metadata": {
        "id": "n4KfKc0z0ulm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This problem occurs during backpropagation in deep neural networks.\n",
        "\n",
        "Vanishing Gradients: When gradients become extremely small as they are propagated back to earlier layers. Because weights are updated proportionally to the gradient, early layers stop learning. This often happens with Sigmoid or Tanh activations where the derivative is $< 1$.\n",
        "\n",
        "Exploding Gradients: When gradients accumulate and result in very large updates to weights, making the model unstable. This is common in Recurrent Neural Networks (RNNs).Prevention: Use ReLU activation, Batch Normalization, proper Weight Initialization (like He or Xavier), or Gradient Clipping for exploding gradients."
      ],
      "metadata": {
        "id": "tCMLR5XV8krC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is regularization ? Explain L1 and L2 regularization"
      ],
      "metadata": {
        "id": "6yaOipvU1JoD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularization adds a penalty term to the loss function to prevent overfitting by discouraging overly complex models.\n",
        "\n",
        "L1 Regularization (Lasso): Adds the absolute value of weights to the loss: $Loss + \\lambda \\sum |w|$. It can force some weights to become exactly zero, effectively performing feature selection.\n",
        "\n",
        "L2 Regularization (Ridge): Adds the square of weights to the loss: $Loss + \\lambda \\sum w^2$. It penalizes large weights more heavily but rarely sets them to zero, leading to \"weight decay.\""
      ],
      "metadata": {
        "id": "eg726yqu8xPu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What is Dropout Layer and To which part of a neural network is dropout generally applied? how does it prevent overfitting"
      ],
      "metadata": {
        "id": "NjeRGpCE7QJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Dropout Layer is a regularization technique where, during training, a randomly selected subset of neurons is \"turned off\" (ignored) at each iteration.\n",
        "\n",
        "It is generally applied to hidden layers (fully connected layers).\n",
        "\n",
        "It prevents neurons from co-adapting (relying too much on specific other neurons). This forces the network to learn more robust features that are useful in conjunction with many different random subsets of other neurons."
      ],
      "metadata": {
        "id": "HAr7q-px89cs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. A hidden layer of a neural network has the following activations:\n",
        "\n",
        "   \\[\n",
        "   a = [2.0, 5.0, 7.12, 4.5, 6.0]\n",
        "   \\]\n",
        "\n",
        "   Dropout is applied **during training** with dropout probability \\(p = 0.5\\).\n",
        "\n",
        "   A randomly generated dropout mask is:\n",
        "\n",
        "   \\[\n",
        "   m = [1, 0, 0, 1, 1]\n",
        "   \\]\n",
        "\n",
        "   Here,  \n",
        "   - `1` → neuron is **kept**  \n",
        "   - `0` → neuron is **dropped**\n",
        "\n",
        "   ---\n",
        "\n",
        "   **Question:**  \n",
        "   What will be the output activations of this layer **after applying dropout**?\n",
        "   *(Show the final activation vector)*\n"
      ],
      "metadata": {
        "id": "jLZFLyv4-A67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1] $a \\cdot m = [2.0, 0, 0, 4.5, 6.0]$\n",
        "\n",
        "2]Scale by $\\frac{1}{1-0.5} = 2$:$$\\mathbf{a_{out} = [4.0, 0, 0, 9.0, 12.0]}$$"
      ],
      "metadata": {
        "id": "pCh14RUk9MLI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Explain the difference between Gradient Descent, Stochastic Gradient Descent, Mini-Batch Gradient Descent"
      ],
      "metadata": {
        "id": "La3K0HmUGj8t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch Gradient Descent: Uses the entire dataset to calculate the gradient for one update. It is stable but very slow for large datasets.\n",
        "\n",
        "Stochastic Gradient Descent (SGD): Updates weights using one sample at a time. It is very fast and can escape local minima due to noise, but the convergence path is very \"jumpy.\"\n",
        "\n",
        "Mini-Batch Gradient Descent: A compromise that uses a small batch (e.g., 32 or 64 samples). It offers a balance between the stability of Batch GD and the speed of SGD."
      ],
      "metadata": {
        "id": "5260LTj39kDu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are optimizers ? Explain Adam, RMS prop and Momentum in detail"
      ],
      "metadata": {
        "id": "FngV9WtV7JxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimizers are algorithms used to change the attributes of the neural network, such as weights and learning rate, to reduce the losses.\n",
        "\n",
        "Momentum: Accelerates SGD by navigating the search space more quickly. It considers the previous gradients to smooth out the updates (like a ball rolling down a hill gaining speed).\n",
        "\n",
        "RMSprop (Root Mean Square Propagation): It resolves the diminishing learning rate in Adagrad by using a moving average of squared gradients. This \"normalizes\" the gradient magnitude.\n",
        "\n",
        "Adam (Adaptive Moment Estimation): Combines the benefits of both Momentum and RMSprop. It maintains moving averages of both the gradients and the squared gradients. It is currently the industry standard because it is computationally efficient and requires little tuning."
      ],
      "metadata": {
        "id": "A0GKDgap-yTN"
      }
    }
  ]
}