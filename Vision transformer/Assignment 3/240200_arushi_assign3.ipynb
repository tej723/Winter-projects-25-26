{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**SUBMISSION INSTRUCTIONS**\n",
        "\n",
        "First make a copy of this colab file and then solve the assignment and upload your final notebook on github.\n",
        "\n",
        "**To write an answer to any question, First Add a TEXT block below the question and type it in only one single block**\n",
        "\n",
        "Before uploading your downloaded notebook, RENAME the file as rollno_name.ipynb\n",
        "\n",
        "Submission Deadline : 19/12/2025 Friday EOD i.e before 11:59 PM\n",
        "\n",
        "The deadline is strict and will not be extended, Late submissions are not allowed\n",
        "\n",
        "Note that you have to upload your solution on the github page of the project Vision Transformer and **under Assignment 3**\n",
        "\n",
        "And remember to keep title of your pull request to be ViT_name_rollno_assgn3\n",
        "\n",
        "Github Submission repo -\n",
        "https://github.com/electricalengineersiitk/Winter-projects-25-26/tree/main/Vision%20transformer/Assignment%203"
      ],
      "metadata": {
        "id": "XmTMSeaTLwVp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**THEORETICAL ASSIGNMENT**\n",
        "\n"
      ],
      "metadata": {
        "id": "-TjKxTBEz8i7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This assignment consists of theoretical questions. Objective answers will not be accepted. You need to write detailed answers (200-300 words) for the questions that require more in-depth explanations (you should be able to identify these after reading the question and finding about them). Questions that need less explanation can be answered in 20-100 words. Please ensure the answers are well-written and thorough."
      ],
      "metadata": {
        "id": "AnVa1_8AKqtv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Assume that the inputs **X**\n",
        " to some scalar function **f**\n",
        " are **n x m**\n",
        " matrices. What is the dimensionality of the gradient of **f**\n",
        " with respect to **X**?\n",
        "**Give reasons to justify your answer.**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ERhYz9qV_YXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dimensionality of the gradient of a scalar function f with respect to an $n \\times m$ matrix X is also $n \\times m$.\n",
        "\n",
        "Justification: The gradient is a collection of partial derivatives for every element in the input matrix. To represent the rate of change for each corresponding element $x_{ij}$, the gradient matrix must mirror the shape of the original input matrix."
      ],
      "metadata": {
        "id": "NfFDwr7Gi9_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Define the evaluation metrics Accuracy, Precision, Recall, and F1 score.\n",
        "\n",
        "   Consider the following diagram:\n",
        "   Find the values of Accuracy, Precision, Recall, and F1 score.\n",
        "   |                | Actual: Cancer | Actual: No Cancer |\n",
        "   |----------------|---------------|------------------|\n",
        "   | Predicted: Cancer | 80 | 80 |\n",
        "   | Predicted: No Cancer | 20 | 820 |\n",
        "    \n",
        "   \n",
        "   Find the values of Accuracy, Precision, Recall, and F1 score using the above data.\n"
      ],
      "metadata": {
        "id": "9vnyYHlq2g4B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy: The ratio of correct predictions to the total number of cases.\n",
        "Accuracy: $\\frac{80 + 820}{1000} = \\mathbf{0.90}$ (or 90%)\n",
        "\n",
        "Precision: The ratio of true positives to all predicted positives (how many predicted \"Cancers\" actually had it).\n",
        "Precision: $\\frac{80}{80 + 80} = \\mathbf{0.50}$ (or 50%)\n",
        "\n",
        "Recall: The ratio of true positives to all actual positives (how many actual \"Cancers\" were caught).\n",
        "\n",
        "Recall: $\\frac{80}{80 + 20} = \\mathbf{0.80}$ (or 80%)\n",
        "\n",
        "F1 Score: The harmonic mean of Precision and Recall, balancing the two.\n",
        "\n",
        "F1 Score: $2 \\times \\frac{0.50 \\times 0.80}{0.50 + 0.80} = \\mathbf{0.615}$"
      ],
      "metadata": {
        "id": "k0KUZWG7jQOV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is a confusion matrix ?"
      ],
      "metadata": {
        "id": "h1dJMD0vJKZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A confusion matrix is a table used to evaluate the performance of a classification model. It displays the counts of True Positives, True Negatives, False Positives, and False Negatives, allowing you to see exactly where the model is making errors (confusing classes)."
      ],
      "metadata": {
        "id": "SUsxuAJ4jy4V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is overfitting and underfitting ?"
      ],
      "metadata": {
        "id": "aOqF8Dzw0DdZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overfitting: Occurs when a model learns the training data \"too well,\" including the noise and random fluctuations. It performs great on training data but poorly on new, unseen data (low bias, high variance).\n",
        "\n",
        "Underfitting: Occurs when a model is too simple to learn the underlying pattern of the data. It performs poorly on both the training data and new data (high bias, low variance)"
      ],
      "metadata": {
        "id": "B3sW1DQ5j8ZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Explain vanishing and exploding gradients, give reasons why they occur and how they can be prevented.\n"
      ],
      "metadata": {
        "id": "n4KfKc0z0ulm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vanishing Gradients: Occur when gradients become extremely small during backpropagation, causing weights in early layers to stop updating. This is often caused by using certain activation functions (like Sigmoid) in very deep networks. Prevention: Use ReLU activation or Batch Normalization.\n",
        "\n",
        "Exploding Gradients: Occur when gradients accumulate and result in very large weight updates, making the model unstable. This is caused by large weight initializations. Prevention: Use Gradient Clipping or proper weight initialization (He/Xavier)"
      ],
      "metadata": {
        "id": "zUHb4MEnkOV-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is regularization ? Explain L1 and L2 regularization"
      ],
      "metadata": {
        "id": "6yaOipvU1JoD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularization is a technique used to prevent overfitting by adding a penalty term to the cost function that discourages complex models.\n",
        "\n",
        "L1 Regularization (Lasso): Adds a penalty equal to the absolute value of the weights ($|w|$). It can lead to \"sparse\" matrices where some weights become exactly zero, effectively performing feature selection.\n",
        "\n",
        "L2 Regularization (Ridge): Adds a penalty equal to the square of the weights ($w^2$). This forces weights to be small but rarely zero, distributing the error across all parameters."
      ],
      "metadata": {
        "id": "x01tqedjkRAm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What is Dropout Layer and To which part of a neural network is dropout generally applied? how does it prevent overfitting"
      ],
      "metadata": {
        "id": "NjeRGpCE7QJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropout is a regularization technique where randomly selected neurons are \"ignored\" (dropped out) during training.\n",
        "\n",
        "Application: It is generally applied to the hidden layers of a neural network.\n",
        "\n",
        "How it prevents overfitting: It prevents neurons from co-adapting too much. By forcing the network to learn multiple independent representations of the same data, it ensures that the model does not rely too heavily on any single neuron or specific path, making it more robust and generalized."
      ],
      "metadata": {
        "id": "mqg4a5mdkXsF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. A hidden layer of a neural network has the following activations:\n",
        "\n",
        "   \\[\n",
        "   a = [2.0, 5.0, 7.12, 4.5, 6.0]\n",
        "   \\]\n",
        "\n",
        "   Dropout is applied **during training** with dropout probability \\(p = 0.5\\).\n",
        "\n",
        "   A randomly generated dropout mask is:\n",
        "\n",
        "   \\[\n",
        "   m = [1, 0, 0, 1, 1]\n",
        "   \\]\n",
        "\n",
        "   Here,  \n",
        "   - `1` → neuron is **kept**  \n",
        "   - `0` → neuron is **dropped**\n",
        "\n",
        "   ---\n",
        "\n",
        "   **Question:**  \n",
        "   What will be the output activations of this layer **after applying dropout**?\n",
        "   *(Show the final activation vector)*\n"
      ],
      "metadata": {
        "id": "jLZFLyv4-A67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given activations $a = [2.0, 5.0, 7.12, 4.5, 6.0]$ and mask $m = [1, 0, 0, 1, 1]$ with $p=0.5$:\n",
        "\n",
        "In standard dropout, we multiply the activations by the mask and scale by $\\frac{1}{1-p}$ (Inverted Dropout) to keep the expected sum of activations the same.\n",
        "\n",
        "Calculation: a_drop = (a*m)/0.5\n",
        "\n",
        "Final Vector: [(2)/0.5, 0, 0, (4.5)/0.5, (6)/0.5] = [4.0, 0, 0, 9.0, 12.0]"
      ],
      "metadata": {
        "id": "Q08IAZXPkfhk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zpjISs0Gkvwz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Explain the difference between Gradient Descent, Stochastic Gradient Descent, Mini-Batch Gradient Descent"
      ],
      "metadata": {
        "id": "La3K0HmUGj8t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch Gradient Descent: Uses the entire dataset to calculate the gradient for one update. Stable but very slow for large data.\n",
        "\n",
        "Stochastic Gradient Descent (SGD): Uses one sample per update. Very fast and can escape local minima, but very \"noisy\"/erratic.\n",
        "\n",
        "Mini-Batch Gradient Descent: Uses a small subset (batch) of data. It provides a balance between the stability of Batch and the speed of SGD."
      ],
      "metadata": {
        "id": "NP83ssbMle4n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are optimizers ? Explain Adam, RMS prop and Momentum in detail"
      ],
      "metadata": {
        "id": "FngV9WtV7JxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimizers are algorithms used to change the attributes of the neural network, such as weights and learning rates, to reduce losses.\n",
        "\n",
        "Momentum: Adds a fraction of the previous update to the current one to accelerate in the right direction and dampen oscillations.\n",
        "\n",
        "RMSprop: Scales the learning rate for each parameter based on the average of recent magnitudes of the gradients, preventing it from diverging.\n",
        "\n",
        "Adam: Combines the benefits of both Momentum and RMSprop. It is currently the most popular choice as it handles sparse gradients and noisy data efficiently."
      ],
      "metadata": {
        "id": "i1yA7FdRlj7z"
      }
    }
  ]
}