{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**SUBMISSION INSTRUCTIONS**\n",
        "\n",
        "First make a copy of this colab file and then solve the assignment and upload your final notebook on github.\n",
        "\n",
        "**To write an answer to any question, First Add a TEXT block below the question and type it in only one single block**\n",
        "\n",
        "Before uploading your downloaded notebook, RENAME the file as rollno_name.ipynb\n",
        "\n",
        "Submission Deadline : 19/12/2025 Friday EOD i.e before 11:59 PM\n",
        "\n",
        "The deadline is strict and will not be extended, Late submissions are not allowed\n",
        "\n",
        "Note that you have to upload your solution on the github page of the project Vision Transformer and **under Assignment 3**\n",
        "\n",
        "And remember to keep title of your pull request to be ViT_name_rollno_assgn3\n",
        "\n",
        "Github Submission repo -\n",
        "https://github.com/electricalengineersiitk/Winter-projects-25-26/tree/main/Vision%20transformer/Assignment%203"
      ],
      "metadata": {
        "id": "XmTMSeaTLwVp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**THEORETICAL ASSIGNMENT**\n",
        "\n"
      ],
      "metadata": {
        "id": "-TjKxTBEz8i7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This assignment consists of theoretical questions. Objective answers will not be accepted. You need to write detailed answers (200-300 words) for the questions that require more in-depth explanations (you should be able to identify these after reading the question and finding about them). Questions that need less explanation can be answered in 20-100 words. Please ensure the answers are well-written and thorough."
      ],
      "metadata": {
        "id": "AnVa1_8AKqtv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Assume that the inputs **X**\n",
        " to some scalar function **f**\n",
        " are **n x m**\n",
        " matrices. What is the dimensionality of the gradient of **f**\n",
        " with respect to **X**?\n",
        "**Give reasons to justify your answer.**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ERhYz9qV_YXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gradient of a scalar function f with respect to a matrix X (of size n x m) has the same dimensions as X, i.e., n x m.\n",
        " This is because the gradient contains the partial derivative of f with respect to each element of X.\n",
        " Each entry (i, j) in the gradient is df/dX_ij, representing how a small change in that element affects f.\n",
        " Therefore, the gradient mirrors the shape of the input matrix X."
      ],
      "metadata": {
        "id": "4lBBcNQ0oY3P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Define the evaluation metrics Accuracy, Precision, Recall, and F1 score.\n",
        "\n",
        "   Consider the following diagram:\n",
        "   Find the values of Accuracy, Precision, Recall, and F1 score.\n",
        "   |                | Actual: Cancer | Actual: No Cancer |\n",
        "   |----------------|---------------|------------------|\n",
        "   | Predicted: Cancer | 80 | 80 |\n",
        "   | Predicted: No Cancer | 20 | 820 |\n",
        "    \n",
        "   \n",
        "   Find the values of Accuracy, Precision, Recall, and F1 score using the above data.\n"
      ],
      "metadata": {
        "id": "9vnyYHlq2g4B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy, Precision, Recall, and F1-score are evaluation metrics used to measure the performance\n",
        "of a classification model, especially in binary classification problems.\n",
        "\n",
        "Accuracy measures the overall correctness of the model.\n",
        "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "\n",
        "Precision measures how many of the predicted positive cases are actually positive.\n",
        "Precision = TP / (TP + FP)\n",
        "\n",
        "Recall (Sensitivity) measures how many of the actual positive cases are correctly predicted.\n",
        "Recall = TP / (TP + FN)\n",
        "\n",
        "F1-score is the harmonic mean of Precision and Recall.\n",
        "F1 = 2 * (Precision * Recall) / (Precision + Recall)\n",
        "\n",
        "From the given confusion matrix:\n",
        "TP (Predicted Cancer & Actual Cancer) = 80\n",
        "FP (Predicted Cancer & Actual No Cancer) = 80\n",
        "FN (Predicted No Cancer & Actual Cancer) = 20\n",
        "TN (Predicted No Cancer & Actual No Cancer) = 820\n",
        "\n",
        "Total samples = 80 + 80 + 20 + 820 = 1000\n",
        "\n",
        "Accuracy = (80 + 820) / 1000 = 900 / 1000 = 0.90 (90%)\n",
        "Precision = 80 / (80 + 80) = 80 / 160 = 0.50 (50%)\n",
        "Recall = 80 / (80 + 20) = 80 / 100 = 0.80 (80%)\n",
        "F1-score = 2 * (0.50 * 0.80) / (0.50 + 0.80) = 0.62 (62%)\n"
      ],
      "metadata": {
        "id": "UNrE6-m4tMsG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is a confusion matrix ?"
      ],
      "metadata": {
        "id": "h1dJMD0vJKZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A confusion matrix is a table used to evaluate the performance of a classification model.\n",
        " It compares the predicted labels from the model with the actual true labels from the data.\n",
        " The matrix has rows representing the actual classes and columns representing the predicted classes.\n",
        " Each entry in the matrix shows the number of instances where a particular actual class was predicted as a certain class.\n"
      ],
      "metadata": {
        "id": "JIXQp7iyoxnS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is overfitting and underfitting ?"
      ],
      "metadata": {
        "id": "aOqF8Dzw0DdZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overfitting occurs when a model learns the training data too well, including noise and minor details,\n",
        " causing it to perform poorly on new, unseen data. It has low training error but high test error.\n",
        "\n",
        "Underfitting occurs when a model is too simple to capture the underlying patterns in the data,\n",
        "resulting in poor performance on both training and test data. It has high training and test error."
      ],
      "metadata": {
        "id": "eZuGMa1epJxB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Explain vanishing and exploding gradients, give reasons why they occur and how they can be prevented.\n"
      ],
      "metadata": {
        "id": "n4KfKc0z0ulm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vanishing gradients occur when the gradients of a neural network become very small during backpropagation.\n",
        "This makes the weights update very slowly, especially in deep networks, and the network struggles to learn.\n",
        "It often happens with activation functions like sigmoid or tanh, where derivatives are less than 1,\n",
        "causing gradients to shrink exponentially across layers.\n",
        "\n",
        "Exploding gradients occur when the gradients become excessively large during backpropagation.\n",
        "This can cause the weights to grow uncontrollably, leading to unstable training or NaN values.\n",
        "It often happens in very deep networks or with large initial weights.\n",
        "\n",
        "Prevention techniques:\n",
        "1. Use proper weight initialization (e.g., Xavier or He initialization) to keep gradients in a reasonable range.\n",
        "2. Use activation functions like ReLU that reduce vanishing gradient problems.\n",
        "3. Apply gradient clipping to prevent exploding gradients.\n",
        "4. Use normalization techniques like batch normalization to stabilize training.\n"
      ],
      "metadata": {
        "id": "hpFmRxoopX4F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is regularization ? Explain L1 and L2 regularization"
      ],
      "metadata": {
        "id": "6yaOipvU1JoD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularization is a technique used to prevent overfitting in machine learning models by adding a penalty to the loss function.\n",
        "' It discourages the model from learning overly complex patterns that do not generalize well to new data.\n",
        "\n",
        "L1 regularization (Lasso) adds the sum of the absolute values of the weights to the loss function.\n",
        "It can drive some weights to exactly zero, effectively performing feature selection and creating a sparse model.\n",
        "\n",
        "L2 regularization (Ridge) adds the sum of the squares of the weights to the loss function.\n",
        "It reduces the magnitude of all weights but does not make them exactly zero, helping to keep the model simple and stable.\n"
      ],
      "metadata": {
        "id": "2FC2Z2qWppPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What is Dropout Layer and To which part of a neural network is dropout generally applied? how does it prevent overfitting"
      ],
      "metadata": {
        "id": "NjeRGpCE7QJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Dropout layer is a regularization technique used in neural networks to prevent overfitting.\n",
        "During training, it randomly \"drops\" a fraction of neurons in the layer by setting their output to zero.\n",
        "This forces the network to not rely on specific neurons and encourages it to learn more robust and general features.\n",
        "\n",
        "Dropout is generally applied to fully connected (dense) layers, but it can also be used in convolutional layers.\n",
        "\n",
        "By randomly dropping neurons, the network effectively trains multiple different sub-networks simultaneously.\n",
        "This reduces the chance of the network memorizing the training data, improving generalization and preventing overfitting.\n"
      ],
      "metadata": {
        "id": "DolW_JT-qY6h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. A hidden layer of a neural network has the following activations:\n",
        "\n",
        "   \\[\n",
        "   a = [2.0, 5.0, 7.12, 4.5, 6.0]\n",
        "   \\]\n",
        "\n",
        "   Dropout is applied **during training** with dropout probability \\(p = 0.5\\).\n",
        "\n",
        "   A randomly generated dropout mask is:\n",
        "\n",
        "   \\[\n",
        "   m = [1, 0, 0, 1, 1]\n",
        "   \\]\n",
        "\n",
        "   Here,  \n",
        "   - `1` → neuron is **kept**  \n",
        "   - `0` → neuron is **dropped**\n",
        "\n",
        "   ---\n",
        "\n",
        "   **Question:**  \n",
        "   What will be the output activations of this layer **after applying dropout**?\n",
        "   *(Show the final activation vector)*\n"
      ],
      "metadata": {
        "id": "jLZFLyv4-A67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given activations: a = [2.0, 5.0, 7.12, 4.5, 6.0]\n",
        "Dropout probability p = 0.5, so keep probability = 1 - p = 0.5\n",
        "Dropout mask: m = [1, 0, 0, 1, 1]\n",
        "\n",
        "During training, inverted dropout is used, where kept activations are scaled by 1/(1-p).\n",
        " Scaling factor = 1 / 0.5 = 2\n",
        "\n",
        "Apply mask and scaling element-wise:\n",
        "[2.0*1*2, 5.0*0*2, 7.12*0*2, 4.5*1*2, 6.0*1*2]\n",
        "= [4.0, 0.0, 0.0, 9.0, 12.0]\n",
        "\n",
        "Final output activation vector after applying dropout:\n",
        "[4.0, 0.0, 0.0, 9.0, 12.0]\n"
      ],
      "metadata": {
        "id": "SWT85CsPt_8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Explain the difference between Gradient Descent, Stochastic Gradient Descent, Mini-Batch Gradient Descent"
      ],
      "metadata": {
        "id": "La3K0HmUGj8t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Descent (GD) is an optimization algorithm where the model's weights are updated\n",
        "using the gradient of the loss function computed over the entire training dataset.\n",
        "It is accurate but can be very slow for large datasets.\n",
        "Stochastic Gradient Descent (SGD) updates the model's weights using the gradient computed\n",
        "from only one training example at a time. It is much faster and introduces noise,\n",
        "which can help escape local minima, but the updates are less stable.\n",
        "\n",
        "Mini-Batch Gradient Descent is a compromise between GD and SGD.\n",
        "It computes the gradient using a small batch of training examples (e.g., 32 or 64) for each update.\n",
        "It balances the speed of SGD and the stability of GD, and is the most commonly used method in practice.\n"
      ],
      "metadata": {
        "id": "1ombY_6arF7h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are optimizers ? Explain Adam, RMS prop and Momentum in detail"
      ],
      "metadata": {
        "id": "FngV9WtV7JxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimizers are algorithms used to update the weights of a neural network during training\n",
        "to minimize the loss function. They determine how the model learns from the gradients.\n",
        "\n",
        "Momentum is an optimizer that accelerates gradient descent by adding a fraction of the previous\n",
        "weight update to the current update. This helps smooth out updates, reduces oscillations,\n",
        "and speeds up convergence, especially in regions with shallow gradients.\n",
        "\n",
        "RMSProp (Root Mean Square Propagation) adapts the learning rate for each parameter individually\n",
        "by dividing the gradient by a moving average of its recent squared values.\n",
        "This helps handle the problem of vanishing or exploding gradients and improves convergence\n",
        "on non-stationary or noisy data.\n",
        "\n",
        "Adam (Adaptive Moment Estimation) combines Momentum and RMSProp.\n",
        "It maintains running averages of both the gradients (first moment) and the squared gradients (second moment).\n",
        "Adam adapts the learning rate for each parameter and also accelerates updates in relevant directions,\n",
        "making it one of the most popular and effective optimizers for deep learning.\n"
      ],
      "metadata": {
        "id": "873xeHLZrZBZ"
      }
    }
  ]
}