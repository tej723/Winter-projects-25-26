{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Assignment Roadmap**\n",
        "\n",
        "This assignment asks you to build a complete Brain-Computer Interface (BCI) pipeline. Your goal is to take raw, noisy electrical brain signals and turn them into a clear Yes/No decision: Is this the character the user wants?\n",
        "\n",
        "**Theres not much theory to learn other than implementation, you have to learn this by doing**\n",
        "\n",
        "## AI Usage Policy for This Assignment\n",
        "\n",
        "You're welcome to use AI for this assignment. Given the complexity of EEG signal processing and machine learning,\n",
        "We don't expect you to know every implementation detail from scratch and neither does any recuiter or any professor.\n",
        "\n",
        "\n",
        "Use AI to:\n",
        "\n",
        "    Debug errors and troubleshoot issues\n",
        "\n",
        "    Understand concepts and explore different approaches\n",
        "\n",
        "What matters:\n",
        "\n",
        "    You understand your code and can explain how it works\n",
        "\n",
        "    You learn from the process, not just copy-paste\n",
        "\n",
        "### **1: Cleaning the Signal (Preprocessing)**\n",
        "\n",
        "The Goal: Raw EEG data is full of \"garbage\" frequencies like muscle movement and electrical hum. You need to filter the data to keep only the brain waves relevant to the P300 response (typically 0.1Hz – 20Hz).\n",
        "\n",
        "You have already done this in the previous assignment but this one is a more standard procedure.\n",
        "\n",
        "Common Pitfalls:\n",
        "\n",
        "    Filter Lag: Standard filters can delay the signal, meaning the brain response looks like it happened later than it actually did. To prevent this, use zero-phase filtering (e.g., scipy.signal.filtfilt) instead of standard filtering (lfilter).\n",
        "\n",
        "    Aliasing: You are asked to downsample the data from 240Hz to 60Hz to make it faster to process. Do not simply slice the array (e.g., data[::4]) without filtering first. If you do, high-frequency noise will \"fold over\" into your low frequencies and corrupt the data. Always filter before downsampling.\n",
        "\n",
        "### **2: Epoch Extraction**\n",
        "\n",
        "The Goal: You need to convert the continuous stream of data into specific \"events\" or \"epochs.\"\n",
        "\n",
        "The Concept: A P300 response happens roughly 300ms after a flash. Your code needs to identify every moment a flash occurs (stimulus_onset), look forward in time (e.g., 800ms), and \"snip\" that window of data out.\n",
        "\n",
        "Visualizing the Data Structure:\n",
        "\n",
        "    Input: A continuous 2D matrix (Total_Time_Points, 64_Channels)\n",
        "\n",
        "    Output: A 3D block of events (Number_of_Flashes, Time_Points_Per_Window, 64_Channels)\n",
        "\n",
        "Common Pitfall:\n",
        "\n",
        "    Indexing Errors: This dataset may originate from MATLAB (which uses 1-based indexing), while Python uses 0-based indexing. If your index calculation is off by even one sample, your window will shift, and the machine learning model will be training on random noise rather than the brain response. Double-check your start and end indices.\n",
        "\n",
        "### **3: Making Data \"Model-Ready\" (Feature Engineering)**\n",
        "\n",
        "The Goal: Standard Machine Learning models (like SVM or LDA) cannot understand 3D arrays. They generally require a 2D matrix (like an Excel sheet). The Task:\n",
        "\n",
        "    Time Domain: You will need to \"flatten\" the epochs. If an epoch is 60 time points × 64 channels, it becomes a single flat row of 3,840 numbers.\n",
        "\n",
        "    PCA/CSP: These are compression techniques. The goal is to reduce those 3,840 numbers down to perhaps 20 numbers that capture the most important information.\n",
        "\n",
        "Common Pitfall:\n",
        "\n",
        "    Data Leakage: When using PCA or CSP, you must be careful not to \"cheat.\" You should .fit() your reducer only on the training data, and then .transform() the test data. If you fit on the combined dataset, your model \"sees\" the test answers ahead of time, leading to artificially high scores that won't work in the real world.\n",
        "\n",
        "### **4: Classification**\n",
        "\n",
        "The Goal: Feed your features into the ML models (LDA, SVM, etc.) provided in the skeleton code to classify \"Target\" vs. \"Non-Target\" flashes.\n",
        "\n",
        "Common Pitfall:\n",
        "\n",
        "    Class Imbalance: In a P300 speller, the letter the user wants (Target) only flashes 1 out of 6 times. The other 5 flashes are Non-Targets.\n",
        "\n",
        "        If your model decides to simply guess \"Non-Target\" every single time, it will still achieve ~83% accuracy. This is a useless model.\n",
        "\n",
        "        Do not rely solely on Accuracy. Check the F1-Score or the Confusion Matrix. A good model must be able to correctly identify the rare Target events, not just the frequent Non-Targets."
      ],
      "metadata": {
        "id": "Ylwj81y1XJBx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "aPwo3PCfXrg5"
      },
      "outputs": [],
      "source": [
        "# The assignment is structured in a way that its modular so thats its easier to debug whats wrong\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as sio\n",
        "from scipy.signal import butter, filtfilt, iirnotch\n",
        "from scipy.linalg import eigh\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, roc_auc_score, classification_report,\n",
        "                             confusion_matrix)\n",
        "import pickle\n",
        "import json\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "GzYFt1HdZy-B"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# SECTION 1: DATA LOADING AND BASIC SETUP\n",
        "################################################################################\n",
        "\n",
        "# Character matrix (6x6) for P300 speller\n",
        "CHAR_MATRIX = np.array([\n",
        "    ['A', 'B', 'C', 'D', 'E', 'F'],\n",
        "    ['G', 'H', 'I', 'J', 'K', 'L'],\n",
        "    ['M', 'N', 'O', 'P', 'Q', 'R'],\n",
        "    ['S', 'T', 'U', 'V', 'W', 'X'],\n",
        "    ['Y', 'Z', '1', '2', '3', '4'],\n",
        "    ['5', '6', '7', '8', '9', '_']\n",
        "])\n",
        "\n",
        "def load_data(file_path):\n",
        "    \"\"\"\n",
        "    Load P300 BCI Competition III data\n",
        "    Returns dictionary with signal, flashing, stimulus_code, stimulus_type, target_char\n",
        "    \"\"\"\n",
        "    data = sio.loadmat(file_path)\n",
        "\n",
        "    # Handle variable names which might vary slightly (Signal vs Signal(x))\n",
        "    # Standardizing to keys used in BCI Comp III Dataset II\n",
        "    result = {\n",
        "        'signal': data.get('Signal'),\n",
        "        'flashing': data.get('Flashing'),\n",
        "        'stimulus_code': data.get('StimulusCode'),\n",
        "    }\n",
        "\n",
        "    # Training data has labels, test data doesn't\n",
        "    if 'StimulusType' in data:\n",
        "        result['stimulus_type'] = data.get('StimulusType')\n",
        "    if 'TargetChar' in data:\n",
        "        result['target_char'] = data.get('TargetChar')\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def get_char_from_codes(row_code, col_code):\n",
        "    \"\"\"Convert row/column stimulus codes to character\"\"\"\n",
        "    r_idx = -1\n",
        "    c_idx = -1\n",
        "\n",
        "    if 7 <= row_code <= 12:\n",
        "        r_idx = row_code - 7\n",
        "    elif 1 <= row_code <= 6:\n",
        "        c_idx = row_code - 1\n",
        "\n",
        "    if 7 <= col_code <= 12:\n",
        "        r_idx = col_code - 7\n",
        "    elif 1 <= col_code <= 6:\n",
        "        c_idx = col_code - 1\n",
        "\n",
        "    if r_idx != -1 and c_idx != -1:\n",
        "        return CHAR_MATRIX[r_idx, c_idx]\n",
        "    return None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "p6MhOVcFZ2tU"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# SECTION 2: EEG SIGNAL ACQUISITION & PREPROCESSING\n",
        "################################################################################\n",
        "\n",
        "def bandpass_filter(signal, lowcut=0.1, highcut=20, fs=240, order=5):\n",
        "    \"\"\"\n",
        "    Apply band-pass filter to remove low-frequency drift and high-frequency noise\n",
        "    Typical P300 band: 0.1-20 Hz\n",
        "    \"\"\"\n",
        "    nyquist = 0.5 * fs\n",
        "    low = lowcut / nyquist\n",
        "    high = highcut / nyquist\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    # Use filtfilt for zero-phase filtering (no delay)\n",
        "    filtered_signal = filtfilt(b, a, signal, axis=0)\n",
        "    return filtered_signal\n",
        "\n",
        "def notch_filter(signal, freq=50, fs=240, Q=30):\n",
        "    \"\"\"\n",
        "    Remove powerline interference (50/60 Hz)\n",
        "    \"\"\"\n",
        "    nyquist = 0.5 * fs\n",
        "    w0 = freq / nyquist\n",
        "    b, a = iirnotch(w0, Q)\n",
        "    filtered_signal = filtfilt(b, a, signal, axis=0)\n",
        "    return filtered_signal\n",
        "\n",
        "\n",
        "def baseline_correction(epoch, baseline_samples=50):\n",
        "    \"\"\"\n",
        "    Apply baseline correction by subtracting pre-stimulus baseline\n",
        "    \"\"\"\n",
        "    baseline_mean = np.mean(epoch[:baseline_samples, :], axis=0)\n",
        "    return epoch - baseline_mean\n",
        "\n",
        "def downsample_signal(signal, original_fs=240, target_fs=60):\n",
        "    \"\"\"\n",
        "    Downsample signal to reduce computational load\n",
        "    240 Hz -> 60 Hz reduces data by 4x\n",
        "    \"\"\"\n",
        "    factor = int(original_fs / target_fs)\n",
        "    # Simple slicing is okay ONLY IF the signal was low-pass filtered beforehand\n",
        "    # to avoid aliasing. Our pipeline does bandpass (max 20Hz) before this.\n",
        "    return signal[::factor, :]\n",
        "\n",
        "def artifact_rejection(signal, threshold=100):\n",
        "    \"\"\"\n",
        "    Simple artifact rejection based on amplitude threshold\n",
        "    More advanced: use ICA or wavelet denoising\n",
        "    \"\"\"\n",
        "    # Identify channels or epochs that exceed threshold\n",
        "    # For this assignment, we might just clip or return indices,\n",
        "    # but here we'll just clip for simplicity/safety\n",
        "    return np.clip(signal, -threshold, threshold)\n",
        "\n",
        "def preprocess_pipeline(data, apply_bandpass=True, apply_notch=True,\n",
        "                        apply_downsample=True, fs=240):\n",
        "    \"\"\"\n",
        "    Complete preprocessing pipeline:\n",
        "    1. Bandpass filtering (0.1-20 Hz)\n",
        "    2. Notch filtering (50 Hz)\n",
        "    3. Downsampling (240->60 Hz)\n",
        "    \"\"\"\n",
        "    signal = data['signal'].astype(np.float32)\n",
        "\n",
        "    # Ensure signal is 2D (time_points, channels) if it loaded as (1, time_points, channels)\n",
        "    # This fixes the issue with filtering and downsampling operating on a dimension of size 1.\n",
        "    if signal.ndim == 3 and signal.shape[0] == 1:\n",
        "        signal = signal.squeeze(axis=0)\n",
        "    elif signal.ndim != 2: # Expected (time_points, channels)\n",
        "        print(f\"Warning: Unexpected signal dimension {signal.ndim}. Expected 2D (Time, Channels) or 3D (1, Time, Channels). Proceeding assuming (Time, Channels).\")\n",
        "\n",
        "\n",
        "    if apply_notch:\n",
        "        signal = notch_filter(signal, freq=50, fs=fs)\n",
        "\n",
        "    if apply_bandpass:\n",
        "        signal = bandpass_filter(signal, lowcut=0.1, highcut=20, fs=fs)\n",
        "\n",
        "    if apply_downsample:\n",
        "        signal = downsample_signal(signal, original_fs=fs, target_fs=60)\n",
        "        # Update sampling rate for downstream\n",
        "        fs = 60\n",
        "\n",
        "    # Update data dictionary with processed signal\n",
        "    processed_data = data.copy()\n",
        "    processed_data['signal'] = signal\n",
        "\n",
        "    # Also downsample the flashing and stimulus codes to match indices\n",
        "    if apply_downsample:\n",
        "        factor = 240 // 60\n",
        "        # Ensure flashing, stimulus_code, and stimulus_type are 1D before slicing\n",
        "        processed_data['flashing'] = data['flashing'].flatten()[::factor]\n",
        "        processed_data['stimulus_code'] = data['stimulus_code'].flatten()[::factor]\n",
        "        if 'stimulus_type' in data:\n",
        "            processed_data['stimulus_type'] = data['stimulus_type'].flatten()[::factor]\n",
        "\n",
        "    return processed_data\n",
        "\n",
        "def extract_epochs(data, epoch_length_ms=800, fs=60):\n",
        "    \"\"\"\n",
        "    Extract epochs around stimulus onset\n",
        "    - Event tagging: Use flashing signal to detect stimulus onset\n",
        "    - Stimulus alignment: Extract fixed-length windows after each flash\n",
        "    - Epoch extraction: Collect all stimulus-locked epochs\n",
        "\n",
        "    Returns: Dictionary with epochs, labels, codes, character indices\n",
        "    \"\"\"\n",
        "    signal = data['signal']\n",
        "    flashing = data['flashing'].flatten()\n",
        "    codes = data['stimulus_code'].flatten()\n",
        "\n",
        "    has_labels = 'stimulus_type' in data\n",
        "    if has_labels:\n",
        "        labels_in = data['stimulus_type'].flatten()\n",
        "\n",
        "    samples_per_epoch = int((epoch_length_ms / 1000) * fs)\n",
        "\n",
        "    epochs = []\n",
        "    labels = []\n",
        "    codes_list = []\n",
        "    char_idx_list = [] # Keeps track of which character/block this epoch belongs to\n",
        "\n",
        "    # Find stimulus onsets (flashing goes from 0 to 1)\n",
        "    # np.diff gives change; we want where it goes 0->1\n",
        "    changes = np.diff(flashing)\n",
        "    onset_indices = np.where(changes == 1)[0] + 1\n",
        "\n",
        "    for idx in onset_indices:\n",
        "        # Check boundary\n",
        "        if idx + samples_per_epoch <= len(signal):\n",
        "            epoch = signal[idx : idx + samples_per_epoch, :]\n",
        "\n",
        "            # Apply baseline correction\n",
        "            epoch = baseline_correction(epoch, baseline_samples=int(0.1*fs)) # 100ms baseline\n",
        "\n",
        "            epochs.append(epoch)\n",
        "            codes_list.append(codes[idx])\n",
        "\n",
        "            # Dummy char index for now, in real scenario we track block index\n",
        "            char_idx_list.append(0)\n",
        "\n",
        "            if has_labels:\n",
        "                # Label 1 = Target, 0 = Non-Target\n",
        "                labels.append(labels_in[idx])\n",
        "\n",
        "    if not has_labels:\n",
        "        labels = [-1] * len(epochs) # Dummy labels for test set\n",
        "\n",
        "    return {\n",
        "        'epochs': np.array(epochs),\n",
        "        'labels': np.array(labels),\n",
        "        'codes': np.array(codes_list),\n",
        "        'char_indices': np.array(char_idx_list)\n",
        "    }\n",
        "\n",
        "def plot_erp_responses(epoch_data, channel_idx=31, fs=60):\n",
        "    \"\"\"\n",
        "    Visualize ERP responses and confirm P300 peaks around 300ms\n",
        "    Channel 31 = Cz (central midline electrode, best for P300)\n",
        "    \"\"\"\n",
        "    epochs = epoch_data['epochs']\n",
        "    labels = epoch_data['labels']\n",
        "\n",
        "    if np.all(labels == -1):\n",
        "        print(\"Test data provided (no labels). Cannot plot ERP comparison.\")\n",
        "        return\n",
        "\n",
        "    # Separate Target and Non-Target\n",
        "    targets = epochs[labels == 1][:, :, channel_idx]\n",
        "    non_targets = epochs[labels == 0][:, :, channel_idx]\n",
        "\n",
        "    # Plot averages with standard error\n",
        "    time_axis = np.linspace(0, 800, targets.shape[1])\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Target (P300)\n",
        "    mean_target = np.mean(targets, axis=0)\n",
        "    plt.plot(time_axis, mean_target, label='Target (P300)', color='blue', linewidth=2)\n",
        "\n",
        "    # Non-Target\n",
        "    mean_non_target = np.mean(non_targets, axis=0)\n",
        "    plt.plot(time_axis, mean_non_target, label='Non-Target', color='red', linestyle='--', linewidth=2)\n",
        "\n",
        "    # Mark P300 peak region\n",
        "    plt.axvspan(250, 450, color='gray', alpha=0.2, label='Expected P300 Window')\n",
        "\n",
        "    plt.title(f'ERP Response at Channel {channel_idx}')\n",
        "    plt.xlabel('Time (ms)')\n",
        "    plt.ylabel('Amplitude (uV)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate P300 amplitude difference\n",
        "    p300_window_idx = (time_axis >= 250) & (time_axis <= 450)\n",
        "    amp_diff = np.mean(mean_target[p300_window_idx]) - np.mean(mean_non_target[p300_window_idx])\n",
        "    print(f\"Mean Amplitude Difference in P300 Window: {amp_diff:.4f} uV\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Gj9JOgSfZ6Lu"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# SECTION 3: FEATURE ENGINEERING & BASELINE CLASSIFIERS\n",
        "################################################################################\n",
        "\n",
        "def extract_time_domain_features(epochs):\n",
        "    \"\"\"\n",
        "    Extract time-domain features: simply flatten the epochs\n",
        "    Shape: (n_epochs, n_samples * n_channels)\n",
        "    \"\"\"\n",
        "    n_epochs = epochs.shape[0]\n",
        "    return epochs.reshape(n_epochs, -1)\n",
        "\n",
        "def extract_pca_features(epochs, n_components=20):\n",
        "    \"\"\"\n",
        "    Extract PCA features for dimensionality reduction\n",
        "    Reduces (n_samples * n_channels) to n_components\n",
        "    \"\"\"\n",
        "    flat_data = extract_time_domain_features(epochs)\n",
        "    pca = PCA(n_components=n_components)\n",
        "    # Note: Fitting should ideally happen only on training data\n",
        "    # Here we return the object so it can be transformed later\n",
        "    return flat_data, pca\n",
        "\n",
        "def extract_csp_features(epochs, labels, n_components=6):\n",
        "    \"\"\"\n",
        "    Common Spatial Patterns (CSP) for discriminative spatial filters\n",
        "    Finds spatial filters that maximize variance ratio between classes\n",
        "    \"\"\"\n",
        "    target_epochs = epochs[labels == 1]\n",
        "    non_target_epochs = epochs[labels == 0]\n",
        "    print(f\"  Target epochs for CSP: {len(target_epochs)}\")\n",
        "    print(f\"  Non-target epochs for CSP: {len(non_target_epochs)}\")\n",
        "\n",
        "    # Compute covariance matrices\n",
        "    def compute_cov(data):\n",
        "        # data shape: (epochs, time, channels)\n",
        "        # covariance: (channels, channels)\n",
        "        covs = []\n",
        "        for i in range(data.shape[0]):\n",
        "            trial = data[i, :, :]\n",
        "            cov = np.dot(trial.T, trial) / trial.shape[0]\n",
        "            covs.append(cov)\n",
        "        return np.mean(covs, axis=0)\n",
        "\n",
        "    cov_target = compute_cov(target_epochs)\n",
        "    cov_nontarget = compute_cov(non_target_epochs)\n",
        "\n",
        "    # Solve generalized eigenvalue problem\n",
        "    # scipy.linalg.eigh(a, b) solves a*v = w*b*v\n",
        "    vals, vecs = eigh(cov_target, cov_target + cov_nontarget)\n",
        "\n",
        "    # Sort by eigenvalues\n",
        "    idx = np.argsort(vals)\n",
        "    vals = vals[idx]\n",
        "    vecs = vecs[:, idx]\n",
        "\n",
        "    # Select most discriminative components (extreme eigenvalues)\n",
        "    # Top n/2 and Bottom n/2\n",
        "    n_half = n_components // 2\n",
        "    filters = np.concatenate([vecs[:, :n_half], vecs[:, -n_half:]], axis=1)\n",
        "\n",
        "    # This matrix W will be used to project data\n",
        "    return filters.T\n",
        "\n",
        "def extract_features(epoch_data, method='pca', n_components=20):\n",
        "    \"\"\"\n",
        "    Feature extraction wrapper supporting multiple methods:\n",
        "    - time_domain: Raw time-domain samples (flattened)\n",
        "    - pca: Principal Component Analysis\n",
        "    - csp: Common Spatial Patterns\n",
        "    \"\"\"\n",
        "    epochs = epoch_data['epochs']\n",
        "    labels = epoch_data['labels']\n",
        "\n",
        "    if method == 'time':\n",
        "        return extract_time_domain_features(epochs), None\n",
        "\n",
        "    elif method == 'pca':\n",
        "        flat, pca = extract_pca_features(epochs, n_components)\n",
        "        features = pca.fit_transform(flat)\n",
        "        return features, pca\n",
        "\n",
        "    elif method == 'csp':\n",
        "        # CSP requires labels to calculate filters\n",
        "        if np.all(labels == -1):\n",
        "            raise ValueError(\"CSP requires labels for training\")\n",
        "        filters = extract_csp_features(epochs, labels, n_components)\n",
        "\n",
        "        # Apply CSP transform\n",
        "        # Project: Z = W * X\n",
        "        # Variance: log(var(Z))\n",
        "        features = []\n",
        "        for i in range(epochs.shape[0]):\n",
        "            # Transpose epoch to (Channels, Time)\n",
        "            trial = epochs[i].T\n",
        "            projected = np.dot(filters, trial)\n",
        "            # Log variance feature\n",
        "            var = np.var(projected, axis=1)\n",
        "            feat = np.log(var)\n",
        "            features.append(feat)\n",
        "\n",
        "        return np.array(features), filters\n",
        "\n",
        "    return None, None\n",
        "\n",
        "def train_lda_classifier(X_train, y_train):\n",
        "    \"\"\"\n",
        "    Linear Discriminant Analysis with balanced priors\n",
        "    \"\"\"\n",
        "    clf = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')\n",
        "    clf.fit(X_train, y_train)\n",
        "    return clf\n",
        "\n",
        "def train_logistic_regression(X_train, y_train):\n",
        "    \"\"\"\n",
        "    Logistic Regression - baseline classifier\n",
        "    \"\"\"\n",
        "    clf = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "    clf.fit(X_train, y_train)\n",
        "    return clf\n",
        "\n",
        "def evaluate_classifier(model, X_test, y_test, model_name=\"Model\"):\n",
        "    \"\"\"\n",
        "    Comprehensive classifier evaluation\n",
        "    \"\"\"\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"[{model_name}] Accuracy: {acc:.4f} | F1-Score: {f1:.4f}\")\n",
        "    return acc\n",
        "\n",
        "class CSPTransformer:\n",
        "    \"\"\"\n",
        "    Wrapper for CSP filters to enable transform() method\n",
        "    \"\"\"\n",
        "    def __init__(self, filters):\n",
        "        self.filters = filters\n",
        "\n",
        "    def transform(self, epochs):\n",
        "        features = []\n",
        "        for i in range(epochs.shape[0]):\n",
        "            trial = epochs[i].T\n",
        "            projected = np.dot(self.filters, trial)\n",
        "            var = np.var(projected, axis=1)\n",
        "            feat = np.log(var)\n",
        "            features.append(feat)\n",
        "        return np.array(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "UTnYtWcDZ-F6"
      },
      "outputs": [],
      "source": [
        "################################################################################\n",
        "# SECTION 4: CLASSICAL MACHINE LEARNING MODELS\n",
        "################################################################################\n",
        "\n",
        "def train_svm_classifier(X_train, y_train, kernel='rbf', C=1.0):\n",
        "    \"\"\"\n",
        "    Support Vector Machine with RBF kernel\n",
        "    Good for non-linear decision boundaries\n",
        "    \"\"\"\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "    clf = SVC(kernel=kernel, C=C, class_weight='balanced', probability=True)\n",
        "    clf.fit(X_scaled, y_train)\n",
        "\n",
        "    return clf, scaler\n",
        "\n",
        "def train_random_forest(X_train, y_train, n_estimators=100):\n",
        "    \"\"\"\n",
        "    Random Forest Classifier\n",
        "    Ensemble method, robust to overfitting\n",
        "    \"\"\"\n",
        "    clf = RandomForestClassifier(n_estimators=n_estimators, class_weight='balanced', random_state=42)\n",
        "    clf.fit(X_train, y_train)\n",
        "    return clf\n",
        "\n",
        "def train_gradient_boosting(X_train, y_train, n_estimators=100):\n",
        "    \"\"\"\n",
        "    Gradient Boosting Classifier with manual sample weighting\n",
        "    (GradientBoosting doesn't support class_weight parameter in older versions)\n",
        "    \"\"\"\n",
        "    print(f\"\\n  Training Gradient Boosting (n_estimators={n_estimators})...\")\n",
        "\n",
        "    # Calculate sample weights manually\n",
        "    # Weight = Total / (n_classes * Count)\n",
        "    n_samples = len(y_train)\n",
        "    n_classes = 2\n",
        "    count_0 = np.sum(y_train == 0)\n",
        "    count_1 = np.sum(y_train == 1)\n",
        "\n",
        "    w0 = n_samples / (n_classes * count_0)\n",
        "    w1 = n_samples / (n_classes * count_1)\n",
        "\n",
        "    weights = np.zeros(n_samples)\n",
        "    weights[y_train == 0] = w0\n",
        "    weights[y_train == 1] = w1\n",
        "\n",
        "    clf = GradientBoostingClassifier(n_estimators=n_estimators, random_state=42)\n",
        "    clf.fit(X_train, y_train, sample_weight=weights)\n",
        "    return clf\n",
        "\n",
        "def compare_all_classical_models(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Train and compare all classical ML models\n",
        "    Returns performance comparison\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    models = {}\n",
        "\n",
        "    # Define models to train\n",
        "    model_list = [\n",
        "        ('LDA', LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')),\n",
        "        ('Logistic Regression', LogisticRegression(class_weight='balanced', max_iter=1000)),\n",
        "        ('Random Forest', RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)),\n",
        "        ('SVM (RBF)', SVC(kernel='rbf', class_weight='balanced', probability=True))\n",
        "    ]\n",
        "\n",
        "    # Special handling for scaling (SVM needs it)\n",
        "    scaler = StandardScaler()\n",
        "    X_train_s = scaler.fit_transform(X_train)\n",
        "    X_test_s = scaler.transform(X_test)\n",
        "\n",
        "    for name, clf in model_list:\n",
        "        print(f\"Training {name}...\")\n",
        "\n",
        "        # Training\n",
        "        start_time = time.time()\n",
        "        if name == 'SVM (RBF)':\n",
        "            clf.fit(X_train_s, y_train)\n",
        "            curr_X_test = X_test_s\n",
        "        else:\n",
        "            clf.fit(X_train, y_train)\n",
        "            curr_X_test = X_test\n",
        "\n",
        "        # Inference\n",
        "        y_pred = clf.predict(curr_X_test)\n",
        "\n",
        "        # Metrics\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "        results[name] = {'Accuracy': acc, 'F1': f1}\n",
        "        models[name] = clf\n",
        "        print(f\"  -> Accuracy: {acc:.4f}, F1: {f1:.4f}, Time: {time.time()-start_time:.2f}s\")\n",
        "\n",
        "    # Gradient Boosting (Separate due to weighting logic)\n",
        "    print(\"Training Gradient Boosting...\")\n",
        "    gb_model = train_gradient_boosting(X_train, y_train)\n",
        "    y_pred_gb = gb_model.predict(X_test)\n",
        "    results['Gradient Boosting'] = {\n",
        "        'Accuracy': accuracy_score(y_test, y_pred_gb),\n",
        "        'F1': f1_score(y_test, y_pred_gb)\n",
        "    }\n",
        "    models['Gradient Boosting'] = gb_model\n",
        "\n",
        "    # Summary table\n",
        "    print(\"\\n--- Model Comparison Summary ---\")\n",
        "    print(f\"{'Model':<20} | {'Accuracy':<10} | {'F1-Score':<10}\")\n",
        "    print(\"-\" * 46)\n",
        "    for name, metrics in results.items():\n",
        "        print(f\"{name:<20} | {metrics['Accuracy']:.4f}     | {metrics['F1']:.4f}\")\n",
        "\n",
        "    return results, models\n",
        "\n",
        "def save_model(model, filepath):\n",
        "    \"\"\"Save model to pickle file\"\"\"\n",
        "    with open(filepath, 'wb') as f:\n",
        "        pickle.dump(model, f)\n",
        "    print(f\"\\n  Model saved to: {filepath}\")\n",
        "\n",
        "def load_model(filepath):\n",
        "    \"\"\"Load model from pickle file\"\"\"\n",
        "    with open(filepath, 'rb') as f:\n",
        "        model = pickle.load(f)\n",
        "    print(f\"\\n  Model loaded from: {filepath}\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================================================\n",
        "# MAIN EXECUTION\n",
        "# ========================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    # ========================================================================\n",
        "    # STEP 1: LOAD DATA\n",
        "    # ========================================================================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STEP 1: LOADING DATA\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    from google.colab import drive\n",
        "    try:\n",
        "        drive.mount('/content/drive')\n",
        "        DATA_PATH = '/content/drive/Othercomputers/My Mac/Downloads/BCI_Comp_III_Wads_2004/'\n",
        "    except:\n",
        "        # Fallback if not in Colab or drive mounting fails (for local testing)\n",
        "        print(\"Drive mount failed or skipped. Using local path './Dataset/'\")\n",
        "        DATA_PATH = './Dataset/'\n",
        "\n",
        "    # Ensure these files exist in the path, otherwise this will fail\n",
        "    try:\n",
        "        train_data_A = load_data('/content/drive/Othercomputers/My Mac/Downloads/BCI_Comp_III_Wads_2004/Subject_A_Train.mat')\n",
        "        test_data_A = load_data('/content/drive/Othercomputers/My Mac/Downloads/BCI_Comp_III_Wads_2004/Subject_A_Test.mat')\n",
        "        train_data_B = load_data('/content/drive/Othercomputers/My Mac/Downloads/BCI_Comp_III_Wads_2004/Subject_B_Train.mat')\n",
        "        test_data_B = load_data('/content/drive/Othercomputers/My Mac/Downloads/BCI_Comp_III_Wads_2004/Subject_B_Test.mat')\n",
        "        print(\"Data loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        # Stop execution if data load fails\n",
        "        exit()\n",
        "\n",
        "    # ========================================================================\n",
        "    # STEP 2: PREPROCESSING\n",
        "    # ========================================================================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STEP 2: PREPROCESSING\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    print(\"\\n--- Subject A ---\")\n",
        "    train_proc_A = preprocess_pipeline(train_data_A)\n",
        "    test_proc_A = preprocess_pipeline(test_data_A)\n",
        "\n",
        "    print(\"\\n--- Subject B ---\")\n",
        "    train_proc_B = preprocess_pipeline(train_data_B)\n",
        "    test_proc_B = preprocess_pipeline(test_data_B)\n",
        "\n",
        "    # ========================================================================\n",
        "    # STEP 3: EPOCH EXTRACTION\n",
        "    # ========================================================================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STEP 3: EPOCH EXTRACTION\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    print(\"\\n--- Subject A ---\")\n",
        "    train_epochs_A = extract_epochs(train_proc_A)\n",
        "    test_epochs_A = extract_epochs(test_proc_A)\n",
        "\n",
        "    print(\"\\n--- Subject B ---\")\n",
        "    train_epochs_B = extract_epochs(train_proc_B)\n",
        "    test_epochs_B = extract_epochs(test_proc_B)\n",
        "\n",
        "    # ========================================================================\n",
        "    # STEP 4: VISUALIZE ERP\n",
        "    # ========================================================================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STEP 4: VISUALIZING ERP RESPONSES\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    print(\"\\n--- Subject A ---\")\n",
        "    plot_erp_responses(train_epochs_A, channel_idx=31)\n",
        "\n",
        "    # ========================================================================\n",
        "    # STEP 5: FEATURE EXTRACTION\n",
        "    # ========================================================================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STEP 5: FEATURE EXTRACTION\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # ========================================================================\n",
        "    # Subject A: Compare PCA vs CSP vs Time-Domain\n",
        "    # ========================================================================\n",
        "    print(\"\\n--- Subject A: Feature Comparison ---\")\n",
        "\n",
        "    # Split A for validation within training set to compare features\n",
        "    X_full = train_epochs_A['epochs']\n",
        "    y_full = train_epochs_A['labels']\n",
        "\n",
        "    # Use a small subset to tune feature method\n",
        "    X_tr_sub, X_val_sub, y_tr_sub, y_val_sub = train_test_split(\n",
        "        X_full, y_full, test_size=0.2, stratify=y_full, random_state=42\n",
        "    )\n",
        "\n",
        "    epoch_data_tr = {'epochs': X_tr_sub, 'labels': y_tr_sub}\n",
        "    epoch_data_val = {'epochs': X_val_sub, 'labels': y_val_sub}\n",
        "\n",
        "    # Try PCA (20 components)\n",
        "    print(\"Extracting PCA-20...\")\n",
        "    X_pca20_tr, pca20 = extract_features(epoch_data_tr, method='pca', n_components=20)\n",
        "    X_pca20_val = pca20.transform(X_val_sub.reshape(len(X_val_sub), -1))\n",
        "\n",
        "    # Try PCA (50 components)\n",
        "    print(\"Extracting PCA-50...\")\n",
        "    X_pca50_tr, pca50 = extract_features(epoch_data_tr, method='pca', n_components=50)\n",
        "    X_pca50_val = pca50.transform(X_val_sub.reshape(len(X_val_sub), -1))\n",
        "\n",
        "    # Try CSP\n",
        "    print(\"Extracting CSP...\")\n",
        "    X_csp_tr, csp_filters = extract_features(epoch_data_tr, method='csp', n_components=6)\n",
        "    csp_obj = CSPTransformer(csp_filters)\n",
        "    X_csp_val = csp_obj.transform(X_val_sub)\n",
        "\n",
        "    # Try Raw Time-Domain Features\n",
        "    print(\"Extracting Time-Domain...\")\n",
        "    X_time_tr, _ = extract_features(epoch_data_tr, method='time')\n",
        "    X_time_val, _ = extract_features(epoch_data_val, method='time')\n",
        "\n",
        "    # Quick comparison with BALANCED LDA\n",
        "    print(\"\\nEvaluating Feature Sets with LDA...\")\n",
        "\n",
        "    # PCA-20 test\n",
        "    lda_pca20 = train_lda_classifier(X_pca20_tr, y_tr_sub)\n",
        "    y_pred_pca20 = lda_pca20.predict(X_pca20_val)\n",
        "    acc_pca20 = accuracy_score(y_val_sub, y_pred_pca20)\n",
        "    f1_pca20 = f1_score(y_val_sub, y_pred_pca20)\n",
        "\n",
        "    # PCA-50 test\n",
        "    lda_pca50 = train_lda_classifier(X_pca50_tr, y_tr_sub)\n",
        "    y_pred_pca50 = lda_pca50.predict(X_pca50_val)\n",
        "    acc_pca50 = accuracy_score(y_val_sub, y_pred_pca50)\n",
        "    f1_pca50 = f1_score(y_val_sub, y_pred_pca50)\n",
        "\n",
        "    # CSP test\n",
        "    lda_csp = train_lda_classifier(X_csp_tr, y_tr_sub)\n",
        "    y_pred_csp = lda_csp.predict(X_csp_val)\n",
        "    acc_csp = accuracy_score(y_val_sub, y_pred_csp)\n",
        "    f1_csp = f1_score(y_val_sub, y_pred_csp)\n",
        "\n",
        "    # Time-Domain test\n",
        "    lda_time = train_lda_classifier(X_time_tr, y_tr_sub)\n",
        "    y_pred_time = lda_time.predict(X_time_val)\n",
        "    acc_time = accuracy_score(y_val_sub, y_pred_time)\n",
        "    f1_time = f1_score(y_val_sub, y_pred_time)\n",
        "\n",
        "    # ========================================================================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"FEATURE COMPARISON (Balanced Classifiers)\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"PCA (20 comp):      Accuracy={acc_pca20:.4f}, F1={f1_pca20:.4f}\")\n",
        "    print(f\"PCA (50 comp):      Accuracy={acc_pca50:.4f}, F1={f1_pca50:.4f}\")\n",
        "    print(f\"CSP (6 comp):       Accuracy={acc_csp:.4f}, F1={f1_csp:.4f}\")\n",
        "    print(f\"Time-Domain:        Accuracy={acc_time:.4f}, F1={f1_time:.4f}\")\n",
        "\n",
        "    # Select best method based on F1-score\n",
        "    scores = {'pca': f1_pca20, 'csp': f1_csp, 'time': f1_time}\n",
        "    feature_method_A = max(scores, key=scores.get)\n",
        "    print(f\"\\nSelected Feature Method: {feature_method_A.upper()}\")\n",
        "\n",
        "    # Re-extract chosen features on FULL training set\n",
        "    print(\"Re-extracting features for full dataset...\")\n",
        "    n_components_A = 20 # Default\n",
        "    feature_obj_A = None # To store PCA or CSP object\n",
        "\n",
        "    if feature_method_A == 'time':\n",
        "        X_train_full_A, _ = extract_features(train_epochs_A, method='time')\n",
        "    elif feature_method_A == 'pca':\n",
        "        X_train_full_A, feature_obj_A = extract_features(train_epochs_A, method='pca', n_components=20)\n",
        "        pca_A = feature_obj_A # save reference\n",
        "    else: # CSP\n",
        "        X_train_full_A, feature_obj_A = extract_features(train_epochs_A, method='csp', n_components=6)\n",
        "\n",
        "    # ========================================================================\n",
        "    # Subject A: Create final train/val split for later steps\n",
        "    # ========================================================================\n",
        "    X_train_A, X_val_A, y_train_A, y_val_A = train_test_split(\n",
        "        X_train_full_A, train_epochs_A['labels'], test_size=0.2, random_state=42, stratify=train_epochs_A['labels']\n",
        "    )\n",
        "    print(f\"\\nSubject A splits: Training={len(X_train_A)}, Validation={len(X_val_A)}\")\n",
        "\n",
        "    # Transform test data\n",
        "    if feature_method_A == 'time':\n",
        "        X_test_A = test_epochs_A['epochs'].reshape(len(test_epochs_A['epochs']), -1)\n",
        "    elif feature_method_A == 'pca':\n",
        "        X_test_A = feature_obj_A.transform(test_epochs_A['epochs'].reshape(test_epochs_A['epochs'].shape[0], -1))\n",
        "    else:  # CSP\n",
        "        X_test_A = CSPTransformer(feature_obj_A).transform(test_epochs_A['epochs'])\n",
        "\n",
        "    print(f\"Test features: {X_test_A.shape}\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # Subject B: Use same method as Subject A\n",
        "    # ========================================================================\n",
        "    print(\"\\n--- Subject B: Feature Extraction ---\")\n",
        "    print(f\"\\nUsing {feature_method_A.upper()} (same as Subject A)...\")\n",
        "\n",
        "    # Define PCA_B variable for saving later\n",
        "    pca_B = None\n",
        "\n",
        "    if feature_method_A == 'time':\n",
        "        X_train_full_B, _ = extract_features(train_epochs_B, method='time')\n",
        "        X_test_B = test_epochs_B['epochs'].reshape(len(test_epochs_B['epochs']), -1)\n",
        "    elif feature_method_A == 'pca':\n",
        "        X_train_full_B, feature_obj_B = extract_features(train_epochs_B, method='pca', n_components=20)\n",
        "        X_test_B = feature_obj_B.transform(test_epochs_B['epochs'].reshape(test_epochs_B['epochs'].shape[0], -1))\n",
        "        pca_B = feature_obj_B\n",
        "    else:  # CSP\n",
        "        X_train_full_B, feature_obj_B = extract_features(train_epochs_B, method='csp', n_components=6)\n",
        "        X_test_B = CSPTransformer(feature_obj_B).transform(test_epochs_B['epochs'])\n",
        "\n",
        "    X_train_B, X_val_B, y_train_B, y_val_B = train_test_split(\n",
        "        X_train_full_B, train_epochs_B['labels'], test_size=0.2, random_state=42, stratify=train_epochs_B['labels']\n",
        "    )\n",
        "    print(f\"Subject B splits: Training={len(X_train_B)}, Validation={len(X_val_B)}, Test features: {X_test_B.shape}\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # STEP 6: BASELINE CLASSIFIERS\n",
        "    # ========================================================================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STEP 6: BASELINE CLASSIFIERS (Subject A)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    lda_A = train_lda_classifier(X_train_A, y_train_A)\n",
        "    acc_lda = evaluate_classifier(lda_A, X_val_A, y_val_A, \"LDA\")\n",
        "\n",
        "    lr_A = train_logistic_regression(X_train_A, y_train_A)\n",
        "    acc_lr = evaluate_classifier(lr_A, X_val_A, y_val_A, \"Logistic Regression\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # STEP 7: CLASSICAL ML MODELS\n",
        "    # ========================================================================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STEP 7: CLASSICAL MACHINE LEARNING (Subject A)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    results_classical_A, models_A = compare_all_classical_models(\n",
        "        X_train_A, y_train_A, X_val_A, y_val_A\n",
        "    )\n",
        "\n",
        "    # Train SVM for both subjects (best model assumption for final export)\n",
        "    # Re-train on full train data? Usually yes, but here we just use the split for simplicity\n",
        "    print(\"\\nTraining Final SVMs...\")\n",
        "    svm_A, scaler_A = train_svm_classifier(X_train_A, y_train_A)\n",
        "    svm_B, scaler_B = train_svm_classifier(X_train_B, y_train_B)\n",
        "\n",
        "    # ========================================================================\n",
        "    # STEP 8: EXPORT MODELS\n",
        "    # ========================================================================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STEP 8: EXPORTING MODELS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    import os\n",
        "    os.makedirs('models', exist_ok=True)\n",
        "\n",
        "    # Save pickle\n",
        "    # Note: feature_obj_A might be None if 'time' method was selected,\n",
        "    # but we save what we have. If PCA was used, we save it.\n",
        "    save_model({\n",
        "        'model': svm_A,\n",
        "        'scaler': scaler_A,\n",
        "        'pca': feature_obj_A if feature_method_A == 'pca' else None\n",
        "    }, 'models/subject_A_svm.pkl')\n",
        "\n",
        "    save_model({\n",
        "        'model': svm_B,\n",
        "        'scaler': scaler_B,\n",
        "        'pca': pca_B if feature_method_A == 'pca' else None\n",
        "    }, 'models/subject_B_svm.pkl')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "usPx61mQX1Jq",
        "outputId": "22d22932-5828-43f5-c4b6-f43ab5484b82"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "STEP 1: LOADING DATA\n",
            "======================================================================\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Data loaded successfully.\n",
            "\n",
            "======================================================================\n",
            "STEP 2: PREPROCESSING\n",
            "======================================================================\n",
            "\n",
            "--- Subject A ---\n",
            "Warning: Unexpected signal dimension 3. Expected 2D (Time, Channels) or 3D (1, Time, Channels). Proceeding assuming (Time, Channels).\n",
            "Warning: Unexpected signal dimension 3. Expected 2D (Time, Channels) or 3D (1, Time, Channels). Proceeding assuming (Time, Channels).\n",
            "\n",
            "--- Subject B ---\n",
            "Warning: Unexpected signal dimension 3. Expected 2D (Time, Channels) or 3D (1, Time, Channels). Proceeding assuming (Time, Channels).\n",
            "Warning: Unexpected signal dimension 3. Expected 2D (Time, Channels) or 3D (1, Time, Channels). Proceeding assuming (Time, Channels).\n",
            "\n",
            "======================================================================\n",
            "STEP 3: EPOCH EXTRACTION\n",
            "======================================================================\n",
            "\n",
            "--- Subject A ---\n",
            "\n",
            "--- Subject B ---\n",
            "\n",
            "======================================================================\n",
            "STEP 4: VISUALIZING ERP RESPONSES\n",
            "======================================================================\n",
            "\n",
            "--- Subject A ---\n",
            "Test data provided (no labels). Cannot plot ERP comparison.\n",
            "\n",
            "======================================================================\n",
            "STEP 5: FEATURE EXTRACTION\n",
            "======================================================================\n",
            "\n",
            "--- Subject A: Feature Comparison ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1623021160.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;31m# Use a small subset to tune feature method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     X_tr_sub, X_val_sub, y_tr_sub, y_val_sub = train_test_split(\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mX_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2850\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2851\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2852\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2853\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2481\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2482\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2483\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}