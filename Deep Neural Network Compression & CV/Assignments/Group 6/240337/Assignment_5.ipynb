{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d7d55cb",
   "metadata": {},
   "source": [
    "\n",
    "# Assignment: Linear Regression, Logistic Regression, and K-Means (From Scratch)\n",
    "\n",
    "**Instructions**\n",
    "- You are NOT allowed to use `scikit-learn` for model implementation, scaling.\n",
    "- You may use it for implementation of clustering\n",
    "- You may use: `numpy`, `matplotlib`, and standard Python libraries only.\n",
    "- Every step (scaling, loss, gradients, optimization) must be implemented manually.\n",
    "- Clearly comment your code and explain your reasoning in Markdown cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ff7cd5",
   "metadata": {},
   "source": [
    "\n",
    "## Question 1: Linear Regression from Scratch (with Standardization and Regularization)\n",
    "\n",
    "You are given a dataset `(X, y)`.\n",
    "\n",
    "### Tasks\n",
    "1. Implement **StandardScaler manually**:\n",
    "   - Compute mean and standard deviation for each feature.\n",
    "   - Standardize the features.\n",
    "2. Implement **Linear Regression using Gradient Descent**.\n",
    "3. Add **L2 Regularization (Ridge Regression)**.\n",
    "4. Plot:\n",
    "   - Loss vs iterations\n",
    "   - True vs predicted values\n",
    "\n",
    "Do NOT use `sklearn`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaf5e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d67bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. DATA GENERATION \n",
    "\n",
    "\n",
    "def generate_regression_data(n_samples=100, n_features=1, noise=10):\n",
    "    \"\"\"Generates random data for regression: y = wX + b + noise\"\"\"\n",
    "    np.random.seed(42)\n",
    "    X = 2 * np.random.rand(n_samples, n_features)\n",
    "    true_weights = np.random.randn(n_features) * 10\n",
    "    bias = 5\n",
    "    # y = Xw + b + noise\n",
    "    y = X.dot(true_weights) + bias + np.random.randn(n_samples) * noise\n",
    "    return X, y\n",
    "\n",
    "def generate_classification_data(n_samples=200):\n",
    "    \"\"\"Generates two blobs of data for binary classification\"\"\"\n",
    "    np.random.seed(42)\n",
    "    # Class 0: Centered at (2, 2)\n",
    "    X0 = np.random.randn(n_samples // 2, 2) + 2\n",
    "    y0 = np.zeros(n_samples // 2)\n",
    "    # Class 1: Centered at (6, 6)\n",
    "    X1 = np.random.randn(n_samples // 2, 2) + 6\n",
    "    y1 = np.ones(n_samples // 2)\n",
    "    \n",
    "    X = np.vstack((X0, X1))\n",
    "    y = np.hstack((y0, y1))\n",
    "    \n",
    "    # Shuffle\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    return X[indices], y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cf71cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.  ALGORITHMS\n",
    "\n",
    "# Implement StandardScaler manually ,  first read about it, how it works and then implement it \n",
    "class StandardScalerManual:\n",
    "    def fit(self, X):\n",
    "        self.mean_ = np.mean(X, axis=0)\n",
    "        self.std_ = np.std(X, axis=0)\n",
    "        self.std_[self.std_ == 0] = 1.0\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return (X - self.mean_) / self.std_\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5b360ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Implement Linear Regression from scratch, here you have to also construct the regulization term coefficient of which will be\n",
    "# denoted by l2_lambda \n",
    "# try to implement L1 regularization or atlease read about it and where it is used\n",
    "class LinearRegressionManual:\n",
    "    def _init_(self, lr=0.01, epochs=1000, l2_lambda=0.0):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.l2_lambda = l2_lambda\n",
    "        self.loss_history = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        for _ in range(self.epochs):\n",
    "            y_pred = np.dot(X, self.weights) + self.bias\n",
    "            \n",
    "            # MSE Loss + L2 Penalty\n",
    "            loss = (1/(2*n_samples)) * np.sum((y - y_pred)**2) + \\\n",
    "                   (self.l2_lambda/(2*n_samples)) * np.sum(self.weights**2)\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "            # Gradients\n",
    "            dw = (1/n_samples) * np.dot(X.T, (y_pred - y)) + (self.l2_lambda/n_samples) * self.weights\n",
    "            db = (1/n_samples) * np.sum(y_pred - y)\n",
    "            \n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5b8470",
   "metadata": {},
   "source": [
    "\n",
    "## Question 2: Logistic Regression from Scratch (with Standardization and Regularization)\n",
    "\n",
    "You are given a binary classification dataset.\n",
    "\n",
    "### Tasks\n",
    "1. Reuse your **manual StandardScaler**.\n",
    "2. Implement **Logistic Regression using Gradient Descent**.\n",
    "3. Use:\n",
    "   - Sigmoid function\n",
    "   - Binary Cross Entropy loss\n",
    "4. Add **L2 Regularization**.\n",
    "5. Report:\n",
    "   - Training loss curve\n",
    "   - Final accuracy\n",
    "\n",
    "Do NOT use `sklearn`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4b6002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Implement sigmoid function as told in the lectures \n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f344717c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Implement Logistic Regression from scratch and here also add the regularizaation term \n",
    "class LogisticRegressionManual:\n",
    "    def _init_(self, lr=0.01, epochs=1000, l2_lambda=0.0):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.l2_lambda = l2_lambda\n",
    "        self.loss_history = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        for _ in range(self.epochs):\n",
    "            linear = np.dot(X, self.weights) + self.bias\n",
    "            y_pred = sigmoid(linear)\n",
    "            y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "            \n",
    "            # BCE Loss + L2 Penalty\n",
    "            loss = -(1/n_samples) * np.sum(y*np.log(y_pred) + (1-y)*np.log(1-y_pred)) + \\\n",
    "                   (self.l2_lambda/(2*n_samples)) * np.sum(self.weights**2)\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "            dw = (1/n_samples) * np.dot(X.T, (y_pred - y)) + (self.l2_lambda/n_samples) * self.weights\n",
    "            db = (1/n_samples) * np.sum(y_pred - y)\n",
    "            \n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return sigmoid(np.dot(X, self.weights) + self.bias)\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return (self.predict_proba(X) > threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d398a9ee",
   "metadata": {},
   "source": [
    "\n",
    "## Question 3: K-Means Clustering from Scratch (Matrix Clustering)\n",
    "\n",
    "You are given a **random matrix** `M` of shape `(n, m)`.\n",
    "\n",
    "### Tasks\n",
    "Implement K-Means clustering **from scratch** such that:\n",
    "\n",
    "1. Input:\n",
    "   - A random matrix `M`\n",
    "   - Number of clusters `k`\n",
    "2. Output:\n",
    "   - `assignment_table`: a matrix of same shape as `M`, where each element stores the **cluster label**\n",
    "   - `cookbook`: a dictionary (hashmap) where:\n",
    "     - Key = cluster index\n",
    "     - Value = list of **positions (i, j)** belonging to that cluster\n",
    "   - `centroids`: array storing centroid values\n",
    "\n",
    "You must cluster **individual elements**, not rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de1b3b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Implement K-Means for matrix elements\n",
    "#CAN USE SK-LEARN FOR THIS TASK AS THIS TASK WILL HELP US DIRECTLY IN OUR PROJECT ! \n",
    "def kmeans_matrix_manual(M, k, max_iters=100):\n",
    "    '''\n",
    "    Returns:\n",
    "    assignment_table: same shape as M, contains cluster labels\n",
    "    cookbook: dict -> cluster_id : list of (i, j) positions\n",
    "    centroids: numpy array of centroid values\n",
    "    '''\n",
    "   \n",
    "    n, m = M.shape\n",
    "    data = M.flatten()\n",
    "    \n",
    "    # 1. Initialize Centroids randomly from the data\n",
    "    np.random.seed(42)\n",
    "    centroids = np.random.choice(data, k, replace=False).astype(float)\n",
    "    \n",
    "    labels = np.zeros(data.shape, dtype=int)\n",
    "    \n",
    "    for _ in range(max_iters):\n",
    "        # 2. Assignment Step\n",
    "        # Calculate distance from each point to each centroid\n",
    "        # distances shape: (n_points, k)\n",
    "        distances = np.abs(data[:, np.newaxis] - centroids)\n",
    "        new_labels = np.argmin(distances, axis=1)\n",
    "        \n",
    "        # Check convergence\n",
    "        if np.all(labels == new_labels):\n",
    "            break\n",
    "        labels = new_labels\n",
    "        \n",
    "        # 3. Update Step\n",
    "        for i in range(k):\n",
    "            points_in_cluster = data[labels == i]\n",
    "            if len(points_in_cluster) > 0:\n",
    "                centroids[i] = np.mean(points_in_cluster)\n",
    "                \n",
    "    # Reshape results\n",
    "    assignment_table = labels.reshape(n, m)\n",
    "    cookbook = {i: [] for i in range(k)}\n",
    "    for r in range(n):\n",
    "        for c in range(m):\n",
    "            cookbook[assignment_table[r, c]].append((r, c))\n",
    "            \n",
    "    return assignment_table, cookbook, centroids\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edd4e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. EXECUTION\n",
    "\n",
    "\n",
    "print(\" 1. Linear Regression \")\n",
    "X_reg, y_reg = generate_regression_data()\n",
    "scaler_reg = StandardScalerManual()\n",
    "X_reg_scaled = scaler_reg.fit_transform(X_reg)\n",
    "\n",
    "lin_reg = LinearRegressionManual(lr=0.1, epochs=500)\n",
    "lin_reg.fit(X_reg_scaled, y_reg)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1); plt.plot(lin_reg.loss_history); plt.title(\"LinReg Loss\")\n",
    "plt.subplot(1, 2, 2); plt.scatter(X_reg, y_reg); plt.plot(X_reg, lin_reg.predict(X_reg_scaled), 'r'); plt.title(\"LinReg Fit\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n 2. Logistic Regression \")\n",
    "X_cls, y_cls = generate_classification_data()\n",
    "scaler_cls = StandardScalerManual()\n",
    "X_cls_scaled = scaler_cls.fit_transform(X_cls)\n",
    "\n",
    "log_reg = LogisticRegressionManual(lr=0.1, epochs=1000)\n",
    "log_reg.fit(X_cls_scaled, y_cls)\n",
    "acc = np.mean(log_reg.predict(X_cls_scaled) == y_cls)\n",
    "print(f\"Accuracy: {acc*100:.2f}%\")\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(log_reg.loss_history); plt.title(\"LogReg Loss\"); plt.show()\n",
    "\n",
    "print(\"\\n 3. K-Means\")\n",
    "M_matrix = np.random.randint(0, 255, (10, 10))\n",
    "assign, cook, cents = kmeans_matrix_manual(M_matrix, k=3)\n",
    "print(\"Centroids:\", np.round(cents, 2))\n",
    "print(\"Assignments (5x5):\\n\", assign[:5, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceca75",
   "metadata": {},
   "source": [
    "\n",
    "## Submission Guidelines\n",
    "- Submit the completed `.ipynb` file.\n",
    "- Clearly label all plots and outputs.\n",
    "- Code readability and correctness matter.\n",
    "- Partial credit will be given for logically correct implementations.\n",
    "\n",
    "**Bonus**\n",
    "- Compare convergence with and without standardization.\n",
    "- Try different values of regularization strength.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
