{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3d7d55cb",
      "metadata": {
        "id": "3d7d55cb"
      },
      "source": [
        "\n",
        "# Assignment: Linear Regression, Logistic Regression, and K-Means (From Scratch)\n",
        "\n",
        "**Instructions**\n",
        "- You are NOT allowed to use `scikit-learn` for model implementation, scaling.\n",
        "- You may use it for implementation of clustering\n",
        "- You may use: `numpy`, `matplotlib`, and standard Python libraries only.\n",
        "- Every step (scaling, loss, gradients, optimization) must be implemented manually.\n",
        "- Clearly comment your code and explain your reasoning in Markdown cells."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3ff7cd5",
      "metadata": {
        "id": "d3ff7cd5"
      },
      "source": [
        "\n",
        "## Question 1: Linear Regression from Scratch (with Standardization and Regularization)\n",
        "\n",
        "You are given a dataset `(X, y)`.\n",
        "\n",
        "### Tasks\n",
        "1. Implement **StandardScaler manually**:\n",
        "   - Compute mean and standard deviation for each feature.\n",
        "   - Standardize the features.\n",
        "2. Implement **Linear Regression using Gradient Descent**.\n",
        "3. Add **L2 Regularization (Ridge Regression)**.\n",
        "4. Plot:\n",
        "   - Loss vs iterations\n",
        "   - True vs predicted values\n",
        "\n",
        "Do NOT use `sklearn`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ecaf5e36",
      "metadata": {
        "id": "ecaf5e36"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "48cf71cd",
      "metadata": {
        "id": "48cf71cd"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Implement StandardScaler manually ,  first read about it, how it works and then implement it\n",
        "class StandardScalerManual:\n",
        "    def __init__(self):\n",
        "        self.mean=None\n",
        "        self.std=None\n",
        "    def fit(self, X):\n",
        "        self.mean=np.mean(X,axis=0)\n",
        "        self.std=np.std(X,axis=0)\n",
        "        self.std[self.std==0]=1.0\n",
        "        return self\n",
        "        pass\n",
        "\n",
        "    def transform(self, X):\n",
        "        return (X - self.mean_) / self.std_\n",
        "        pass\n",
        "\n",
        "    def fit_transform(self, X):\n",
        "        self.fit(X)\n",
        "        return self.transform(X)\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "a5b360ce",
      "metadata": {
        "id": "a5b360ce"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Implement Linear Regression from scratch, here you have to also construct the regulization term coefficient of which will be\n",
        "# denoted by l2_lambda\n",
        "# try to implement L1 regularization or atlease read about it and where it is used\n",
        "class LinearRegressionManual:\n",
        "      def __init__(self, lr=0.01, epochs=1000, l2_lambda=0.0):\n",
        "        self.lr=lr\n",
        "        self.epochs=epochs\n",
        "        self.l2_lambda=l2_lambda\n",
        "        self.w=None\n",
        "        self.b=None\n",
        "        pass\n",
        "\n",
        "      def fit(self, X, y):\n",
        "        n,m=X.shape\n",
        "        y=y.reshape(-1,1)\n",
        "        self.w=np.zeros((m,1),dtype=np.float32)\n",
        "        self.b=0.00\n",
        "        msearr=np.zeros(self.epochs)\n",
        "        for epoch in range(self.epochs):\n",
        "          ypred=np.dot(X,self.w)+self.b\n",
        "          mse=np.mean((ypred-y)**2)+self.l2_lambda*np.sum(self.w**2)\n",
        "          dw=2*np.dot(X.T,ypred-y)/n+2*self.l2_lambda*self.w\n",
        "          db=2*np.sum((ypred-y))/n\n",
        "          self.w-=self.lr*dw\n",
        "          self.b-=self.lr*db\n",
        "        plt.plot(range(self.epochs),msearr,color='green',label=\"Loss vs iterations\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "        plt.plot(y,ypred,color=\"pink\",label=\"True vs predicted values\")\n",
        "        plt.show()\n",
        "        plt.scatter(X,y,color='blue',label=\"Actual data\")\n",
        "        plt.plot(X,ypred,color=\"red\",label=\"Predicted line\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "        pass\n",
        "\n",
        "\n",
        "      def predict(self, X):\n",
        "        return np.dot(X,self.w)+self.b\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a5b8470",
      "metadata": {
        "id": "3a5b8470"
      },
      "source": [
        "\n",
        "## Question 2: Logistic Regression from Scratch (with Standardization and Regularization)\n",
        "\n",
        "You are given a binary classification dataset.\n",
        "\n",
        "### Tasks\n",
        "1. Reuse your **manual StandardScaler**.\n",
        "2. Implement **Logistic Regression using Gradient Descent**.\n",
        "3. Use:\n",
        "   - Sigmoid function\n",
        "   - Binary Cross Entropy loss\n",
        "4. Add **L2 Regularization**.\n",
        "5. Report:\n",
        "   - Training loss curve\n",
        "   - Final accuracy\n",
        "\n",
        "Do NOT use `sklearn`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "c4b6002a",
      "metadata": {
        "id": "c4b6002a"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Implement sigmoid function as told in the lectures\n",
        "def sigmoid(z):\n",
        "    res=1/(1+np.exp(-z))\n",
        "    return res\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "f344717c",
      "metadata": {
        "id": "f344717c"
      },
      "outputs": [],
      "source": [
        "from logging import log\n",
        "\n",
        "#Implement Logistic Regression from scratch and here also add the regularizaation term\n",
        "class LogisticRegressionManual:\n",
        "    def __init__(self, lr=0.01, epochs=1000, l2_lambda=0.0):\n",
        "        self.lr=lr\n",
        "        self.epochs=epochs\n",
        "        self.l2_lambda=l2_lambda\n",
        "        self.w=None\n",
        "        self.b=None\n",
        "        self.y=None\n",
        "        pass\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n,m=X.shape\n",
        "        self.y=y.reshape(-1,1)\n",
        "        self.w=np.zeros((m,1),dtype=np.float32)\n",
        "        self.b=0.00\n",
        "        loss=np.zeros(self.epochs)\n",
        "        for epoch in range(self.epochs):\n",
        "            z=np.dot(X,self.w)+self.b\n",
        "            res=sigmoid(z)\n",
        "            loss[epoch]=-np.mean(y*np.log(res+1e-9)+(1-y)*np.log(1-res+1e-9))+self.l2_lambda*np.sum(self.w**2)\n",
        "            dw=1*np.dot(X.T,res-y)/n+2*self.l2_lambda*self.w\n",
        "            db=1*np.sum((res-y))/n\n",
        "            self.w-=self.lr*dw\n",
        "            self.b-=self.lr*db\n",
        "        plt.plot(range(self.epochs),loss,color='green',label=\"Loss vs iterations\")\n",
        "        plt.show()\n",
        "        pass\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        z=np.dot(X,self.w)+self.b\n",
        "        res2=sigmoid(z)\n",
        "        pass\n",
        "\n",
        "    def predict(self, X):\n",
        "        probab=self.predict_proba(X)   # sigmoid outputs\n",
        "        preds=np.zeros_like(probab)\n",
        "        for i in range(len(probab)):\n",
        "            if probab[i]>=0.5:\n",
        "                preds[i]=1\n",
        "            else:\n",
        "                preds[i]=0\n",
        "\n",
        "\n",
        "        return preds,np.mean(preds==self.y)*100\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d398a9ee",
      "metadata": {
        "id": "d398a9ee"
      },
      "source": [
        "\n",
        "## Question 3: K-Means Clustering from Scratch (Matrix Clustering)\n",
        "\n",
        "You are given a **random matrix** `M` of shape `(n, m)`.\n",
        "\n",
        "### Tasks\n",
        "Implement K-Means clustering **from scratch** such that:\n",
        "\n",
        "1. Input:\n",
        "   - A random matrix `M`\n",
        "   - Number of clusters `k`\n",
        "2. Output:\n",
        "   - `assignment_table`: a matrix of same shape as `M`, where each element stores the **cluster label**\n",
        "   - `cookbook`: a dictionary (hashmap) where:\n",
        "     - Key = cluster index\n",
        "     - Value = list of **positions (i, j)** belonging to that cluster\n",
        "   - `centroids`: array storing centroid values\n",
        "\n",
        "You must cluster **individual elements**, not rows.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "de1b3b21",
      "metadata": {
        "id": "de1b3b21"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Implement K-Means for matrix elements\n",
        "#CAN USE SK-LEARN FOR THIS TASK AS THIS TASK WILL HELP US DIRECTLY IN OUR PROJECT !\n",
        "def kmeans_matrix(M, k, max_iters=100):\n",
        "    '''\n",
        "    Returns:\n",
        "    assignment_table: same shape as M, contains cluster labels\n",
        "    cookbook: dict -> cluster_id : list of (i, j) positions\n",
        "    centroids: numpy array of centroid values\n",
        "    '''\n",
        "    n,m=M.shape\n",
        "    assignment_table=np.zeros((n,m),dtype=int)\n",
        "    centroids=np.random.choice(M.flatten(),size=k,replace=False)\n",
        "    centroids1=np.zeros((k))\n",
        "    for iter in range(max_iters):\n",
        "        for i in range(n):\n",
        "          for j in range(m):\n",
        "            distance=np.abs(centroids-M[i,j])\n",
        "            assignment_table[i,j]=np.argmin(distance)\n",
        "        for r in range(k):\n",
        "          values=M[assignment_table==r]\n",
        "          if len(values)>0:\n",
        "                centroids1[r]=np.mean(values)\n",
        "          else:\n",
        "                centroids1[r]=np.random.choice(M.flatten())\n",
        "        centroids = centroids1.copy()\n",
        "    cookbook={c: [] for c in range(k)}\n",
        "    for i in range(n):\n",
        "            for j in range(m):\n",
        "                c=assignment_table[i, j]\n",
        "                cookbook[c].append((i, j))\n",
        "    return assignment_table,cookbook,centroids"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acceca75",
      "metadata": {
        "id": "acceca75"
      },
      "source": [
        "\n",
        "## Submission Guidelines\n",
        "- Submit the completed `.ipynb` file.\n",
        "- Clearly label all plots and outputs.\n",
        "- Code readability and correctness matter.\n",
        "- Partial credit will be given for logically correct implementations.\n",
        "\n",
        "**Bonus**\n",
        "- Compare convergence with and without standardization.\n",
        "- Try different values of regularization strength.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}