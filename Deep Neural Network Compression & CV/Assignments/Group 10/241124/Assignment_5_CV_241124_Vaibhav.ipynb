{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d7d55cb",
   "metadata": {},
   "source": [
    "\n",
    "# Assignment: Linear Regression, Logistic Regression, and K-Means (From Scratch)\n",
    "\n",
    "**Instructions**\n",
    "- You are NOT allowed to use `scikit-learn` for model implementation, scaling.\n",
    "- You may use it for implementation of clustering\n",
    "- You may use: `numpy`, `matplotlib`, and standard Python libraries only.\n",
    "- Every step (scaling, loss, gradients, optimization) must be implemented manually.\n",
    "- Clearly comment your code and explain your reasoning in Markdown cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ff7cd5",
   "metadata": {},
   "source": [
    "\n",
    "## Question 1: Linear Regression from Scratch (with Standardization and Regularization)\n",
    "\n",
    "You are given a dataset `(X, y)`.\n",
    "\n",
    "### Tasks\n",
    "1. Implement **StandardScaler manually**:\n",
    "   - Compute mean and standard deviation for each feature.\n",
    "   - Standardize the features.\n",
    "2. Implement **Linear Regression using Gradient Descent**.\n",
    "3. Add **L2 Regularization (Ridge Regression)**.\n",
    "4. Plot:\n",
    "   - Loss vs iterations\n",
    "   - True vs predicted values\n",
    "\n",
    "Do NOT use `sklearn`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecaf5e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48cf71cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Implement StandardScaler manually ,  first read about it, how it works and then implement it \n",
    "class StandardScalerManual:\n",
    "    def fit(self, X):\n",
    "\n",
    "        self.mean= np.mean(X,axis=0)  # Compute mean for each feature\n",
    "        self.std= np.std(X,axis=0)   # Compute standard deviation for each feature\n",
    "        self.std[self.std== 0]= 1    # To prevent division by zero\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "\n",
    "        return (X - self.mean)/ self.std   # Standardize the features\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        # Fit the scaler and transform the data\n",
    "        self.fit(X)   \n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5b360ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Implement Linear Regression from scratch, here you have to also construct the regulization term coefficient of which will be\n",
    "# denoted by l2_lambda \n",
    "# try to implement L1 regularization or atlease read about it and where it is used\n",
    "class LinearRegressionManual:\n",
    "    def __init__(self, lr=0.01, epochs=1000, l2_lambda=0.0):\n",
    "        \n",
    "        self.lr= lr                # Learning rate for gradient descent\n",
    "        self.epochs= epochs        # Number of training iterations\n",
    "        self.l2_lambda= l2_lambda  # Regularization coefficient (λ)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Train linear regression using gradient descent\n",
    "\n",
    "        row, col= X.shape  \n",
    "        X= np.c_[np.ones(row), X]       # Add bias column of ones\n",
    "        self.weights= np.zeros(col)      # Initialize weights\n",
    "        self.loss=[]                    # initialize loss \n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            y_pred= np.dot(X, self.weights)  # Predicted values\n",
    "            error= y_pred - y                # Prediction error\n",
    "\n",
    "            gradient= (1/ row) * (np.dot(X.T, error))   # Gradient of MSE loss\n",
    "            gradient+=  self.l2_lambda * self.weights    # Add L2 regularization gradient\n",
    "            self.weights-= self.lr * gradient            # Update weights\n",
    "\n",
    "            mse= np.mean(error ** 2)/ 2     # Mean Squared Error loss\n",
    "            l2_term= (self.l2_lambda / 2) * np.sum(self.weights ** 2)\n",
    "            loss= mse +l2_term \n",
    "            self.losses.append(loss)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Return predicted probabilities\n",
    "\n",
    "        row= X.shape[0]            \n",
    "        X= np.c_[np.ones(row), X]       # Add bias term\n",
    "        return np.dot(X, self.weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dfb2fbf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (1,101) and (100,) not aligned: 101 (dim 1) != 100 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Training the Model\u001b[39;00m\n\u001b[32m     10\u001b[39m lr_manual= LinearRegressionManual(lr=\u001b[32m0.1\u001b[39m, epochs=\u001b[32m500\u001b[39m, l2_lambda=\u001b[32m0.1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mlr_manual\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_raw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Plot Results\u001b[39;00m\n\u001b[32m     14\u001b[39m plt.figure(figsize=(\u001b[32m14\u001b[39m,\u001b[32m7\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mLinearRegressionManual.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mself\u001b[39m.loss=[]                    \u001b[38;5;66;03m# initialize loss \u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.epochs):\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     y_pred= \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Predicted values\u001b[39;00m\n\u001b[32m     21\u001b[39m     error= y_pred - y                \u001b[38;5;66;03m# Prediction error\u001b[39;00m\n\u001b[32m     23\u001b[39m     gradient= (\u001b[32m1\u001b[39m/ row) * (np.dot(X.T, error))   \u001b[38;5;66;03m# Gradient of MSE loss\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: shapes (1,101) and (100,) not aligned: 101 (dim 1) != 100 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Testing the Model\n",
    "X_raw= 2*np.random.rand(1,100)\n",
    "y_raw= 4+3*X_raw.flatten() + np.random.randn(100) # y= 4+ 3x +noise\n",
    "\n",
    "# Standardizing \n",
    "res= StandardScalerManual()\n",
    "X_scale= res.fit_transform(X_raw)\n",
    "\n",
    "# Training the Model\n",
    "lr_manual= LinearRegressionManual(lr=0.1, epochs=500, l2_lambda=0.1)\n",
    "lr_manual.fit(X_scale, y_raw)\n",
    "\n",
    "# Plot Results\n",
    "plt.figure(figsize=(14,7))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(lr_manual.loss)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss vs Iterations\")\n",
    "\n",
    "# Plot Prediction\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_scale, y_raw, color='red', label='True Data')\n",
    "plt.title(\"True vs predicted values\")\n",
    "plt.plot(X_scale, lr_manual.predict(X_scale), color='blue', label='Predicted values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5b8470",
   "metadata": {},
   "source": [
    "\n",
    "## Question 2: Logistic Regression from Scratch (with Standardization and Regularization)\n",
    "\n",
    "You are given a binary classification dataset.\n",
    "\n",
    "### Tasks\n",
    "1. Reuse your **manual StandardScaler**.\n",
    "2. Implement **Logistic Regression using Gradient Descent**.\n",
    "3. Use:\n",
    "   - Sigmoid function\n",
    "   - Binary Cross Entropy loss\n",
    "4. Add **L2 Regularization**.\n",
    "5. Report:\n",
    "   - Training loss curve\n",
    "   - Final accuracy\n",
    "\n",
    "Do NOT use `sklearn`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b6002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Implement sigmoid function as told in the lectures \n",
    "def sigmoid(z):\n",
    "    \n",
    "    return 1 / (1 + np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f344717c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Implement Logistic Regression from scratch and here also add the regularizaation term \n",
    "class LogisticRegressionManual:\n",
    "    def __init__(self, lr=0.01, epochs=1000, l2_lambda=0.0):\n",
    "        \n",
    "        self.lr= lr                # Learning rate for gradient descent\n",
    "        self.epochs= epochs        # Number of training iterations\n",
    "        self.l2_lambda= l2_lambda  # Regularization coefficient (λ)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "        X= np.c_[np.ones(n_samples), X]           # Add bias column\n",
    "        self.weights = np.zeros(n_features + 1)   # Initialize weights\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            linear_output = np.dot(X, self.weights)  # Linear combination\n",
    "            y_pred = sigmoid(linear_output)          # Apply sigmoid to get probabilities\n",
    "            error = y_pred - y                       # Error between predictions and actual labels\n",
    "\n",
    "            gradient= (1/ n_samples)* (np.dot(X.T, error))  # Gradient of cross-entropy loss\n",
    "            gradient+= 2 * self.l2_lambda * self.weights    # Add L2 regularization term\n",
    "            self.weights-= self.lr * gradient               # Update weights\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "\n",
    "        n_samples= X.shape[0]\n",
    "        X= np.c_[np.ones(n_samples), X]\n",
    "        return sigmoid(np.dot(X, self.weights))\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs= self.predict_proba(X)\n",
    "        return (probs>= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dd15e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The training loss decreases steadily as the number of iterations increases indicating that the gradient descent algorithm is successful in minimizing the loss function.\n",
    "After a certain number of iterations, the loss curve flattens, showing that the model has converged.\n",
    "Accuracy= Total Number of Samples/ Number of Correct Predictions. The model achieves a high final accuracy, indicating that it correctly classifies the majority of samples.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d398a9ee",
   "metadata": {},
   "source": [
    "\n",
    "## Question 3: K-Means Clustering from Scratch (Matrix Clustering)\n",
    "\n",
    "You are given a **random matrix** `M` of shape `(n, m)`.\n",
    "\n",
    "### Tasks\n",
    "Implement K-Means clustering **from scratch** such that:\n",
    "\n",
    "1. Input:\n",
    "   - A random matrix `M`\n",
    "   - Number of clusters `k`\n",
    "2. Output:\n",
    "   - `assignment_table`: a matrix of same shape as `M`, where each element stores the **cluster label**\n",
    "   - `cookbook`: a dictionary (hashmap) where:\n",
    "     - Key = cluster index\n",
    "     - Value = list of **positions (i, j)** belonging to that cluster\n",
    "   - `centroids`: array storing centroid values\n",
    "\n",
    "You must cluster **individual elements**, not rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de1b3b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centroids\n",
      " [215.75        28.58333333 113.9         71.88888889 166.66666667]\n",
      "Assignment Table\n",
      "[[2 2 4 2 2 1 3]\n",
      " [3 0 1 0 2 2 4]\n",
      " [1 0 4 1 2 4 1]\n",
      " [1 0 2 4 0 1 2]\n",
      " [0 0 0 1 3 3 3]\n",
      " [3 4 0 0 3 3 1]\n",
      " [0 1 3 2 1 1 0]]\n",
      "\n",
      "Coordinates for Cluster 0 (First 10): [(1, 1), (1, 3), (2, 1), (3, 1), (3, 4), (4, 0), (4, 1), (4, 2), (5, 2), (5, 3)]\n",
      "\n",
      "Number of items in Cluster 0: 12\n"
     ]
    }
   ],
   "source": [
    "# Implement K-Means for matrix elements\n",
    "#CAN USE SK-LEARN FOR THIS TASK AS THIS TASK WILL HELP US DIRECTLY IN OUR PROJECT ! \n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "def kmeans_matrix(M, k, max_iters=100):\n",
    "    '''\n",
    "    Returns:\n",
    "    assignment_table: same shape as M, contains cluster labels\n",
    "    cookbook: dict -> cluster_id : list of (i, j) positions\n",
    "    centroids: numpy array of centroid values\n",
    "    '''\n",
    "    M_flat= M.reshape(-1, 1)   # Reshape\n",
    "    kmeans= KMeans(n_clusters=k, max_iter=max_iters, n_init=5)\n",
    "    kmeans.fit(M_flat)\n",
    "    label= kmeans.labels_\n",
    "    centroid= kmeans.cluster_centers_.flatten()\n",
    "\n",
    "    row, col= M.shape\n",
    "    assignment_table= label.reshape(row, col)\n",
    "    cookbook= {i: [] for i in range(k)}\n",
    "\n",
    "    for r in range(row):\n",
    "        for c in range(col):\n",
    "            id= assignment_table[r, c]\n",
    "            cookbook[id].append((r, c))\n",
    "            \n",
    "    return assignment_table, cookbook, centroid\n",
    "\n",
    "M= np.random.randint(0, 255, size=(7, 7)) \n",
    "table, cookbook, centroid= kmeans_matrix(M, k=5)\n",
    "print(\"Centroids\\n\", centroid)\n",
    "print(\"Assignment Table\")\n",
    "print(table)\n",
    "print(f\"\\nCoordinates for Cluster 0 (First 10): {cookbook[0][:10]}\")\n",
    "print(f\"\\nNumber of items in Cluster 0: {len(cookbook[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceca75",
   "metadata": {},
   "source": [
    "\n",
    "## Submission Guidelines\n",
    "- Submit the completed `.ipynb` file.\n",
    "- Clearly label all plots and outputs.\n",
    "- Code readability and correctness matter.\n",
    "- Partial credit will be given for logically correct implementations.\n",
    "\n",
    "**Bonus**\n",
    "- Compare convergence with and without standardization.\n",
    "- Try different values of regularization strength.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
