{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 6 - Group Assignment 1"
      ],
      "metadata": {
        "id": "DUybEBj1q01Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A bit of info before you start.\n",
        "\n",
        "\n",
        "> ***First and foremost, dont use AI to code. Use it to understand and then write code yourself. Use AI to verify if the code you wrote it correct, that will help you learn far far more.***\n",
        "\n",
        "> ***Use cv2 functions only or atleast wherever possible.***\n",
        "\n",
        "> **This assignment marks 1/3rd of your project. And hence, it is the first part of the final submission from your team.**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pazCryz8q2cT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a DataLoader Class\n",
        "\n",
        "Use the pytorch docs sent on group and deepen your understanding using sample practice, or even AI.\n",
        "\n",
        "Finally, create a dataloader for the Fruits-360 dataset ( https://www.kaggle.com/datasets/moltean/fruits ).\n",
        "In <code>__getitem__</code> , include the processing to create LBP image, canny image, and find the 6 color features and 6 shape features.\n",
        "Best implementation should include a proper division of tasks between the method itself and utility functions.\n",
        "Also brainstorm about the data structures you use to process and pass the info. Use cv2 functions for best performance."
      ],
      "metadata": {
        "id": "FruzGhd5rSqE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bxYPAm4Zqqun",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9416b12b-9095-44d4-b32c-1acbe67f272f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.3.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2025.11.12)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.4.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.11)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "import timm\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import cv2 as cv2\n",
        "import numpy as np\n",
        "!pip install kaggle\n",
        "import os\n",
        "import zipfile\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageProcessingUtils:\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_canny_edges(image, threshold1=100, threshold2=200):\n",
        "        if len(image.shape) == 3:\n",
        "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        else:\n",
        "            gray = image\n",
        "        edges = cv2.Canny(gray, threshold1, threshold2)\n",
        "        return edges\n",
        "\n",
        "    @staticmethod\n",
        "    def get_lbp_image(image):\n",
        "        if len(image.shape) == 3:\n",
        "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        else:\n",
        "            gray = image\n",
        "\n",
        "        lbp = np.zeros_like(gray, dtype=np.uint8)\n",
        "        neighbours = [(-1,-1),(-1,0),(-1,1),(0,1),\n",
        "                      (1,1),(1,0),(1,-1),(0,-1)]\n",
        "\n",
        "        for idx, (dy, dx) in enumerate(neighbours):\n",
        "            shifted = np.roll(np.roll(gray, dy, axis=0), dx, axis=1)\n",
        "            # Explicitly cast to uint8 before bitwise operations\n",
        "            lbp |= (((shifted >= gray).astype(np.uint8)) << (7 - idx))\n",
        "\n",
        "        return lbp\n",
        "\n",
        "    @staticmethod\n",
        "    def get_binary_mask(image):\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        _, binary = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "        kernel = np.ones((3, 3), np.uint8)\n",
        "        binary = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)\n",
        "\n",
        "        return binary"
      ],
      "metadata": {
        "id": "IrxZX-UBst6R"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.formatters import Configurable\n",
        "class Fruits360Dataset(Dataset):\n",
        "    def __init__(self, root_dir, image_size=100):\n",
        "        self.dataset = ImageFolder(root_dir)\n",
        "        self.image_size = image_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # ImageFolder gives PIL image\n",
        "        pil_image, label = self.dataset[idx]\n",
        "\n",
        "        # Convert PIL -> NumPy\n",
        "        image = np.array(pil_image)\n",
        "\n",
        "        # Convert RGB -> BGR for OpenCV\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "        # Resize using OpenCV\n",
        "        image = cv2.resize(image, (self.image_size, self.image_size))\n",
        "\n",
        "        # ---- Preprocessing ----\n",
        "        lbp_image = ImageProcessingUtils.get_lbp_image(image)\n",
        "        canny_image = ImageProcessingUtils.compute_canny_edges(image)\n",
        "        binary_mask = ImageProcessingUtils.get_binary_mask(image)\n",
        "        binary_mask = torch.from_numpy(binary_mask).unsqueeze(0)\n",
        "\n",
        "        # Normalize original image\n",
        "        image = image.astype(np.float32) / 255.0\n",
        "\n",
        "        # Convert to torch tensors\n",
        "        image = torch.from_numpy(image).permute(2, 0, 1)\n",
        "        lbp_image = torch.from_numpy(lbp_image).unsqueeze(0)\n",
        "        canny_image = torch.from_numpy(canny_image).unsqueeze(0)\n",
        "\n",
        "        sample = {\n",
        "            \"image\": image,\n",
        "            \"lbp\": lbp_image,\n",
        "            \"canny\": canny_image,\n",
        "            \"mask\": binary_mask,\n",
        "            \"label\": label\n",
        "        }\n",
        "\n",
        "        return sample\n"
      ],
      "metadata": {
        "id": "xlDacpdVVRky"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download(\"moltean/fruits\")\n",
        "print(path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Afekquh8WT5j",
        "outputId": "5f36cbd3-7ff6-4f5a-f583-15e67d058eee"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'fruits' dataset.\n",
            "/kaggle/input/fruits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir(path))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WJWh8-hXA7r",
        "outputId": "d066e90c-f42e-4363-cab2-2e9160c4b36f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['fruits-360_100x100', 'fruits-360_3-body-problem', 'fruits-360_dataset_meta', 'fruits-360_original-size', 'fruits-360_multi']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = os.path.join(path, \"fruits-360_100x100\")\n",
        "print(os.listdir(base_path))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klfVwdlQXSjO",
        "outputId": "deeed250-9199-44b3-d62a-ca674a620669"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['fruits-360']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = os.path.join(path, \"fruits-360_100x100\", \"fruits-360\", \"Training\")\n",
        "\n",
        "dataset = Fruits360Dataset(root_dir=root_dir)\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    dataset=dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    num_workers=2\n",
        ")\n"
      ],
      "metadata": {
        "id": "oi54dyD4QRy3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWKmy5C-YDOx",
        "outputId": "9651508a-32fd-4fa1-a49b-c124e7d2575a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "124716"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample = dataset[4865]\n",
        "image = sample['image']\n",
        "label = sample['label']\n",
        "image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45dTpYEvYimS",
        "outputId": "9eae0a40-e2df-42ed-ad30-a21155d7dbfa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
              "\n",
              "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
              "\n",
              "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "         [1., 1., 1.,  ..., 1., 1., 1.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample = dataset[100]\n",
        "image = sample['image']\n",
        "label = sample['label']\n",
        "image.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClFAKSSElaNh",
        "outputId": "bd31fb0e-ea31-47cf-99b6-473d1d02b810"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 100, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in dataloader:\n",
        "  images = batch['image']\n",
        "  labels = batch['label']\n",
        "  print(images.shape)\n",
        "  print(labels.shape)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KogLoYQukRc3",
        "outputId": "1c14ed35-aac9-4fcf-e1fe-c3b09e509c86"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 3, 100, 100])\n",
            "torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\n",
        "def get_fruit_mask(image_bgr: np.ndarray) -> np.ndarray:\n",
        "    # Convert to HSV\n",
        "    hsv = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2HSV)\n",
        "    # White background has low saturation and high value\n",
        "    # Keep pixels that are NOT white\n",
        "    lower = np.array([0, 30, 0])     # allow colors\n",
        "    upper = np.array([180, 255, 255])\n",
        "    mask = cv2.inRange(hsv, lower, upper)\n",
        "    kernel = np.ones((5, 5), np.uint8)\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
        "\n",
        "    return mask\n",
        "\n",
        "\n",
        "def extract_color_features(image_bgr: np.ndarray, mask: np.ndarray = None) -> np.ndarray:\n",
        "\n",
        "    hsv = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "    h, s, v = cv2.split(hsv)\n",
        "\n",
        "    if mask is None:\n",
        "        mask = get_fruit_mask(image_bgr)\n",
        "\n",
        "    h_pixels = h[mask > 0]\n",
        "    s_pixels = s[mask > 0]\n",
        "    v_pixels = v[mask > 0]\n",
        "\n",
        "    if len(h_pixels) == 0:\n",
        "        return np.zeros(6, dtype=np.float32)\n",
        "\n",
        "    mean_hue = np.mean(h_pixels)\n",
        "    std_hue = np.std(h_pixels)\n",
        "\n",
        "    mean_sat = np.mean(s_pixels)\n",
        "    std_sat = np.std(s_pixels)\n",
        "\n",
        "    mean_val = np.mean(v_pixels)\n",
        "    std_val = np.std(v_pixels)\n",
        "\n",
        "\n",
        "    color_features = np.array(\n",
        "        [\n",
        "            mean_hue,\n",
        "            std_hue,\n",
        "            mean_sat,\n",
        "            std_sat,\n",
        "            mean_val,\n",
        "            std_val,\n",
        "        ],\n",
        "        dtype=np.float32,\n",
        "    )\n",
        "\n",
        "    return color_features"
      ],
      "metadata": {
        "id": "KcyP7fU1kRZY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_shape_features(image_bgr: np.ndarray) -> np.ndarray:\n",
        "    mask = get_fruit_mask(image_bgr)\n",
        "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # Step 1: Generate fruit mask\n",
        "    mask = get_fruit_mask(image_bgr)\n",
        "\n",
        "    # Step 2: Find contours from the mask\n",
        "    contours, _ = cv2.findContours(\n",
        "        mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
        "    )\n",
        "\n",
        "    if len(contours) == 0:\n",
        "        return np.zeros(6, dtype=np.float32)\n",
        "\n",
        "    cnt = max(contours, key=cv2.contourArea)\n",
        "    area = cv2.contourArea(cnt)\n",
        "\n",
        "    h, w = mask.shape\n",
        "    area_ratio = area / (h * w)\n",
        "\n",
        "    x, y, bw, bh = cv2.boundingRect(cnt)\n",
        "    aspect_ratio = bw / bh if bh != 0 else 0\n",
        "\n",
        "    hull = cv2.convexHull(cnt)\n",
        "    hull_area = cv2.contourArea(hull)\n",
        "    solidity = area / hull_area if hull_area > 0 else 0\n",
        "\n",
        "    perimeter = cv2.arcLength(cnt, True)\n",
        "    circularity = (4 * math.pi * area) / (perimeter * perimeter + 1e-6)\n",
        "\n",
        "    moments = cv2.moments(cnt)\n",
        "    hu = cv2.HuMoments(moments)\n",
        "    hu1 = -np.sign(hu[0]) * np.log10(abs(hu[0]))\n",
        "    hu2 = -np.sign(hu[1]) * np.log10(abs(hu[1]))\n",
        "\n",
        "    return np.array([area_ratio, aspect_ratio, solidity, circularity, hu1[0], hu2[0]], dtype=np.float32)\n"
      ],
      "metadata": {
        "id": "V5L7oc1GNYrP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "train_path = os.path.join(path, \"fruits-360_100x100\", \"fruits-360\", \"Training\")\n",
        "test_path = os.path.join(path, \"fruits-360_100x100\", \"fruits-360\", \"Test\")\n",
        "\n",
        "print(\"Train Path:\", train_path)\n",
        "print(\"Test Path:\", test_path)\n",
        "\n",
        "# Dataset init\n",
        "train_dataset = Fruits360Dataset(train_path)\n",
        "test_dataset = Fruits360Dataset(test_path)\n",
        "\n",
        "# Train\n",
        "train_idx, val_idx = train_test_split(\n",
        "    np.arange(len(train_dataset)),\n",
        "    test_size=0.1,\n",
        "    shuffle=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "train_subset = Subset(train_dataset, train_idx)\n",
        "val_subset = Subset(train_dataset, val_idx)\n",
        "\n",
        "print(\"Training Samples:\", len(train_subset))\n",
        "print(\"Validation Samples:\", len(val_subset))\n",
        "print(\"Test Samples:\", len(test_dataset))\n",
        "\n",
        "# Dataloader\n",
        "train_loader = DataLoader(\n",
        "    train_subset,\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_subset,\n",
        "    batch_size=64,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(\"DataLoaders Ready!\")\n",
        "\n",
        "batch = next(iter(train_loader))\n",
        "for k, v in batch.items():\n",
        "    if torch.is_tensor(v):\n",
        "        print(k, v.shape)\n",
        "    else:\n",
        "        print(k, v)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8n0r3rvzTsFk",
        "outputId": "b6f5a101-cebf-4dbb-effe-7fd8192152b0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Path: /kaggle/input/fruits/fruits-360_100x100/fruits-360/Training\n",
            "Test Path: /kaggle/input/fruits/fruits-360_100x100/fruits-360/Test\n",
            "Training Samples: 112244\n",
            "Validation Samples: 12472\n",
            "Test Samples: 41577\n",
            "DataLoaders Ready!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image torch.Size([64, 3, 100, 100])\n",
            "lbp torch.Size([64, 1, 100, 100])\n",
            "canny torch.Size([64, 1, 100, 100])\n",
            "mask torch.Size([64, 1, 100, 100])\n",
            "label torch.Size([64])\n"
          ]
        }
      ]
    }
  ]
}